SELFGENERATED WARGAME AI: D OUBLE LAYER AGENT TASK
PLANNING BASED ON LARGE LANGUAGE MODEL
A P REPRINT
Yuxiang Sun
School of Management and Engineering
Nanjing University
Nanjing, 210023
sunyuxiang@nju.edu.cn
Junjie Zhao
School of Management and Engineering
Nanjing University
Nanjing, 210023
junjiezhao@smail.nju.edu.cn
Checheng Yu
School of Management and Engineering
Nanjing University
Nanjing, 210023
211870228@smail.nju.edu.cn
Wei Wang
School of Intelligence Science and Technology
Nanjing University
Nanjing, 210023
221900255@smail.nju.edu.cn
Xianzhong Zhou∗
School of Management and Engineering
Nanjing University
Nanjing, 210023
zhouxz@nju.edu.cn
December 19, 2023
ABSTRACT
The large language models represented by ChatGPT have a disruptive impact on the field of artificial
intelligence. But it mainly focuses on natural language processing, speech recognition, machine
learning and natural language understanding. This paper innovatively applies the large language
model to the field of intelligent decision-making, places the large language model in the decision-
making center, and constructs an agent architecture with the large language model as the core. Based
on this, it further proposes a two-layer agent task planning, issues and executes decision commands
through the interaction of natural language, and carries out simulation verification through the
wargame simulation environment. Through the game confrontation simulation experiment, it is found
that the intelligent decision-making ability of the large language model is significantly stronger than
the commonly used reinforcement learning AI and rule AI, and the intelligence, understandability
and generalization are all better. And through experiments, it was found that the intelligence of the
large language model is closely related to prompt. This work also extends the large language model
from previous human-computer interaction to the field of intelligent decision-making, which has
important reference value and significance for the development of intelligent decision-making.
Keywords Reinforcement learning ·Large language models ·Agents ·Generative AI
∗This work was supported by the National Natural Science Foundation of China under Grant 61876079. (Corresponding author:
Xianzhong Zhou)arXiv:2312.01090v2  [cs.AI]  18 Dec 2023
A P REPRINT
1 Introduction
Since ChatGPT was officially launched on November 30, 2022, it has quickly become one of the most popular intelligent
Chatbot Van Dis et al. [2023], Stokel-Walker and Van Noorden [2023]. Since its inception, ChatGPT has been applied in
multiple fields such as code correction Surameery and Shakor [2023], public health Som S [2023], and global warming
Biswas [2023]. In July 2023, OpenAI released the Code Interpreter plugin, further enhancing ChatGPT’s data parsing
capabilities and addressing the natural weaknesses of large language models in mathematics and language. These
developments have provided new inspiration for improving the intelligence and generalization of AI in the field of
intelligent wargame simulation, that is, using ChatGPT self generated AI to make intelligent decisions in war games.
Although the development and application of rule AI and data-driven AI Cheng et al. [2021]is the starting point in
the field of intelligent wargame, data-driven AI has gradually become a research hotspot in recent years, in which
Reinforcement learning AI has made a series of breakthroughs. In terms of data-driven AI, Liu Man, Zhang Hongjun,
and others have designed a wargame decision-making framework that balances rules and data Liu et al. [2020a]. In
terms of Reinforcement learning AI, Li Chen’s team from Nanjing University of Science and Technology designed a
multi-agent decision-making method under the Actor Critical framework and achieved good intelligence Chen et al.
[2021]. Xu Jiale, Zhang Haidong, and others designed a CNN based strategy learning model to improve the accuracy of
wargame situation prediction Xu et al. [2022]. Tencent’s AI Lab used Deep reinforcement learning to achieve game
confrontation in the King’s Glory game, and defeated professional players Ye et al. [2020], Chen et al. [2020]. In a
word, with the deepening of the combination of deep learning, Reinforcement learning and intelligent wargame, the
intelligence of agents has been continuously improved Mnih et al. [2015], Silver et al. [2016], Vinyals et al. [2019], Liu
et al. [2020b].
Although rule AI does not require a long period of training, due to its limitations in rules, the upper limit of intelligence
level is difficult to break through the upper limit of rules; While data-driven AI and Reinforcement learning AI improve
their intelligence and flexibility by processing large amounts of data through Reinforcement learning algorithms, their
interpretability is poor, and it is difficult to achieve model migration under scenario and capture point changes SUN et al.
[2022], Wurman et al. [2022], Schrittwieser et al. [2020], Silver et al. [2018]. Therefore, improving the intelligence and
generalization of AI in the field of intelligent wargame becomes the focus of further research.
Moreover, the decision-making of adversarial games is complex and continuous. In order to make decisions more
intelligent and generalized, this article focuses on introducing a self generated AI wargame architecture based on a large
language model. Create a decision-making mechanism that involves multiple generative agents interacting, mimicking
human behavior and generating interpretable, credible, and highly generalizable game adversarial intelligent decisions.
The core work of this article is as follows:
1.The self generated AI wargame architecture is an intelligent agent architecture centered on a large language
model. This architecture consists of multiple generative agents, each with its own large language model (this
article uses ChatGPT as the driving tool). These intelligent agents can communicate and cooperate with each
other through reflective and memory streams, and jointly make decisions. By talking to each other, they can
share information, analyze the situation, and make inferences and decisions based on the conversation content.
2.Build a two-layer agent task planning model, targeting strategic agents and tactical agents to plan tasks for the
game confrontation process. Strategic agents describe specific situations observed by all current agents.
Planning refers to task allocation and execution based on all observed situational information. The tactical
agent only focuses on the situation observed by a single agent chess piece and executes related tasks according
to the strategic planning agent. However, tactical agents can also have their own judgments and feedback
based on the prompts issued by strategic agents.
3.Taking wargame as the experimental platform, the experiment shows that the intelligent decision-making
ability of the large language model is significantly stronger than reinforcement learning AI and rule AI, and
the intelligence, understandability, and generalization are all better. And through experiments. Meanwhile,
research has found that providing prior knowledge from experts in the field specifically to large language
models can significantly improve their intelligence.
2 Generative Wargame AI Architecture
In the war chess environment, we have realized the confrontation between six red chessmen and five blue chessmen as
Figure 1 shows. The red and blue chessmen in different clusters have different semantic interaction information, which
is generated through ChatGPT.
2
A P REPRINT
…
…
…
…
Transport the Squad, 
Provide Fire Support
CP: APC, transport the 
Squad to the front line and 
remain vigilant. 
APC : CP, we are rapidly 
transporting the Squad to 
the front line.Adjust the Task Plan, Clear the 
Enemy’s Firepower Points, and 
then Support the Squad
CP: TK_A and TK_B, you are required 
to provide fire support to our Squad.  
TK_A: CP, the enemy defense is strong, 
should eliminate the enemy’s firepower 
points first and then support the infantry.
CP: Agreed. TK_A and TK_B, clear the 
enemy’s firepower points first and then 
support our Squad.
Squad: IFV, please suppress 
the armored forces. 
IFV: Understood, I will do 
my best to destroy it.Request Fire Suppression
…
…
Figure 1: Specific interaction of self generated wargame AI in the environment
Individual Situational 
Awareness
Battle SituationCombat 
Memory Flow
Command 
Experience FlowExtracted 
Combat Memory
Extracted
Command 
Experience Tactical Agent
Combat Operation
Strategic Agentadjust
Overall Situational 
Awareness
Battle Situationextract
Task Plan
Figure 2: The relationship between strategic and tactics agent
To implement the decision-making mechanism mentioned above, we have developed an agent architecture consisting of
three main components: a memory stream for storing and allocating buffers and generating batches; a reflection stream
for using batches as prompts for the large language model to understand its role in the decision-making process; and a
task planning stream for synthesizing higher-level reasoning from batches to enable the agent to integrate situational
information and make better pre-battle plans. The agent architecture is designed to store, synthesize, and apply past
battlefield experience to enable the large language model to generate trustworthy decisions.
3
A P REPRINT
3 Generative Wargame AI Model
3.1 Wargame Agent Interaction
In the architecture described above, the wargame agents obtain situational information and interact with each other in
natural language to maintain collaboration. Each agent describe their action in natural language, such as “red agent 1 is
preparing to capture the control point and moving along the road”, “blue agent 2 is preparing to aim at the enemy target
1”. Then the sentence is translated into specific actions and directly affect the wargame environment. In the meanwhile,
all the actions and movements will be displayed as a series of number symbols which appear above each avatar to
provide abstract representation of actions. To achieve this, the architecture utilizes a language model to translate
language into actions, while representing a concise symbol above each wargame to represent ChatGPT’s suggestions
for actions that this agent should take. For example, “red agent 1 is preparing to capture the control point” is displayed
as “!” appearing above the wargame, while “red agent 1 is preparing to aim at the enemy” is displayed as “ →".
In this environment, agents communicate with each other in natural language which is fully understandable by humans.
They obtain the situational information of other operators and environment from the semantics of sentences. Here is a
sample of an agent communicating with another.
Figure 3: Double Layer Agent Task Planning Decision Framework Based on Large Language Model
3.2 Composing model
The generative wargame AI aims to provide a novel decision-making framework for intelligent decision-making in
wargame environment. Compared to the traditional rule-based AI, data-driven AI , or the reinforcement learning
4
A P REPRINT
AI, our architecture utilizes ChatGPT for intelligent decision-making and interaction with the wargame environment.
Generative wargame AI takes the current environment and past experiences as inputs, and produces output in the form
of generated actions.
Generated actions can be described as these steps: the architecture provides the large language model with well-trained
prompts, the language model chooses the numbers which encompass all the actions to take according to prompts,
the agents take actions with the corresponding number. The innovation of the architecture lies in the combination of
the large language model with retrieval of relevant information and the utilization of prompts to adjust the output of
language model.
Building upon the foundation of the architecture, we constructed a dual-layer agent system including the strategic agent
and tactical agent. Strategic agent obtains all the information regarding the states of both its own sites and the observed
opponents as input, then it combines this with the overall environment and input as prompts to generate a macro-level
tactics intelligent task planning flow. And strategic agent assigns tasks to the tactics agent in the form of prompts, and
tactics agent, based on its own states, provides modification suggestions and reasons for modifying. Then strategic
agent keeps replanning according to these suggestions until all the tactics agents are not going to provide furthermore
suggestions.
Certainly, the strategic and tactics agents still face a lot of challenges even with the use of the state-of-the-art large
language model like GPT-4 LLM(Large Language Model). Since extensive generation of events and memories generated
by the two agents, the most critical challenge in this architecture lies in generating the most relevant memory fragments
while retrieving and synthesizing relevant data from the memory stream. Therefore, this article attempts to reduce
computational power and memory requirements, and uses GPT-4 LLM for strategic agent decision-making, facilitating
overall strategic input and expert knowledge document input. For tactical agents, this article uses GPT-3.5 LLM for
decision-making, as tactical agents can interact and provide feedback on results in turn, reducing computational power
and memory requirements without affecting intelligence
Memory Stream As the central component of the architecture, the memory stream directly influences the efficiency
and accuracy of decision-making. The entire memory stream is a list of memory objects, with each object consisting
of a natural language description, a creation timestamp and a recent access timestamp. The fundamental element in
memory list is observation, which contains all the situational information observed by agents. Due to the presence of
the fog of the war, the battlefield environment doesn’t allow for complete knowledge and awareness. The common
information observed by an agent in a particular state is subject to certain limitations, which includes individual actions,
actions taken by our own side’s agent and the actions taken by opponent agents within our visible range.
Examples:
Observation 1: agent observes its own side’s agent approaching the control point and trying to control it.
Observation 2: agent observes its opposed agent approaching urban residential area and trying to shoot.
We construct a retrieval function within the entire memory stream architecture and utilize it to extract observations
from the historical experiences of the agents, providing a foundation for generating reasonable prompts and enable the
language model to produce rational decision. The retrieval function can be selective, with the form of prioritizing the
extraction of recently observations, important nodes set before and relevant memory to produce effective outcome.
Recency assigns a higher score to the observation added recently, in which case the agent prioritizes the memory
information generated by the recent several steps. To account for the influence of time factors, we implement a time
decay coefficient to calculate the score.
Importance categorizes the data within the memory stream into regular memories and core memories, and allocating
higher scores to the core memories generated by agents. For example, a red agent moving towards the left and
approaching the road can be categorized as a regular memory while a red agent approaching the control point and
eliminating a blue agent can be classified as a core memory. In this architecture, we ask the language model to directly
output the importance integer scores in a range from 1to10, in which case 1means the purely common memory
like moving on the road while 10means the most important core memory like seizing the control point or shooting
successfully. The specific implementation process can be described as follows: retrieving the corresponding memory
from the memory stream to form a prompt, allowing the agent to generate importance scores accordingly and storing
them back to the memory stream.
Example:
Memory: the red agent one is seizing the control point.
Importance score: 8Relevance assigns a higher score to those object that are relevant to the current situation because
of the presence of relevance between different memory objects. For example, a red agent arrives at the road and
approaching the control point at a higher speed, this memory has a strong correlation with the red agent’s seizing the
5
A P REPRINT
second_half 58th
Figure 4: Experimental Simulation Environment Display
control point. In this paper we ask ChatGPT to generate the relevance score in a scale from 1 to 10 to describe the
degree of relevance between memory objects.
As mentioned above, the three components of our architecture have been quantified into scores for the agents. To
calculate the final scores of agents we normalize recency ,importance andrelevance scores to a range of [0,1]by scaling
method. The final scores can be calculated using the following formula: score final =αrecency ∗score recency +
αimportance ∗score importance +αrelevance ∗score relevance
We use this score to comprehensively determine the prompts that should be extracted, and instruct the agent to generate
corresponding and reasonable action-plannings based on these prompts.
Reflection Stream However, the observational performance of memory flow has limitations in the decision-making
process in practical wargame environment. Reasoning based on raw observation is not efficient enough to allow a
large language model to generate high-level decision results. It is necessary to infer and generate advanced reasoning
semantics through the observation of information and the planning of actions. This paper defines this reasoning process
as a higher-level memory flow, referred to as reflection . It is essentially a higher-level and more abstract thinking process.
The reflection flow is generated together with the memory flow, but the generation of reflection is distinguished by the
retrieval function in the preceding memory flow. When the score in the retrieval function exceeds a certain threshold,
reflection is triggered. This reflective process involves a higher-level abstraction and understanding of previously
observed information. It is essentially a combination of observed semantics and planned semantics generated through
prompts, and it is generated periodically, providing the wargame agent with reasoning semantics.
The first step of reflection is to raise questions and clarify the reflective process based on the previous experience flow
of wargame agent. For example, the blue agent is approaching the road and accelerating towards the control point.
The planning suggests that the red agent should reach hexagon 1403 and shoot the blue agent at that point. From this,
reflective semantics are generated: The blue agent poses a significant threat and may create a disadvantageous situation
for the red agent in this confrontation.
The reflection process allows the agent to reflect not only on their current observation but on other reflection. As a
result, memories generated by the agent can be divided into different levels under the reflection mechanism, allowing
for more accurate decision-making at an abstract level.
6
A P REPRINT
Task Planning Stream Strategic agent, based on the current situation observed by all the agents of our side, describes
it as a prompt following a specific format: <Summary ,Observations ,Planning >. The Summary aims to convert the
current situation from visual to semantic information Sun et al. [2022].
Observations describe specific circumstances observed by all agents, further enrich semantic information based on the
summary. Planning involves task allocation and execution based on the observed situation.
Example:
Summary : our10agents are moving towards the control point and have identified 3blue agents. Observations : blue
agent 1is nearing the control point.
Planning : red agent 1−3will prioritize engaging blue agent 1, while agents 4−10will quickly move towards the
control point.
3.3 ChatGPT + Wargaming Business Process
The whole core process is to transform the situation image information in the wargame simulation into semantic
information, which includes description information and situation information, and this information is sent to the
wargame agent in the form of prompt, and then the agent feedback the corresponding planning semantics, which is goal
oriented. The planning semantics are then transformed into action sequences (such as 1,2,3,4, ..10, where numbers
represent specific actions. Alternatively, they are transformed into corresponding actions such as attack, defense,
evasion, acceleration, shooting, left movement, etc.), which affect the environment and generate new environments.
These actions are then recycled back to the starting situation image and converted into semantics.
On this basis, in order to reduce computational power and memory requirements, and improve operational efficiency,
this article allows strategic agents to use GPT-4 LLM and tactical agents to use GPT-3.5 LLM. Compared to using
GPT-4 or GPT-3.5 LLM entirely, this can comprehensively improve the intelligence of intelligent decision-making
without requiring too much computing power and memory space. Firstly, input expert prior knowledge documents
into the strategic agent for learning through GPT-4 LLM, and then provide appropriate prompt inputs to enable the
strategic agent to make decisions through GPT-4 LLM and convert them into action outputs that affect the wargame
environment. The strategic agent then sends corresponding instructions to each tactical agent for execution. The tactical
agent provides feedback on whether the task is suitable for the current agent through GPT-3.5 LLM combined with
appropriate prompts, and provides the recommended execution results to the strategic agent for adjustment.
Strategic agent : Based on the task planning flow, the strategic agent synthesizes states 1 to 10 and provides a task
planning sequence, which is the action that each wargame should take in the step allocation;
Tactical agent : The tactical agent receives task planning and provides modification suggestions and reasons for the
assigned tasks based on its own state;
The strategic agent plans again based on the modification suggestions until all tactical agents no longer provide
modification suggestions.
4 Verification of Simulation Experimental Environment
4.1 Experimental Environment Display
This paper verifies the above established large language model through simulation experiments. The simulation platform
is a wargame simulation platform, which can conduct game confrontation between red and blue sides. Both red and
blue sides can use intelligent algorithms to make decisions and execute actions Sun et al. [2020]. The basic adversarial
rule is that the red and blue sides compete for the middle control point (red flag), and the party who first reaches the
control point wins. Or if one party is completely destroyed by the other party, the other party wins.
4.2 Advantages of large language model over reinforcement learning intelligent decision-making
In the previous experiments, we mainly made decisions through rule AI and reinforcement learning AI. For the first time,
this work used the large language model to make decisions for agents, and it was verified on this platform. Interestingly,
this work found that there is a large difference between large language models and Reinforcement learning. First, large
language models or trained large language models can make decisions without waiting for the convergence of training,
and can directly obtain high intelligence. Reinforcement learning algorithms often need a lot of training to gradually
adapt to a new task. At the same time, compared with the reinforcement learning algorithm, the decision making using
the large language model can directly achieve excellent intelligence in multiple different tasks, and does not need to
re-train for different tasks, which is of high value for practical applications.
7
A P REPRINT
This article proposes two algorithms, GWA algorithm and GWAE algorithm. The GWA algorithm adopts the composi-
tion model proposed in this article and utilizes ChatGPT for decision-making in large language models. GWAE inputs
expert experience on the basis of GWA. This paper inputs expert experience of Military simulation in the form of a
document. See the appendix for the document.
MethodMissions
Kill Goal Survive
GWAE 298±11 10504±64 4238±28
GWA 332±9 9106±99 5102±33
RNM-PPO 745±9 9102±141 4985±44
PPO 850±19 7804±44 5068±38
PK-DQN 792±14 7732±60 5026±53
DQN 1285±7 6948±161 5154±57
Table 1: Scores of different algorithms for three tasks: kill, get goal, and survive.
0 25 50 75 100 125 150 175 200
Episodes020406080100Cumulative Winrates
Line Plot for Cumulative Winrate of All Algorithms Over Episodes
GWAE - Ours
GWA - Ours
RNM-PPO
PPO
PK-DQN
DQN
Figure 5: Line Plot for Winrate of All Algorithms Over Episodes. The winning rate of GWA algorithm is generally
higher than that of Reinforcement learning algorithm, including RNM-PPO, PPO, PK-DQN, DQN. After inputting
expert prior knowledge documents for GWA algorithm, the intelligence of GWAE algorithm is significantly improved
on the basis of the original GWA.
The experiment compares the GWAE algorithm and GWA algorithm proposed in this paper, and compares the winning
rate of the algorithm proposed in this paper with the RNM-PPO Xue et al. [2023], PPO, PK-DQN Sun et al. [2020]
and DQN algorithms. Through Figure 6, Figure 7 and Figure 8, it can be found that the intelligence of reinforcement
learning algorithms DQN, PK-DQN, PPO, and RNM-PPO is enhanced in turn. However, the GWA algorithm that uses
the large language model to make decisions is better than the reinforcement learning algorithm. Only the RNM-PPO
8
A P REPRINT
GWAE - Ours GWA - Ours RNM-PPO PPO PK-DQN DQN
Algorithm20
020406080100Mean WinrateViolin Plot for Cumulative Mean Winrate of All Algorithms Over Episodes
Figure 6: Violin Plot for Mean Winrate of All Algorithms Over Episodes. The GWAE algorithm and GWA algorithm
have a higher winning rate and are more stable.
020406080GWAE - Ours_mean_winrate
020406080GWA - Ours_mean_winrate
0204060RNM-PPO_mean_winrate
0204060PPO_mean_winrate
0204060PK-DQN_mean_winrate
0 50
GWAE - Ours_mean_winrate02040DQN_mean_winrate
0 25 50 75
GWA - Ours_mean_winrate
0 25 50 75
RNM-PPO_mean_winrate
0 20 40 60
PPO_mean_winrate
0 20 40 60
PK-DQN_mean_winrate0 20 40
DQN_mean_winrate
Figure 7: Scatter plot of the average winning rate of all
algorithms. The results of comparing the overall winning
rates of GWAE, GWA, RNM-PPO, PPO, PK-DQN, and
DQN algorithms
kill control surviveGWAE - Ours GWA - Ours RNM-PPO PPO PK-DQN DQNMethodT ask Mean Scores
0.00.20.40.60.81.0Figure 8: Task Mean Scores Heatmap.The performance
of GWAE, GWA, RNM-PPO, PPO, PK-DQN, and DQN
algorithms in three typical tasks: kill, control, and survive.
The darker the color, the better the algorithm performs in
this task.
algorithm is closer to GWA. If expert experience documents are fed to GWA, the GWAE algorithm’s victory rate will be
significantly improved. Overall, the overall winning rate of the algorithm proposed in this paper is significantly better
than the previous classical reinforcement learning algorithm. Also on the premise of fixed blue intelligence and fixed
reasoning scenarios, the overall effect of intelligent decision-making using the large language model is also relatively
9
A P REPRINT
stable, the overall victory rate fluctuates relatively little. For different typical task scenarios, GWA and GWAE exhibits
outstanding performance, especially in the task of scoring, the performance of GWA and GWAE is significantly better
than that of classical reinforcement learning algorithms. These two algorithms have demonstrated the potential value of
large language models in the field of intelligent decision-making by making appropriate decisions for different task
planning scenarios without undergoing extensive training.
5 Conclusion
This work innovatively applies the large language model to intelligent decision-making, and verifies the feasibility of
the large language model for decision-making in the wargame platform. Compared with the intelligent decision-making
of Reinforcement learning, this paper finds that the large language model has obvious advantages. Firstly, the large
language model for decision-making has strong adaptability in practical game confrontations due to sufficient training in
advance. There is no need to wait to restart training, and it has strong intelligence and generalization for different tasks.
Secondly, the intelligence shown by the large language model is obviously stronger than the general Reinforcement
learning algorithm, which proves the great potential of the large language model in decision-making. Finally, this article
also found through experiments that there is a significant correlation between the intelligence of large language models
and prompt. If there is a more suitable prompt, its displayed intelligence is significantly improved. Of course, the work
of this article is still an initial exploration of the large language model, and there is still much work to be innovated in
the future, such as the attempt of the large language model in different scenarios, and the use of the large language
model in more complex game adversarial environments to further enhance the intelligence of the adversarial blue, in
order to test the intelligence level of the large language model. This work also extends the large language model from
previous human-computer interaction to the field of intelligent decision-making, which has important reference value
and significance for the development of intelligent decision-making.
References
Eva AM Van Dis, Johan Bollen, Willem Zuidema, Robert van Rooij, and Claudi L Bockting. Chatgpt: five priorities for
research. Nature , 614(7947):224–226, 2023.
Chris Stokel-Walker and Richard Van Noorden. What chatgpt and generative ai mean for science. Nature , 614(7947):
214–216, 2023.
Nigar M Shafiq Surameery and Mohammed Y Shakor. Use chat gpt to solve programming bugs. International Journal
of Information Technology & Computer Engineering (IJITC) ISSN: 2455-5290 , 3(01):17–22, 2023.
Biswas Som S. Role of chat gpt in public health. Annals of biomedical engineering , 51(5):868–869, 2023.
Som S Biswas. Potential use of chat gpt in global warming. Annals of biomedical engineering , 51(6):1126–1127, 2023.
K Cheng, G Chen, X Yu, M Liu, and T Shao. Knowledge traction and data-driven wargame ai design and key
technologies. Syst. Eng. Electron. Technol , 43:2911–2917, 2021.
M Liu, H Zhang, W Hao, K Cheng, and J Wang. Decision method for tactical level military chess entity operations.
Control and Decision , 35(12):2977–2985, 2020a.
Li Chen, Huang Yanyan, Zhang Yongliang, and Chen Tiande. Multi-agent decision-making method under the actor-critic
framework and its application in wargames [j]. Systems engineering and electronic technology , 43(03):755–762,
2021.
J Xu, H Zhang, D Zhao, and W Ni. Tactical maneuver strategy learning of land war based on convolutional neural
network. Journal of System Simulation , 34(10):2181, 2022.
Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei
Guo, et al. Mastering complex control in moba games with deep reinforcement learning. In Proceedings of the AAAI
Conference on Artificial Intelligence , volume 34, pages 6672–6679, 2020.
Guibin Chen, Deheng Ye, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng
Yu, et al. Towards playing full moba games with deep reinforcement learning. Advances in Neural Information
Processing Systems , 33:621–632, 2020.
V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,
Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement
learning. nature , 518(7540):529–533, 2015.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser,
Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks
and tree search. nature , 529(7587):484–489, 2016.
10
A P REPRINT
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H
Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent
reinforcement learning. Nature , 575(7782):350–354, 2019.
CY Liu, CX Mu, and CY Sun. Overview of deep reinforcement learning algorithm and its application. Journal of
Intelligent Science and Technology , 2(4):314–326, 2020b.
Yuxiang SUN, Yihui PENG, Bin LI, Jiawei ZHOU, Xinlei ZHANG, and Xianzhong ZHOU. Review of intelligent
games: Implications of game ai for combat inference. Chinese Journal of Intelligent Science and Technology , 4(2):
157–173, 2022.
Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas J Walsh,
Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al. Outracing champion gran turismo drivers
with deep reinforcement learning. Nature , 602(7896):223–228, 2022.
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur
Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with
a learned model. Nature , 588(7839):604–609, 2020.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot,
Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters
chess, shogi, and go through self-play. Science , 362(6419):1140–1144, 2018.
Yuxiang Sun, Bo Yuan, Qi Xiang, Jiawei Zhou, Jiahui Yu, Di Dai, and Xianzhong Zhou. Intelligent decision-making
and human language communication based on deep reinforcement learning in a wargame environment. IEEE
Transactions on Human-Machine Systems , 53(1):201–214, 2022.
Yuxiang Sun, Bo Yuan, Tao Zhang, Bojian Tang, Wanwen Zheng, and Xianzhong Zhou. Research and implementation
of intelligent decision based on a priori knowledge and dqn algorithms in wargame environment. Electronics , 9(10):
1668, 2020.
Yufan Xue, Yuxiang Sun, Jiawei Zhou, Lisha Peng, and Xianzhong Zhou. Multi-attribute decision-making in wargames
leveraging the entropy-weight method in conjunction with deep reinforcement learning. IEEE Transactions on
Games , 2023.
A Large Language Model Prompt
In this experiment, it was found that the intelligence of the large language model is closely related to prompt. Two
different prompts were used for guidance in this experience, and the results returned after prompt input are shown in the
figure. The figure 9 shows the response results of the strategic agent feedback. The figure 10 shows the response results
of tactical agent feedback.
B Prior Knowledge Document
This article feeds professional domain knowledge documents into the proposed GWA algorithm, forming the GWAE
algorithm. Document link: https://github.com/sunyuxiang926/wargame/blob/b0118cd0
1fa46f2790a89895336df81519d91707/files/theses/explanat
ion_of_domain_expert_knowledge.pdf
11
A P REPRINT
Strategic Agent :
Red tank 1 Move_to : 16, 14 Reason: Tank 1 is moved from the closest corner to the target, while avoiding the direct 
attack of blue tank 2.
Red tank 2 Move_to : 17, 17 Reason: Tank 2 is moved as close to the target as possible while avoiding the possible fire 
range of blue tanks 2 and 10.
Red tank 3 Move_to : 18, 19 Reason: Tank 3 is moved towards the target position, trying to stay out of the range of blue 
tanks 2 and 10.
Red tank 4 Move_to : 15, 17 Reason: Tank 4 is moved to a location that can fend off the firepower of blue tanks 2 and 10 
and quickly reach the target when needed.
Red tank 5 Move_to : 16, 19 Reason: Tank 5 is trying to get close to the target, while maintaining a safe distance to avoid 
the firepower of blue tanks 2 and 10.
Red tank 6 Move_to : 17, 19 Reason: Tank 6 is heading towards the target, keeping a distance as much as possible to avoid 
the firepower of blue tanks 2 and 10.
Red tank 7 Move_to : 16, 17 Reason: Tank 7 is maintained in a good position towards the target, keeping itself out of the 
direct range of blue tanks 2 and 10.
Red tank 8 Move_to : 19, 18 Reason: Tank 8 is moved towards the target, trying to stay out of the range of blue tanks 2 
and 10.
Red tank 9 Move_to : 20, 17 Reason: Tank 9 is moving forward to the target, avoiding the fire range of blue tanks 2 and 10.
Red tank 10 Move_to : 20, 16 Reason: Tank 10 is trying to get as close to the target as possible, keeping a safe distance to 
avoid the firepower of blue tanks 2 and 10.
These moves aim to position each tank towards the target point and to keep each tank at as varied locations as possible 
to avoid the concentrated firepower from the blue team. This strategy aims to get the tanks to the target point as quickly 
as possible and to avoid enemy firepower where possible.
Figure 9: Strategic Agent Prompt Feedback Display
Tactical Agent :
As Red tank 1, the command "Red tank 1 Hide" given by the command center seems somewhat unreasonable from my 
perspective.
"Hide" implies that I should avoid blue tank 2's attacks, which may lead to distancing myself from the target control point. 
As an attacking force, my priority should be to approach the target and engage in combat at the right moment, rather 
than evading the battle.
My ultimate goal is to capture control point [12, 24], and currently, blue tank 2 (located at [16, 40]) is closer to the cont rol
point. If I choose to hide, it will further increase the distance between me and the target, which is not conducive to 
reaching the objective quickly.
In the last step, blue tank 5, 7, and 8 launched indirect attacks at [17, 7], and blue tank 10 also moved near [17, 41]. 
Given that blue tank 5, 7, and 8 launched indirect attacks, I have reason to believe that the area around [17, 7] might not 
be the position of blue tank 2. Therefore, avoiding direct confrontation with blue tank 2 is not the most urgent task at the 
moment.
In conclusion, from my perspective, I believe that "Red tank 1 Hide" is not the most reasonable command. Instead, I 
should choose to advance towards the control point [12, 24], adopt a suitable strategy to avoid direct engagement with 
blue tank 2, and consider how to coordinate with the movements of other friendly tanks to achieve victory.
Figure 10: Tactical Agent Prompt Feedback Display
12
