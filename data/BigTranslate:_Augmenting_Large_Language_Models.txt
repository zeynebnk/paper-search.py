BigTranslate: Augmenting Large Language Models with Multilingual
Translation Capability over 100 Languages
Wen Yang1,2, Chong Li1,2, Jiajun Zhang1,2,3 ∗, and Chengqing Zong1,2
1Institute of Automation, Chinese Academy of Sciences
2School of Artificial Intelligence, University of Chinese Academy of Sciences
3Wuhan AI Research
{yangwen2023, lichong2021 }@ia.ac.cn
{jjzhang, cqzong }@nlpr.ia.ac.cn
Abstract
Large language models (LLMs) demonstrate
promising translation performance among vari-
ous natural languages. However, many LLMs
especially the open-sourced ones, such as
BLOOM (Scao et al., 2023) and LLaMA (Tou-
vron et al., 2023), are English-dominant and
support only dozens of natural languages, mak-
ing the potential of LLMs on language trans-
lation less explored. In this work, we present
BigTranslate which adapts LLaMA that covers
only 20 languages and enhances it with multi-
lingual translation capability on more than 100
languages. BigTranslate is built upon LLaMA-
13B and it is optimized in three steps. First, we
continue training LLaMA with massive Chi-
nese monolingual data. Second, we continue
training the model with a large-scale paral-
lel dataset that covers 102 natural languages.
Third, we instruct-tune the foundation model
with multilingual translation instructions, lead-
ing to our BigTranslate model. The preliminary
experiments on multilingual translation show
that BigTranslate performs comparably with
ChatGPT and Google Translate in many lan-
guages and even outperforms ChatGPT in 8
language pairs. We release the BigTranslate
model1and hope it can advance the research
progress.
1 Introduction
Large language models (LLMs), such as Chat-
GPT (OpenAI, 2022) and PaLM 2 (Anil et al.,
2023), demonstrate impressive translation capabili-
ties among various natural languages. For example,
several recent studies indicate that ChatGPT is a
good translator in many scenarios (e.g., spoken lan-
guage translation, document translation, and mul-
tilingual translation), and it can even outperform
SOTA translation engines in some specific scenar-
ios (Bawden and Yvon, 2023; Hendy et al., 2023;
∗Corresponding author.
1https://github.com/ZNLP/BigTranslateJiao et al., 2023; Wang et al., 2023; Zhu et al.,
2023). LLMs are also preferred as a translator for
their interactive usage.
However, most of the existing LLMs are English-
dominant and the popular LLMs support only sev-
eral or dozens of natural languages. For example,
GLM (Du et al., 2022; Zeng et al., 2022) just sup-
ports English and Chinese. BLOOM (Scao et al.,
2023) covers 46 languages while LLaMA (Touvron
et al., 2023) only supports 20 languages. It is well-
known that there are over 7000 natural languages
in the world and existing LLMs cover only a very
small fraction of the languages. Obviously, a large
population in the world cannot be benefited from
the multilingual capability of LLMs.
In order to equip LLMs with much more multilin-
gual ability, we introduce BigTranslate that adapts
LLaMA making it capable of translating over 100
natural languages. Instead of optimizing the foun-
dation model with self-supervised learning on mas-
sive monolingual data over multiple languages, we
mainly employ the bitexts which can transfer the
knowledge from high-resource languages to low-
resource ones through semantic mapping between
parallel sentences.
Specifically, we build BigTranslate based on the
13B version of LLaMA which is proven to be com-
parable with GPT-3 in many natural language pro-
cessing tasks. The BigTranslate is constructed in
three steps. In the first step, we utilize massive
Chinese texts to continue training LLaMA, result-
ing in a strong model which well supports Chinese.
In the second step, a large-scale parallel dataset
covering 102 natural languages is employed to con-
tinue training the LLMs in a curriculum learning
manner and we obtain a multilingual foundation
model that has the potential to perform translating
among more than 100 natural languages. In the
third step, instruction tuning is applied to optimize
the multilingual foundation model with rich transla-
tion instructions. Finally, we get our BigTranslatearXiv:2305.18098v3  [cs.CL]  21 Nov 2023
model.
To verify the effectiveness of our BigTranslate
model, we conduct preliminary multilingual trans-
lation experiments on all 102 languages. We com-
pare BigTranslate with both Google Translate and
ChatGPT. Since the automatic evaluation metric
BLEU is usually criticized for the poor correla-
tion with human judgments in machine translation
quality, we further employ GPT-4 (OpenAI, 2023)
which shows a high correlation with human (Liu
et al., 2023) as the evaluator and we design well-
defined prompts to make GPT-4 act like a human
evaluator. The experiments show that BigTranslate
performs comparably with Google and ChatGPT in
many languages, and even outperforms ChatGPT
in 8 language pairs.
2 Related Work
2.1 Large Language Models
Since the advent of GPT-3 (Brown et al., 2020)
by OpenAI in 2020, large language models that
employ Transformer as the backbone and contain
tens or hundreds of billions of parameters, such
as PaLM (Chowdhery et al., 2022), OPT (Zhang
et al., 2022), BLOOM (Scao et al., 2023), Chin-
chilla (Hoffmann et al., 2022), Galactica (Taylor
et al., 2022), GLM (Du et al., 2022; Zeng et al.,
2022), and LLaMA (Touvron et al., 2023), are con-
stantly emerging. Among these LLMs, ChatGPT
is a big milestone that demonstrates that the foun-
dation large language model exhibits emergent and
general abilities when the model is large enough
and equipped with instruction tuning.
Many recent studies investigate the ability of
ChatGPT, GPT-4, and other LLMs in traditional
natural language processing tasks, including ma-
chine translation (Bawden and Yvon, 2023; Hendy
et al., 2023; Jiao et al., 2023; Wang et al., 2023; Zhu
et al., 2023). (Jiao et al., 2023) reports that Chat-
GPT and GPT-4 are good translators, especially for
high-resource languages and spoken language sce-
narios. (Wang et al., 2023) demonstrates that Chat-
GPT and GPT-4 perform quite well in document
translation with the help of long context modeling.
(Bawden and Yvon, 2023) and (Zhu et al., 2023)
show that LLMs like ChatGPT and BLOOM are
also multilingual translators and even outperform
SOTA online translation engines. However, most
of the existing LLMs are English-dominant and
cover up to only dozens of languages. In this work,
we address this challenge and present BigTranslatewhich can support more than 100 languages.
2.2 Multilingual Neural Machine Translation
Multilingual neural machine translation aims at
translating multiple languages with a single shared
model (Johnson et al., 2017). Most studies focus
on the unbalanced problem in multilingual trans-
lation. For example, some works investigate how
to design shared and language-dependent model
parameters in a multilingual translation framework
(Wang et al., 2018, 2019; Lin et al., 2021; Xie et al.,
2021; Wang and Zhang, 2022). Several works
explore how to train the multilingual translation
model more effectively and efficiently when the
training data are quite unbalanced across languages
(Zhou et al., 2021; Huang et al., 2022). Few studies
pursue the potential of a multilingual translation
model on handling more than 100 languages. For
example, NLLB (Costa-jussà et al., 2022) proposed
by Meta aims to build a multilingual translation
model that could translate as many languages as
possible (currently covering more than 200 lan-
guages). However, this kind of model can only
perform translating. In this work, we pursue con-
structing a multilingual translator by adapting an
LLM while maintaining its generic ability.
3 BigTranslate Construction
3.1 LLaMA as Foundation Model
Considering its impressive performance on most
English benchmarks after pre-training on 1.4T to-
kens (Touvron et al., 2023), LLaMA is adopted as
our foundation model. Specifically, the BigTrans-
late is initialized from the LLaMA-13B model to
reduce the computational cost and continues to
train on massive Chinese and parallel corpus.
3.2 Augmenting Foundation Model with
Chinese
LLaMA shows a poor Chinese language under-
standing and generation performance due to the
lack of a sufficient Chinese pre-training corpus (Cui
et al., 2023), although its performance in English
is comparable to or even better than GPT-3. More-
over, Shah et al. (2023) found that Chinese exhibits
a significantly low cross-lingual similarity with
other languages, including English and German,
which means that the inferior processing ability in
Chinese text will hinder our foundation model to-
wards a better multilingual translation model. Thus
it is indispensable to augment the ability of our
foundation model with additional Chinese vocab-
ulary and pre-training corpus. By doing so, we
expect the final model to be capable of multilingual
translation centered on both English and Chinese.
To achieve this, we first append the original vo-
cabulary with 6,223 Chinese tokens, most of which
are Chinese characters, generated from Sentence-
Piece (Kudo and Richardson, 2018) using the byte-
pair encoding (BPE) algorithm (Sennrich et al.,
2016) on Chinese text. Then, a large-scale Chi-
nese pre-training corpus, including CLUE (Xu,
2019; Xu et al., 2020), Chinese News, and Chinese
question-answering dataset, is adopted to pre-train
the model resulting in Chinese LLaMA.
3.3 Augmenting Foundation Model with 102
languages
The purpose of this study is to equip a large lan-
guage model with Chinese-centric multilingual
ability. Despite intensive training on massive Chi-
nese monolingual data, the Chinese LLaMA model
primarily exhibits proficiency in Chinese language
processing, but lacks adequate multilingual capabil-
ities. Furthermore, the continuous training on Chi-
nese monolingual data has the potential to diminish
the model’s performance in the 20 languages origi-
nally supported by LLaMA. To address these lim-
itations, we further refine the foundational model
by incorporating a substantial parallel dataset en-
compassing 102 languages. This second training
stage aims to enhance the model’s ability to facil-
itate multilingual translation tasks, enabling it to
support a broader range of languages.
Large language model pre-training is typically
conducted using monolingual data, focusing on
autoregressive training in the language to enhance
the model’s capabilities. In contrast, multilingual
large language model pre-training involves training
on multilingual parallel data, enabling the language
model to learn across different languages. The
primary challenge in multilingual large language
model pre-training lies in achieving a balance in
learning between high-resource and low-resource
languages. This balance ensures that the model
acquires proficiency in both high-resource and low-
resource languages during training.
The issue of maintaining balanced learning be-
tween high-resource and low-resource languages
has been a long-time topic of concern in multilin-
gual learning. Despite the notable progress made
in the development of large language models, thisproblem is still not well solved as the model ca-
pacity increases. To address this challenge encoun-
tered during the pre-training phase, our work pro-
poses an incremental data sampling strategy. By
employing the strategy, the model is trained on a
harmonized multilingual parallel corpus at each
stage, mitigating the concern of unbalanced lan-
guage proficiency.
3.3.1 Large-scale Parallel Dataset
Construction
In order to enhance the language capabilities of
the Chinese LLaMA model to support 102 lan-
guages, we constructed a comprehensive parallel
corpus dataset consisting of 102 languages. This
dataset was employed to continue training the foun-
dational model. The compilation of this dataset
drew upon multiple sources, including widely avail-
able public parallel corpus datasets and household
datasets. The public datasets utilized in our study
contain IWSLT, WMT, CCMT, and OPUS-100
(Zhang et al., 2020), forming the initial corpus of
our dataset.
After obtaining the initial corpus, a key con-
sideration is to balance the two-way translation
in language pairs. For example, the data size of
Chinese-to-English translation should be similar to
English-to-Chinese. To achieve this balance, we
utilize data augmentation to enrich the corpus if
necessary. The data augmentation follows the sub-
sequent strategy. In cases where the number of
parallel corpus falls below 1 million, we flip the
entire corpus to create the corpus for the opposite
translation direction. In contrast, for corpora with
more than 1 million instances, we randomly flip
half the amount of corpus to generate the corre-
sponding corpus. After data augmenting, the initial
corpus of 142 translation directions is substantially
enriched, expanding to a significantly larger corpus
of 242 translation directions.
To effectively illustrate the distribution of the
corpus, we present a visual representation of the
language-pair distribution within the multilingual
datasets as figure 1. The matter pertaining to the
imbalance between high-resource and low-resource
language pairs continues to be a prominent concern
within the current corpus.
3.3.2 Tokenizer
LLaMA tokenizes data with the byte-level byte-
pair (BBPE) encoding algorithm, implemented by
SentencePiece. LLaMA’s original vocabulary in-
en-frfr-enen-cscs-enen-deen-ruru-ende-enen-zhzh-eses-zhzh-enfr-zhzh-frzh-hihi-zhen-lven-fizh-ptpt-zhlv-enzh-mofi-enen-etzh-bomo-zhet-enzh-heen-rohe-zhro-enen-trtr-enen-urur-enmt-enen-mtbo-zhen-plpl-enen-bnbn-enzh-koen-ltlt-enko-zhen-gaga-enen-nnka-enen-kann-enid-zhen-nlen-uzuz-enar-cscs-arko-enen-koen-esen-iten-ptpt-ennl-enzh-idar-enes-enen-brja-enbr-enit-enzh-uyen-jaen-mlml-enel-enen-elen-tttt-enen-tgtg-enzh-mymy-zhen-mgmg-enen-gugu-enzh-kaen-eoka-zheo-enen-dada-enen-viuy-zhvi-enen-svsv-enen-ugug-enen-eueu-enen-neen-arne-enen-sqsq-enen-sksk-enha-enen-haen-glgl-enen-mken-hemk-enhe-enen-iden-huth-enhu-enid-enen-then-sisi-enen-amam-enen-bgen-fafa-enzh-nebg-enne-zhen-hren-bshr-enen-hibs-enhi-enen-ms0.1110% of corpus
ms-en
en-rw
rw-en
en-ca
ca-en
en-sl
sl-en
en-km
km-en
en-nb
nb-en
en-sh
sh-en
ar-he
he-ar
en-cy
cy-en
en-xh
xh-en
en-is
en-uk
uk-en
is-en
en-az
az-en
en-sr
sr-en
ar-de
de-ar
en-af
ro-it
it-ro
af-en
en-as
ro-nl
nl-ro
as-en
nl-it
it-nl
ro-de
de-ro
it-de
de-it
en-ku
ku-en
nl-de
de-nl
en-ta
ta-en
en-no
no-en
en-be
be-en
en-wa
wa-en
en-zu
zu-en
pa-en
en-pa
en-ps
ps-en
en-te
te-en
en-kk
kk-en
my-en
en-my
en-fy
fy-en
se-en
en-se
en-ig
ig-en
en-mr
mr-en
en-li
li-en
kn-en
en-kn
en-oc
oc-en
en-yo
yo-en
or-en
en-or
yi-en
en-yi
en-ky
ky-en
en-tk
tk-en
hy-en
en-hy
en-gd
gd-en
an-en
en-an
en-dz
dz-en
en-mn
mn-en0.0010.010.1% of corpusFigure 1: The language-pairs distribution of multilingual corpus. All the datasets consist of about 300 million
sentence pairs.
cludes 32,000 tokens, primarily comprising En-
glish and Latin tokens. To augment LLaMA’s pro-
ficiency in handling Chinese text, we expand its
vocabulary by incorporating additional 6,223 Chi-
nese tokens as section 3.2 introduced. Furthermore,
to boost LLaMA’s comprehension across multilin-
gual parallel datasets, we further extend the vocab-
ulary to 53,613 tokens, most of the added tokens
are trained on texts spanning 102 languages.
To alleviate the issue of vocabulary skew result-
ing from imbalances in instances across the corpus,
we implement a strategy that involved selecting a
subset of the large-scale parallel dataset for vocab-
ulary training. Specifically, we randomly sample
max_num instances from each language to be in-
cluded in the vocabulary training. This approach
ensures that the common words shared among
102 languages are adequately represented, serving
as a crucial prerequisite for model understanding
multi-languages. Concretely, we set max_num to
1,000,000.
3.3.3 Incremental Multilingual Pre-training
Prior to data sampling, we employ multilingual
vocabulary to segment the entire multilingual par-
allel corpus. Subsequently, we construct training
samples by concatenating the same language sen-
tence pairs. Each sample is comprised of multiple
parallel sentence pairs and has a fixed length of
1,024 tokens. This approach ensured the forma-
tion of coherent and consistent training samples for
subsequent model training.To mitigate the issue of the model disproportion-
ately focusing on learning high-resource corpus
during the training phase, which could potentially
hinder the learning of low-resource languages, we
draw inspiration from curriculum learning (Bengio
et al., 2009) to propose an incremental approach
for multilingual LLMs pre-training.
In this incremental pre-training method, we grad-
ually expose the model to language pairs in a
curriculum-like manner. Initially, the model is
exposed to high-resource language pairs, allow-
ing it to establish a solid foundation in those lan-
guages. Subsequently, we progressively introduce
low-resource language pairs, enabling the model to
gradually expand its knowledge and proficiency in
these languages.
Specifically, we follow a three-step approach in
our incremental pre-training method, as shown in
Figure 2. Firstly, we set the sample interval size
and divide language pairs into distinct intervals
based on the number of instances for each language
pair. Secondly, we calculate the sample mean for
all language pairs in each interval. Thirdly, we
dynamically measure the moment of adding the
language-pair samples next interval according to
the sample means in the previous sample interval.
In the following part, we detail the three steps.
In the first step, we set sample interval size, de-
noted as S. We hypothesize that if the interval size
Sis below a specific threshold, we can consider
that all language pairs in this interval have average
resources. Consequently, training the multilingual
Step 1 
en-defr-en
zh-en…Interval A
da-enzh-my
eu-en…Interval B
kk-enen-wa
mr-en…Interval N
……en-wa
en-defr-en
kk-ensk-en
eu-en
da-enzh-enmr-enMultilingual Corpus
en-defr-en
zh-en…Interval A
da-enzh-my
eu-en…Interval B
kk-enen-wa
mr-en…Interval N
…
Interval A
Sample MeanInterval B
Sample MeanInterval N
Sample Mean
Interval S1
Sample Mean
Interval S2
Sample Mean
Interval Sn
Sample MeanInterval S1
Interval S2
Interval Sn
BigTranslate
AlgorithmStep 2 Step 3 
…
…
……Figure 2: The outline of three-step incremental multilingual pre-training approach. ①represents dividing multilin-
gual language pairs into different intervals, ②denotes calculating sample means for all language pairs within each
sample interval, ③represents sorting the intervals in descending order based on sample mean values. The algorithm
in step 3 is detailed in Algorithm 1 for incremental pre-training.
model on this interval enables the model to acquire
the average proficiency in each language.
In our preliminary experiments, it is observed
that an excessively large language interval results
in an unbalanced learning distribution among lan-
guage pairs within the interval. Conversely, setting
the interval too small leads to the model focusing
on only a few or even just one language within
the interval. Additionally, we noticed that high-
resource and low-resource languages exhibited dif-
ferent sensitivities to interval size. To minimize the
learning discrepancy between language pairs, we
determine the interval size for language pairs based
on their respective sample sizes. For language pairs
with a sample size exceeding 10,000, we set the
interval size Sto 10,000. Conversely, for language
pairs with a sample size below 10,000, we opted
for a sample interval Sof 5,000.
For example, the sample size of En-Ro ( English-
to-Romanian ) pair is 80,980, and the sample size
is greater than 10,000, so we set the interval size
to 10,000 and partition it into the interval [80,000,
90,000). Otherwise, for the Mr-En ( Marathi-to-
English ) language pair, the sample size is 5,080,
which falls below 10,000. In this case, the inter-
val size is defined as 5,000 and the Mr-En pair is
categorized into the interval [5,000, 10,000).In the second step, we calculate the sample mean
for all language pairs within each sample interval.
The sample mean serves as an approximation of the
sample size for each language pair in the interval.
While the sample mean cannot substitute the actual
sample size of each language pair in the interval,
we narrow down the interval size to minimize the
disparity between the sample size of each language
pair and the sample mean. Then, the sample inter-
vals will be sorted in descending order based on
sample mean values.
Finally, the model is exposed to all sample inter-
vals following Algorithm 1 . Initially, the model is
exposed to the current sample interval, and dynam-
ically calculates the mean value of the untrained
samples in the current sample interval during train-
ing process. When the mean value of the untrained
sample is not greater than the sample mean of the
next interval, we mix the current untrained sample
and the next interval sample to form a new sam-
ple interval, and train model on the new sample
interval. This process is repeated iteratively.
Assuming that the sample mean value of the sam-
ple interval S1is 103,251, denoted as MS1, and the
sample mean value of the sample interval S2is
95,280, denoted as MS2. According to Algorithm
1, BigTranslate will be exposed to all samples in
Algorithm 1 Framework of the incremental pre-
training algorithm.
Given: The set of sample intervals sorted in de-
scending order, [S1,S2,...,S n];
1:forSiin[S1,S2,...,S n]do
2: Set the sample mean in Si,MSi
3: Set the mean value of the untrained sam-
ples in Si,Mut
4: Initialize Mut←MSi
5: Set the sample mean in Si+1,MSi+1
6: whileMut>MSi+1do
7: Pre-train BigTranslate onSi;
8: Calculate Mutafter each step of Pre-
training;
9: end while
10: Add the untrained samples in SitoSi+1
11: Shuffle and Update Si+1
12:end for
theS1interval for batch training. Following each
training step, we will calculate the mean of the
untrained samples in S1, denoted as Mut. With
the increase of training steps, Mutwill gradually
approach MS2. When Mutis not greater than MS2
for the first time, we will merge the untrained sam-
ples in S1intoS2, and then start training on new
S2.
By adopting this incremental multilingual pre-
training approach, we aim to ensure a more bal-
anced and comprehensive model learned across dif-
ferent language pairs, thus addressing the challenge
of uneven resource allocation during training. As a
result, the model will successfully achieve mastery
in 102 languages through its multilingual learning
journey. We name the multilingually pre-trained
model BigTranslate .
3.4 Multilingual Translation Instruction
Tuning
Previous work (Mishra et al., 2022; Wei et al.,
2022; Sanh et al., 2022) has shown that utilizing
instruction-following data to tune LLMs enables
such models to understand tasks described in natu-
ral languages, and show better multi-task learning
ability on training tasks and generalization ability
on unseen tasks. Instruction tuning does not inject
new capabilities into the model. Instead, its pur-
pose lies in activating the existing capabilities that
were established during the training phase.
In this section, we construct a multilingual trans-
lation instruction dataset containing 102 languagesand 242 language-pairs. Furthermore, we fine-tune
theBigTranslate model to unlock the performance
of model on multilingual machine translation abil-
ity.
Instruction tuning data construction The mul-
tilingual translation instruction tuning dataset con-
sists of 1,000 parallel sentence pairs for each lan-
guage pair, which is selected from the training set.
If the number of sentence pairs in the training set
is below 1,000, all training examples are chosen
for the instruction fine-tuning data. Utilizing this
approach, the instruction tuning dataset comprises
241,128 parallel sentence pairs across 242 language
pairs.
Instruction tuning prompts selection We have
designed a set of 28 multilingual translation
prompts that encompass various application sce-
narios for multilingual translation. We randomly
select a prompt from the set for instruction tun-
ing for each parallel sentence. Accordingly, the
instruction tuning dataset is scrambled to ensure
randomness and diversity.
3.5 Training Details
The BigTranslate model is pre-trained on 42.8B
and 47.0B tokens in the Chinese and Multilingual
pre-training stages, respectively. The learning rates
are empirically set to 5e-5 under a cosine scheduler
with a 3% warm-up percentage, and batch sizes are
65,536 in all pre-training stages.
Then, we fine-tune BigTranslate on 240k mul-
tilingual translation instructions, where the batch
size and the number of epochs are set to 32 and 3,
respectively. The learning rate and weight decay
are empirically set to 2e-5 and 0. In inference, we
employ beam search for decoding with a beam size
of 5.
To speed up training, we adopted DeepSpeed
(Rasley et al., 2020) for data parallelization,
FlashAttention (Dao et al., 2022), and Gradient
Checkpointing (Chen et al., 2016) for saving mem-
ory. All training processes are conducted on 16
A100 GPUs with 80GB of RAM.
4 Experiments
To demonstrate the effectiveness of the BigTrans-
late model, we conduct preliminary multilingual
translation experiments on all 102 languages. We
compare our model with Google Translate and
ChatGPT2. We conduct the translation experiments
2We use gpt-3.5-turbo API in May 2023
fr-en
ca-en
da-en
mt-en
ro-en
de-en
pt-en
sv-en
bo-zh
nb-en
ru-en
af-en
gl-en
cs-en
en-zh
es-en
oc-en
it-en
nn-en
nl-en
id-en
sk-en
sr-en
tr-en
lv-en
eo-en
mo-zh
sh-en
lt-en
uy-zh
hr-en
et-en
uk-en
fr-zh
pl-en
ms-en
pt-zh
es-zh
bs-en
zh-en
ar-en
bg-en
hu-en
fi-en
bn-en
he-en
ko-zh
ja-en
sl-en
id-zh
mk-en
sq-en010203040506070BLEUBigTranslate
ChatGPT
Google Translateko-en
hi-en
ga-en
hi-zh
ne-en
mg-en
li-en
vi-en
ne-zh
he-zh
el-en
eu-en
ur-en
my-zh
my-en
gd-en
is-en
mr-en
as-en
cy-en
tt-en
be-en
gu-en
az-en
xh-en
tg-en
ml-en
ha-en
ka-en
kk-en
kn-en
ps-en
tk-en
hy-en
ky-en
th-en
ka-zh
dz-en
te-en
yo-en
zu-en
fa-en
uz-en
ig-en
km-en
pa-en
si-en
ta-en
yi-en
rw-en
am-en
ku-en010203040506070BLEUBigTranslate
ChatGPT
Google TranslateFigure 3: An illustrated comparison of 102 languages from X to English or Chinese between BigTranslate, ChatGPT,
and Google Translate. We sort the language scores in BLEU for BigTranslate in descending order.
pt-en
fr-en
ro-en
zh-en
es-en
de-en
pl-en
en-zh
ca-en
sv-en
da-en
it-en
cs-en
nl-en
ru-en
sk-en
nb-en
gl-en
fr-zh
pt-zh
es-zh
af-en
fi-en
lv-en
hr-en
nn-en
mt-en
ko-zh
ja-en
et-en
id-en
bg-en
sh-en
ko-en
sl-en01234SCOREBigTranslate
ChatGPT
Google Translateeo-en
lt-en
hu-en
ms-en
uk-en
id-zh
tr-en
oc-en
bs-en
sr-en
bo-zh
ar-en
bn-en
hi-zh
mo-zh
sq-en
he-en
mk-en
he-zh
ga-en
vi-en
mg-en
ne-en
eu-en
uy-zh
el-en
my-zh
ur-en
ml-en
uz-en
ka-zh
tt-en
ka-en
tg-en
gu-en01234SCOREBigTranslate
ChatGPT
Google Translate
Figure 4: An illustrated comparison of 70 languages from X to English or Chinese between BigTranslate, ChatGPT,
and Google Translate. We sort the language scores in GPT-4 score for BigTranslate in descending order.
from 102 languages to English or Chinese with the
three systems3.
4.1 Datasets and Evaluation Metrics
To assess the effectiveness of the multilingual
machine translation model, we conducted evalu-
ations using the devtest subset of the FLORES-200
dataset4(Costa-jussà et al., 2022). The FLORES-
200 dataset comprises a corpus extracted from 842
web articles encompassing various fields and top-
ics, resulting in a total of 3,001 sentences. Notably,
a team of experts meticulously translated these sen-
tences into 200 languages, thereby constituting the
FLORES-200 dataset.
4.2 Main Results
In this section, we report the main results on multi-
lingual machine translation on BigTranslate, Chat-
GPT5, and Google Translate. Then, we report our
main findings about the exploration of multilingual
machine translation using a large language model.
Automatic Evaluation with BLEU Figure 3
shows the detailed outcomes in BLEU scores of the
translation results from 102 languages into English
and Chinese. Detailed results for each translation
direction are listed in Appendix B. In Figure 3, we
have sorted the BLEU scores obtained from the
BigTranslate model in descending order. We split
the whole figure into two parts for clarity. The up-
per figure presents the first half of all the language
pairs, while the bottom figure presents the second
half. Notably, the upper figure reveals that a signif-
icant proportion of language pairs in the BigTrans-
late model, specifically 46 out of 104 (equivalent to
over 44%), yield more than 20 BLEU scores. This
observation suggests that the BigTranslate model
exhibits commendable performance with respect
to these languages, if we believe BLEU is a reli-
able evaluation metric. We also find that, when
comparing our BigTranslate model with ChatGPT,
the results demonstrate that BigTranslate performs
on par with ChatGPT in many languages. Intrigu-
ingly, BigTranslate surpasses ChatGPT in 9 lan-
guage pairs (e.g. mt-en, bo-zh, it-en, mo-zh, uy-zh,
mg-en, my-zh, my-en, dz-en) in terms of BLEU
scores. When comparing BigTranslate with Google
3Since we plan to perform human-like evaluation and we
cannot well evaluate the languages except English and Chi-
nese, we just evaluate the translation direction from other
languages to English or Chinese.
4We select 50 sentences in each direction of the devtest set
for evaluation
5gpt-3.5-turbo APITranslate, Figure 3 clearly shows that BigTranslate
significantly lags behind Google Translate in most
of the language pairs in BLEU scores. However, as
(Hendy et al., 2023; Anil et al., 2023) pointed out
that BLEU is not a good evaluator when the BLEU
scores exceed a threshold (e.g. 20.0), and it has a
low correlation with human preference.
Automatic Evaluation with GPT-4 (Liu et al.,
2023) demonstrates that GPT-4 achieves a much
higher Spearman correlation with human than all
previous methods on the summarization task. We
thus follow (Liu et al., 2023), and employ GPT-4
with Chain-of-Thoughts (CoT) prompts to automat-
ically evaluate the quality of translation. In the
evaluation, GPT-4 assigns a score ranging from 0
to 5 to assess the translation quality of every sen-
tence across three models. The prompt is a natural
language instruction and contains two parts. In the
first part, we define the evaluation task and explain
the scoring scope. The first part of the prompt we
use is below.
You will be given two sentences, trans-
lated sentence is translated from source
sentence, reference sentence is the
ground truth of translation.
Your task is to rate the translation result
between translated sentence and refer-
ence sentence.
Assign a score for translation result on a
scale of 0 to 5, where 0 is the lowest and
5 is the highest based on the Evaluation
Criteria.
In the second part, the prompt should contain
specific evaluation criteria for evaluating machine
translation, which can guide GPT-4 on how to score.
We describe evaluation criteria below:
Semantic similarity refers to the mea-
surement of how similar or related two
sentences are in terms of their meaning
or semantics. It focuses on capturing
the similarity in the underlying concepts,
ideas, or information conveyed by the
sentences, rather than just the surface-
level lexical or syntactic similarities.
The translated sentence can completely
express the meaning of the reference sen-
tence. The closer the translated sentence
is to the reference sentence, the higher
the score.
The style of the translated sentence
should be as consistent as possible with
the reference sentence
Subsequently, we calculate the average score
of all sentences within the same language pairs,
which represents the overall translation score of the
respective language pair. In Appendix A, we can
see the details of the GPT-4 evaluation prompt.
We utilize GPT-4 to rate the translation of 70
language pairs for three translation models. The
results are illustrated in Figure 4 and the detailed
results can be found in Appendix C. The figure
indicates that a total of 28 language pairs exhibit
good or excellent translation scores surpassing 3.5,
thereby accounting for 40% of all language pairs.
This result demonstrates that BigTranslate has re-
markable multilingual translation performance un-
der GPT-4 evaluation. If we compare Figure 4
against Figure 3, we can observe some interesting
phenomena. For example, the performance gap
between our BigTranslate and Google Translate
becomes much narrowed in many language pairs,
indicating that BigTranslate is approaching Google
Translate in dozens of languages in terms of GPT-4
score. In comparison to ChatGPT, we can find that
BigTranslate achieves similar performance to Chat-
GPT in 27 languages, with a difference in GPT-4
scores of less than 0.3 points. Moreover, BigTrans-
late outperforms ChatGPT in 8 languages in GPT-4
scores.
4.3 Discussion
The experimental results displayed in the previ-
ous section demonstrate the effectiveness of our
BigTranslate model in extending large language
models to enable multilingual translation over 100
languages. As a multilingual translator, BigTrans-
late can be employed in translation for many lan-
guages, although we still need to further improve
the translation quality for extremely low-resource
languages. In addition to language translation, Big-
Translate can also be applied in other natural lan-
guage processing tasks just as ChatGPT does. No-
tably, the LLM ability especially the English capa-
bility can be transferred to many other languages
including several low-resource ones with the help
of our BigTranslate model. For example, BigTrans-
late is now good at translating Tibetan and Mon-
golian languages, and English and Chinese NLP
abilities in LLMs can be transferred into these lan-
guages.5 Conclusion and Future Work
In this work, we introduced BigTranslate which
is a large language model equipped with the capa-
bility of multilingual translation over 100 natural
languages. After two steps of continuing training
with massive Chinese monolingual data and large
scale multilingual parallel data of 102 languages,
LLaMA is extended to have the potential multilin-
gual ability on 102 natural languages. Using the
final step of instruction tuning with multilingual
translation data, BigTranslate is obtained. The ex-
periments demonstrate that BigTranslate performs
comparable to Google Translate and ChatGPT in
many languages, and even surpasses ChatGPT in 8
languages when evaluated with GPT-4.
Due to the issue of unbalanced data, BigTrans-
late is still weak in dozens of languages. In the
future, we will extend BigTranslate to further en-
hance its ability in low-resource languages. More-
over, we will take full advantage of the multilingual
ability of BigTranslate, and improve the perfor-
mance of the languages in other natural language
processing tasks.
Acknowledgements
We thank Jinliang Lu, Yu Lu, Yangyifan Xu and
Qian Wang for multilingual translation data con-
struction.
References
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, et al. 2023. Palm 2 technical report. arXiv
preprint arXiv:2305.10403 .
Rachel Bawden and François Yvon. 2023. Investigating
the translation performance of a large multilingual
language model: the case of bloom. arXiv preprint
arXiv:2303.01911 .
Yoshua Bengio, Jérôme Louradour, Ronan Collobert,
and Jason Weston. 2009. Curriculum learning. In
Proceedings of the 26th annual international confer-
ence on machine learning , pages 41–48.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos
Guestrin. 2016. Training deep nets with sublinear
memory cost. arXiv preprint arXiv:1604.06174 .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311 .
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha
Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe
Kalbassi, Janice Lam, Daniel Licht, Jean Maillard,
et al. 2022. No language left behind: Scaling
human-centered machine translation. arXiv preprint
arXiv:2207.04672 .
Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient
and effective text encoding for chinese llama and
alpaca. arXiv preprint arXiv:2304.08177 .
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
InAdvances in Neural Information Processing Sys-
tems, volume 35, pages 16344–16359. Curran Asso-
ciates, Inc.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 320–335,
Dublin, Ireland. Association for Computational Lin-
guistics.
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf,
Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita,
Young Jin Kim, Mohamed Afify, and Hany Hassan
Awadalla. 2023. How good are gpt models at ma-
chine translation? a comprehensive evaluation. arXiv
preprint arXiv:2302.09210 .
Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-
sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
ford, Diego de Las Casas, Lisa Anne Hendricks,
Johannes Welbl, Aidan Clark, et al. 2022. Train-
ing compute-optimal large language models. arXiv
preprint arXiv:2203.15556 .
Yichong Huang, Xiaocheng Feng, Xinwei Geng, and
Bing Qin. 2022. Unifying the convergences in multi-
lingual neural machine translation. In Proceedings
of the 2022 Conference on Empirical Methods in
Natural Language Processing , pages 6822–6835.
WX Jiao, WX Wang, JT Huang, Xing Wang, and ZP Tu.
2023. Is chatgpt a good translator? yes with gpt-4 as
the engine. arXiv preprint arXiv:2301.08745 .
Melvin Johnson, Mike Schuster, Quoc V . Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-
rat, Fernanda B. Viégas, Martin Wattenberg, Greg
Corrado, Macduff Hughes, and Jeffrey Dean. 2017.
Google’s multilingual neural machine translation sys-
tem: Enabling zero-shot translation. Trans. Assoc.
Comput. Linguistics , 5:339–351.Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 66–71, Brussels, Belgium.
Association for Computational Linguistics.
Zehui Lin, Liwei Wu, Mingxuan Wang, and Lei Li.
2021. Learning language specific sub-network for
multilingual machine translation. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing,
ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual
Event, August 1-6, 2021 , pages 293–305.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023. G-eval:
Nlg evaluation using gpt-4 with better human align-
ment. arXiv preprint arXiv:2303.16634 .
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2022. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 3470–3487, Dublin, Ireland.
Association for Computational Linguistics.
OpenAI. 2022. Introducing chatgpt. OpenAI blog .
OpenAI. 2023. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774 .
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
Yuxiong He. 2020. Deepspeed: System optimiza-
tions enable training deep learning models with over
100 billion parameters. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining , pages 3505–3506.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,
Manan Dey, M Saiful Bari, Canwen Xu, Urmish
Thakker, Shanya Sharma Sharma, Eliza Szczechla,
Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-
bajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,
Han Wang, Matteo Manica, Sheng Shen, Zheng Xin
Yong, Harshit Pandey, Rachel Bawden, Thomas
Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,
Andrea Santilli, Thibault Fevry, Jason Alan Fries,
Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao,
Thomas Wolf, and Alexander M. Rush. 2022. Multi-
task prompted training enables zero-shot task gener-
alization. arXiv preprint arXiv:2110.08207 .
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, Jonathan Tow, Alexander M. Rush,
Stella Biderman, Albert Webson, Pawan Sasanka Am-
manamanchi, Thomas Wang, Benoît Sagot, Niklas
Muennighoff, Albert Villanova del Moral, Olatunji
Ruwase, Rachel Bawden, Stas Bekman, Angelina
McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile
Saulnier, Samson Tan, Pedro Ortiz Suarez, Vic-
tor Sanh, Hugo Laurençon, Yacine Jernite, Julien
Launay, Margaret Mitchell, Colin Raffel, Aaron
Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri
Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg
Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue,
Christopher Klamm, Colin Leong, Daniel van Strien,
David Ifeoluwa Adelani, Dragomir Radev, Ed-
uardo González Ponferrada, Efrat Levkovizh, Ethan
Kim, Eyal Bar Natan, Francesco De Toni, Gérard
Dupont, Germán Kruszewski, Giada Pistilli, Hady
Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris
Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios,
Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu,
Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joy-
deep Bhattacharjee, Khalid Almubarak, Kimbo Chen,
Kyle Lo, Leandro V on Werra, Leon Weber, Long
Phan, Loubna Ben allal, Ludovic Tanguy, Manan
Dey, Manuel Romero Muñoz, Maraim Masoud,
María Grandury, Mario Šaško, Max Huang, Max-
imin Coavoux, Mayank Singh, Mike Tian-Jian Jiang,
Minh Chien Vu, Mohammad A. Jauhar, Mustafa
Ghaleb, Nishant Subramani, Nora Kassner, Nuru-
laqilla Khamis, Olivier Nguyen, Omar Espejel, Ona
de Gibert, Paulo Villegas, Peter Henderson, Pierre
Colombo, Priscilla Amuok, Quentin Lhoest, Rheza
Harliman, Rishi Bommasani, Roberto Luis López,
Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Se-
bastian Nagel, Shamik Bose, Shamsuddeen Hassan
Muhammad, Shanya Sharma, Shayne Longpre, So-
maieh Nikpoor, Stanislav Silberberg, Suhas Pai, Syd-
ney Zink, Tiago Timponi Torrent, Timo Schick, Tris-
tan Thrush, Valentin Danchev, Vassilina Nikoulina,
Veronika Laippala, Violette Lepercq, Vrinda Prabhu,
Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin
Heinzerling, Chenglei Si, Davut Emre Ta¸ sar, Eliz-
abeth Salesky, Sabrina J. Mielke, Wilson Y . Lee,
Abheesht Sharma, Andrea Santilli, Antoine Chaffin,
Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla,
Gunjan Chhablani, Han Wang, Harshit Pandey, Hen-
drik Strobelt, Jason Alan Fries, Jos Rozen, Leo
Gao, Lintang Sutawika, M Saiful Bari, Maged S.
Al-shaibani, Matteo Manica, Nihal Nayak, Ryan
Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-
David, Stephen H. Bach, Taewoon Kim, Tali Bers,
Thibault Fevry, Trishala Neeraj, Urmish Thakker,
Vikas Raunak, Xiangru Tang, Zheng-Xin Yong,
Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar
Tojarieh, Adam Roberts, Hyung Won Chung, Jae-
sung Tae, Jason Phang, Ofir Press, Conglong Li,
Deepak Narayanan, Hatim Bourfoune, Jared Casper,
Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia
Zhang, Mohammad Shoeybi, Myriam Peyrounette,
Nicolas Patry, Nouamane Tazi, Omar Sanseviero,
Patrick von Platen, Pierre Cornette, Pierre François
Lavallée, Rémi Lacroix, Samyam Rajbhandari, San-
chit Gandhi, Shaden Smith, Stéphane Requena, Suraj
Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet
Singh, Anastasia Cheveleva, Anne-Laure Ligozat,
Arjun Subramonian, Aurélie Névéol, Charles Lover-
ing, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,Ekaterina Taktasheva, Ekaterina V oloshina, Eli Bog-
danov, Genta Indra Winata, Hailey Schoelkopf, Jan-
Christoph Kalo, Jekaterina Novikova, Jessica Zosa
Forde, Jordan Clive, Jungo Kasai, Ken Kawamura,
Liam Hazan, Marine Carpuat, Miruna Clinciu, Na-
joung Kim, Newton Cheng, Oleg Serikov, Omer
Antverg, Oskar van der Wal, Rui Zhang, Ruochen
Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani
Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun,
Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov,
Vladislav Mikhailov, Yada Pruksachatkun, Yonatan
Belinkov, Zachary Bamberger, Zden ˇek Kasner, Al-
ice Rueda, Amanda Pestana, Amir Feizpour, Am-
mar Khan, Amy Faranak, Ana Santos, Anthony
Hevia, Antigona Unldreaj, Arash Aghagol, Are-
zoo Abdollahi, Aycha Tammour, Azadeh HajiHos-
seini, Bahareh Behroozi, Benjamin Ajibade, Bharat
Saxena, Carlos Muñoz Ferrandis, Danish Contrac-
tor, David Lansky, Davis David, Douwe Kiela,
Duong A. Nguyen, Edward Tan, Emi Baylor, Ez-
inwanne Ozoani, Fatima Mirza, Frankline Onon-
iwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-
tacharya, Irene Solaiman, Irina Sedenko, Isar Ne-
jadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis
Sanz, Livia Dutra, Mairon Samagaio, Maraim El-
badri, Margot Mieskes, Marissa Gerchick, Martha
Akinlolu, Michael McKenna, Mike Qiu, Muhammed
Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Ra-
jani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel,
Ran An, Rasmus Kromann, Ryan Hao, Samira Al-
izadeh, Sarmad Shubber, Silas Wang, Sourav Roy,
Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le,
Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap,
Alfredo Palasciano, Alison Callahan, Anima Shukla,
Antonio Miranda-Escalada, Ayush Singh, Benjamin
Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag
Jain, Chuxin Xu, Clémentine Fourrier, Daniel León
Periñán, Daniel Molano, Dian Yu, Enrique Manjava-
cas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,
Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec,
Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi,
Jonas Golde, Jose David Posada, Karthik Ranga-
sai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa
Shinzato, Madeleine Hahn de Bykhovetz, Maiko
Takeuchi, Marc Pàmies, Maria A Castillo, Mari-
anna Nezhurina, Mario Sänger, Matthias Samwald,
Michael Cullan, Michael Weinberg, Michiel De
Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank,
Myungsun Kang, Natasha Seelam, Nathan Dahlberg,
Nicholas Michio Broad, Nikolaus Muellner, Pascale
Fung, Patrick Haller, Ramya Chandrasekhar, Renata
Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline
Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,
Shlok S Deshmukh, Shubhanshu Mishra, Sid Ki-
blawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Ku-
mar, Stefan Schweter, Sushil Bharati, Tanmay Laud,
Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Ya-
nis Labrak, Yash Shailesh Bajaj, Yash Venkatraman,
Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli
Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and
Thomas Wolf. 2023. Bloom: A 176b-parameter
open-access multilingual language model. arXiv
preprint arXiv:2211.05100 .
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 1715–1725,
Berlin, Germany. Association for Computational Lin-
guistics.
Cheril Shah, Yashashree Chandak, and Manan Suri.
2023. The geometry of multilingual language mod-
els: A fairness lens. In The First Tiny Papers Track
at 11th International Conference on Learning Repre-
sentations, ICLR 2023, Virtual Event, Kigali Rwanda,
May 1-5, 2023 . OpenReview.net.
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas
Scialom, Anthony Hartshorn, Elvis Saravia, Andrew
Poulton, Viktor Kerkez, and Robert Stojnic. 2022.
Galactica: A large language model for science. arXiv
preprint arXiv:2211.09085 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang,
Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023.
Document-level machine translation with large lan-
guage models. arXiv preprint arXiv:2304.02210 .
Qian Wang and Jiajun Zhang. 2022. Parameter differen-
tiation based multilingual neural machine translation.
InProceedings of the AAAI Conference on Artificial
Intelligence , volume 36, pages 11440–11448.
Yining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu,
and Chengqing Zong. 2018. Three strategies to im-
prove one-to-many multilingual translation. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, Brussels, Bel-
gium, October 31 - November 4, 2018 , pages 2955–
2960. Association for Computational Linguistics.
Yining Wang, Long Zhou, Jiajun Zhang, Feifei Zhai,
Jingfang Xu, and Chengqing Zong. 2019. A com-
pact and language-sensitive multilingual translation
method. In Proceedings of the 57th Conference of
the Association for Computational Linguistics, ACL
2019, Florence, Italy, July 28- August 2, 2019, Vol-
ume 1: Long Papers , pages 1213–1223.
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2022. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 .
Wanying Xie, Yang Feng, Shuhao Gu, and Dong Yu.
2021. Importance-based neuron allocation for multi-
lingual neural machine translation. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing,
ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual
Event, August 1-6, 2021 , pages 5725–5737.
Bright Xu. 2019. Nlp chinese corpus: Large scale chi-
nese corpus for nlp.
Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,
Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong
Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,
Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang,
Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,
Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao,
Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang
Yang, Kyle Richardson, and Zhenzhong Lan. 2020.
CLUE: A Chinese language understanding evalua-
tion benchmark. In Proceedings of the 28th Inter-
national Conference on Computational Linguistics ,
pages 4762–4772, Barcelona, Spain (Online). Inter-
national Committee on Computational Linguistics.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 .
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sen-
nrich. 2020. Improving massively multilingual neu-
ral machine translation and zero-shot translation. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 1628–
1639, Online. Association for Computational Linguis-
tics.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .
Chunting Zhou, Daniel Levy, Xian Li, Marjan
Ghazvininejad, and Graham Neubig. 2021. Distri-
butionally robust multilingual machine translation.
InProceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing , pages
5664–5674.
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,
Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian
Huang. 2023. Multilingual machine translation with
large language models: Empirical results and analy-
sis.arXiv preprint arXiv:2304.04675 .
A GPT-4 prompt for Evaluating Machine Translation
Example Prompt:
You will be given two sentences, translated sentence is translated from source sentence, reference
sentence is the ground truth of translation.
Your task is to rate the translation result between translated sentence and reference sentence.
Assign a score for translation result on a scale of 0 to 5, where 0 is the lowest and 5 is the
highest based on the Evaluation Criteria.
Evaluation Criteria:
Semantic similarity refers to the measurement of how similar or related two sentences are in
terms of their meaning or semantics. It focuses on capturing the similarity in the underlying
concepts, ideas, or information conveyed by the sentences, rather than just the surface-level
lexical or syntactic similarities.
The translated sentence can completely express the meaning of the reference sentence. The
closer the translated sentence is to the reference sentence, the higher the score.
The style of the translated sentence should be as consistent as possible with the reference
sentence
Sample 1:
Translated Sentence: {}
Reference Sentence: {}
...
Sample 5:
Translated Sentence: {}
Reference Sentence: {}
Evaluation Form (Please output score ONLY):
-Overall rating
B Detailed Results on 102 Languages with BLEU
Detailed results of our evaluated models on 102 languages with BLEU are shown in Table 1.
C Detailed Results on 70 Languages with GPT-4 Evaluation
Detailed results of our evaluated models on 70 languages with GPT-4 score are shown in Table 2.
D The corresponding table of the language code and its name
Involved languages and the corresponding language codes in this paper are listed in Table 3.
Table 1: Detailed results on 102 languages with BLEU
Language pair BigTranslate ChatGPT Google Translate Language pair BigTranslate ChatGPT Google Translate
fr-en 41.4 48.8 53.8 ko-en 16.5 30.2 31.6
ca-en 41.2 43.3 52.8 hi-en 16.3 27.4 41.6
da-en 41.0 46.4 55.6 ga-en 15.7 24.2 46.3
mt-en 40.1 32.5 62.6 hi-zh 15.1 23.6 40.7
ro-en 40.1 43.3 53.5 ne-en 14.4 24.5 51.3
de-en 39.5 43.0 47.3 mg-en 13.5 8.8 38.3
pt-en 38.7 48.6 54.2 li-en 12.7 30.3 23.5
sv-en 37.3 43.8 55.2 vi-en 11.6 33.4 40.5
bo-zh 36.3 0.5 0.0 ne-zh 10.8 21.2 0.0
nb-en 35.4 37.5 46.4 he-zh 10.0 30.6 42.9
ru-en 34.5 37.9 43.4 el-en 9.2 32.3 40.8
af-en 34.4 59.5 67.2 eu-en 7.5 20.1 38.6
gl-en 32.1 37.4 48.8 ur-en 6.7 20.9 42.4
cs-en 31.4 39.6 45.2 my-zh 5.9 1.6 38.0
en-zh 31.1 46.6 54.6 my-en 5.3 1.5 38.2
es-en 30.9 32.6 36.7 gd-en 4.1 17.4 43.6
oc-en 30.9 43.4 50.8 is-en 3.2 26.1 38.9
it-en 29.4 28.8 36.7 mr-en 2.8 15.6 41.6
nn-en 29.2 43.5 48.6 as-en 2.6 13.6 34.2
nl-en 28.2 33.1 38.4 cy-en 2.4 26.0 70.2
id-en 28.1 38.5 46.9 tt-en 2.3 6.5 36.9
sk-en 27.8 38.8 46.0 be-en 2.1 15.4 27.5
sr-en 27.4 37.1 53.5 gu-en 2.0 15.6 42.9
tr-en 26.9 36.3 46.5 az-en 2.0 17.3 31.0
lv-en 26.4 36.1 48.7 xh-en 1.9 7.8 44.9
eo-en 25.8 36.6 48.5 tg-en 1.8 10.7 41.6
mo-zh 25.7 0.9 0.0 ml-en 1.7 18.5 44.0
sh-en 25.7 38.3 42.6 ha-en 1.5 4.2 35.6
lt-en 25.2 32.6 41.3 ka-en 1.3 8.6 31.0
uy-zh 25.2 19.3 48.4 kk-en 1.2 12.8 40.6
hr-en 24.9 37.8 42.6 kn-en 1.2 19.8 35.9
et-en 24.4 34.6 45.5 ps-en 1.2 4.8 34.8
uk-en 24.4 39.7 47.6 tk-en 1.2 11.5 38.1
fr-zh 24.2 38.2 43.0 hy-en 1.1 16.2 49.9
pl-en 24.2 27.8 33.0 ky-en 1.1 6.6 26.1
ms-en 23.7 41.6 50.5 th-en 1.1 23.6 33.2
pt-zh 23.6 37.6 42.2 ka-zh 0.9 9.9 36.0
es-zh 23.5 32.3 36.4 dz-en 0.9 0.2 0.0
bs-en 23.5 41.6 50.6 te-en 0.9 13.7 37.1
zh-en 23.4 29.6 38.6 yo-en 0.9 3.7 23.4
ar-en 22.1 39.1 54.1 zu-en 0.9 6.8 43.2
bg-en 21.2 38.4 45.8 fa-en 0.8 34.0 45.4
hu-en 21.0 29.9 39.0 uz-en 0.7 14.3 41.9
fi-en 20.9 31.6 41.0 ig-en 0.6 2.4 34.5
bn-en 20.5 24.6 41.3 km-en 0.6 8.3 32.9
he-en 20.5 34.8 55.1 pa-en 0.6 22.0 44.5
ko-zh 19.6 29.6 36.3 si-en 0.6 3.4 45.5
ja-en 19.4 25.3 33.7 ta-en 0.6 15.3 43.4
sl-en 18.6 28.2 37.0 yi-en 0.6 15.6 60.9
id-zh 18.4 31.5 43.3 rw-en 0.5 6.1 35.2
mk-en 17.5 37.2 52.5 am-en 0.3 2.0 47.1
sq-en 16.6 33.8 43.5 ku-en 0.3 11.8 40.2
Table 2: Detailed results on 70 languages with GPT-4 Evaluation
Language pair BigTranslate ChatGPT Google Translate Language pair BigTranslate ChatGPT Google Translate
pt-en 4.31 4.35 4.43 eo-en 3.37 3.89 4.31
fr-en 4.26 4.28 4.37 lt-en 3.37 3.86 4.11
ro-en 4.24 4.32 4.4 hu-en 3.34 4.1 4.24
zh-en 4.19 4.2 4.35 ms-en 3.32 4.14 4.28
es-en 4.14 4.18 4.18 uk-en 3.3 4.19 4.35
de-en 4.11 4.384 4.33 id-zh 3.28 3.76 3.98
pl-en 4.1 4.06 4.19 tr-en 3.27 4.03 4.19
en-zh 4.09 4.1 4.13 oc-en 3.24 3.99 4.07
ca-en 4.08 4.1 4.23 bs-en 3.21 4.21 4.24
sv-en 4.06 4.26 4.34 sr-en 3.19 4.15 4.334
da-en 4.05 4.29 4.38 bo-zh 3.0 0.12 0.0
it-en 4.05 4.18 4.14 ar-en 2.93 4.08 4.22
cs-en 4.04 4.14 4.37 bn-en 2.85 3.52 4.22
nl-en 4.04 4.23 4.23 hi-zh 2.82 3.58 3.82
ru-en 4.0 4.24 4.23 mo-zh 2.82 0.16 0.0
sk-en 3.89 4.2 4.36 sq-en 2.72 4.03 4.35
nb-en 3.89 4.17 4.26 he-en 2.68 3.95 4.226
gl-en 3.808 4.088 4.32 mk-en 2.4 4.01 4.29
fr-zh 3.8 4.03 3.86 he-zh 2.36 3.65 4.01
pt-zh 3.8 2.78 4.01 ga-en 2.29 3.54 4.18
es-zh 3.78 3.97 4.02 vi-en 2.26 4.09 4.27
af-en 3.71 4.33 4.32 mg-en 2.25 2.02 4.05
fi-en 3.67 4.22 4.22 ne-en 2.16 3.61 4.13
lv-en 3.62 3.97 4.32 eu-en 2.07 3.3 4.18
hr-en 3.61 4.14 4.18 uy-zh 2.07 1.87 4.12
nn-en 3.56 4.26 4.284 el-en 1.38 4.06 4.19
mt-en 3.54 3.5 4.332 my-zh 1.34 0.64 3.71
ko-zh 3.5 3.78 3.98 ur-en 1.18 3.7 4.14
ja-en 3.48 4.07 4.2 ml-en 0.96 3.44 4.14
et-en 3.47 4.21 4.24 uz-en 0.8 3.19 4.33
id-en 3.47 4.16 4.286 ka-zh 0.58 2.22 3.91
bg-en 3.46 4.26 4.27 tt-en 0.58 1.62 4.04
sh-en 3.46 4.15 4.222 ka-en 0.54 2.42 4.1
ko-en 3.45 4.21 4.16 tg-en 0.38 2.57 4.16
sl-en 3.41 4.03 4.22 gu-en 0.36 3.19 4.23
Table 3: The corresponding table of the language code and its name
Language code Language Language code Language Language code Language
af Afrikaans he Hebrew or Oriya
am Amharic hi Hindi pa Panjabi
an Aragonese hr Croatian pl Polish
ar Arabic hu Hungarian ps Pashto
as Assamese hy Armenian pt Portuguese
ast Asturian id Indonesian ro Romanian
az Azerbaijani ig Igbo ru Russian
be Belarusian is Icelandic rw Kinyarwanda
bg Bulgarian it Italian se Northern Sami
bn Bengali ja Japanese sh Serbo-Croatian
bo Tibetan ka Georgian si Sinhala
br Breton kk Kazakh sk Slovak
bs Bosnian km Central Khmer sl Slovenian
ca Catalan kn Kannada sq Albanian
cs Czech ko Korean sr Serbian
cy Welsh ku Kurdish sv Swedish
da Danish ky Kyrgyz ta Tamil
de German li Limburgan te Telugu
dz Dzongkha lt Lithuanian tg Tajik
el Greek lv Latvian th Thai
en Engilish mg Malagasy tk Turkmen
eo Esperanto mk Macedonian tr Turkish
es Spanish ml Malayalam tt Tatar
et Estonian mo Mongolian uk Ukrainian
eu Basque mr Marathi ur Urdu
fa Persian ms Malay uy Uighur
fi Finnish mt Maltese uz Uzbek
fr French my Burmese vi Vietnamese
fy Western Frisian nb Norwegian Bokmal wa Walloon
ga Irish ne Nepali xh Xhosa
gd Gaelic nl Dutch yi Yiddish
gl Galician nn Norwegian Nynorsk yo Yoruba
gu Gujarati no Norwegian zh Chinese
ha Hausa oc Occitan zu Zulu
