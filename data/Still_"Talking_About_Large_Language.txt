arXiv:2412.10291v1  [cs.CL]  13 Dec 2024Still “Talking About Large Language Models”: Some
Clariﬁcations
Murray Shanahan∗1,2
1Imperial College London
2Institute of Philosophy, School of Advanced Study, Univers ity of London
November 2024
Abstract
My paper Talking About Large Language Models has more than once been in-
terpreted as advocating a reductionist stance towards larg e language models. But
the paper was not intended that way, and I do not endorse such p ositions. This
short note situates the paper in the context of a larger philo sophical project that
is concerned with the (mis)use of words rather than metaphys ics, in the spirit of
Wittgenstein’s later writing.
In (Shanahan, 2024b), I wrote “[a] bare-bones LLM does not rea lly know anything
because all it does, at a fundamental level, is sequence prediction” . Looking at that
sentence in isolation, a reader might be forgiven for assuming that I am taking some
sort of reductionist stance according to which an LLM-based chat bot, such as ChatGPT,
Claude, or Gemini, is justa next token predictor, where the word “just” here carries
great metaphysical weight, and that LLM-based systems theref ore do not and cannot
have beliefs.1There are several other sentences in that paper of a similar kind, a nd with
hindsight, I wish I had taken greater care not to express myself in w ays that are so easily
open to misreading. So I am grateful for the opportunity, here, t o set the record straight.2
First, andmostimportantly, Iwouldliketomakeexplicittheoverarch ing philosophical
project I see myself as engaged in, which is very much in the spirit of W ittgenstein’s later
work, exempliﬁed by the Philosophical Investigations (Wittgenstein, 1953). Generally
speaking, I dislike all philosophical claims of the form X is Y (or X is not Y) where the
word “is” carries metaphysical weight. In my 2010 book, Embodiment and the Inner Life ,
I put the matter rather forcefully: “Such philosophically insidious us es of the existential
copula are to be banished” (Shanahan, 2010, p.106). In general, I prefer to ask questions
abouthowwordsare(orshouldbe)used. Theupshot ofthisisthat whenever Isay “LLMs
∗m.shanahan@imperial.ac.uk
1See Downes et al. (2024).
2Another example of such a sentence is “[A] great many tasks that d emand intelligence in humans
can be reduced to next-token prediction with a suﬃciently perform ant model” (Shanahan, 2024b, p.68).
It would have been better to have written “cast as next-token pr ediction” rather than “reduced to next-
token prediction”, and not to have expressed the point in a way tha t seems to preclude describing LLMs
as “intelligent”.
1
do not literally have beliefs” (or some such thing), this should be take n as shorthand for
“It is not always appropriate to use the word ‘belief’ (or its relatives ) in the context of
what an LLM says, even though it would be appropriate if a human bein g said the same
thing” (or something similar).3
Inshort, wordslike“literally”, “really”, and“just”shouldnottobe takenashallmarks
of a metaphysical pronouncement, and no sentence in (Shanahan , 2024b) that uses those
words should be taken as endorsing a reductionist view of LLM-base d systems. However,
this leaves plenty of room for debate over what constitutes appro priate versus inappropri-
ate uses of a word. My paper takes a position on this with respect to belief. The strategy
of the paper is to consider a hierarchy of increasingly sophisticated LLM-based systems,
noting that, as we ascend the hierarchy, it becomes increasingly ap propriate to speak of
belief without the need for caveats, exceptions, or clariﬁcations.
At the base of this hierarchy is what I call the “bare-bones” LLM. I n the strict sense of
the term, a “large language model” is a function that takes as input a sequence of tokens
and returns a probability distribution over tokens representing th e model’s prediction
for the next token in the sequence. This is the bare-bones LLM. It is a computational
modelofthedistributionofwordsinhumanlanguage, anditdoesn’td oanything untilitis
embeddedinalargersystem, suchasachatbotapp. Confusingly, t hough,incontemporary
usage, the term “large language model” or LLM is also used for these larger systems.
Hence, people refer to ChatGPT as an LLM, when strictly speaking it is an application
built around the core component of an LLM.
While it seems reasonable to allude to the knowledge encoded in a bare- bones LLM,
I do think it is misleading to speak of the beliefs of a completely passive e ntity. To my
mind, the very idea of belief is bound up with behaviour. That is to say, the original
context for using the word “belief” – its natural home, so to speak – is living, behaving,
active human beings (and other animals), and to use it for a complete ly passive, inactive,
computational entity is to depart too far from the word’s original h ome for comfort. But
the bare-bones LLM is hardly an interesting case.
Farmoreinteresting thanthebare-bonesLLMisthe simple LLM-bas edconversational
agent.4We obtain one of these by embedding the bare-bones LLM in an inner lo op that,
given the transcript of the conversation so far, repeatedly samp les from the distribution
output by the model to obtain a sequence of words (the agent’s re sponse), and an outer,
turn-taking loop that alternates between the user’s input and the agent’s replies. Now
we have a system, based on an LLM, that actually does something, a nd we can speak
of its behaviour. Moreover, we have moved a little closer to the natu ral home of the
word “belief”. Now suppose the resulting system is very convincing. It is, let us say, a
human-level conversationalist. Is it appropriate to use the word “ belief” in its full sense,
without the need for caveats, exceptions, or clariﬁcations? I don ’t think so. Not in its
full sense. Not at this level in the hierarchy of systems we are asce nding.
On the one hand, it’s perfectly natural to speak loosely of such an a gent’s beliefs. I
might say to a colleague, for example, “Oh, ChatGPT knows you’re a c omputer scientist,
but it thinks you wrote a paper I’ve never heard of”. In the spirit of Dennett’s intentional
stance (Dennett, 2009), this way of talking helps to make sense of the subsequent con-
3In (Shanahan, 2024a), I take this approach to consciousness, a far trickier case than belief.
4The word here “agent” is itself philosophically fraught. By my own light s, I should perhaps be
more cautious in its use. But in the ﬁeld of AI, the term is used in a lightw eight technical sense to
mean “anything that can be viewed as perceiving its environment thr ough sensors and acting upon that
environment through actuators”(Russell and Norvig, 2010, p.24 ), where the environment in question can
be a purely textual interface with a human user.
2
versation, and is easier to say than “ChatGPT’s weights predispose it to emit the string
XYZ when prompted with the string ABC”. On the other hand, such a n agent “cannot
participate fully in the human language game of truth because it does not inhabit the
world we human language users share” (Shanahan, 2024b, p.73). W e cannot, for exam-
ple, ask a simple LLM-based conversational agent whether the pot is full of water. That
pot. Look. That one over there. Go and have a look. Is it full or is it e mpty? The
simple LLM-based conversational agent cannot wander over ther e, peer down, ascertain
the status of the pot, and report back to us. Yet this is the sort o f primal scene – a scene
wherein a person adjusts what they do and say after engaging with the world and ﬁnding
something out – that I see as the original home of the word “belief” a nd its relatives.
However, as we move up the hierarchy of LLM-based systems, laye ring on more ca-
pabilities, the need for caution in using the word “belief” gradually less ens. First, we
can consider multi-modal LLMs capable of taking visual as well as tex tual input. Then
we can consider LLMs capable of a wider range of actions than merely issuing textual
output, a range that could include retrieving web pages or running P ython code, for ex-
ample. Finally, we can consider embodied (or virtually embodied) LLM-b ased systems,
which take input from a camera mounted on a robot or on an avatar in a 3D games-like
environment, and whose repertoire of actions includes controlling a robot’s eﬀectors or an
avatar’s movements.
With each of these steps, we move closer to the natural home of th e word “belief” and
itsrelatives, which isinpredicting, explaining, andingeneralmaking se nse of, theactivity
of embodied creatures like ourselves, creatures whose behaviour changes in response to
what we ﬁnd out by interacting with the world and the objects it cont ains. That is the arc
of my original paper. The intent was not to take up a metaphysical p osition with respect
to belief, nor to bolster deﬂationary views of LLM capabilities based o n such positions.
The aim, rather, was to remind readers of how unlike humans LLM-ba sed systems are,
how very diﬀerently they operate at a fundamental, mechanistic lev el, and to urge caution
when using anthropomorphic language to talk about them.5
References
D. Dennett. Intentional systems theory. In The Oxford Handbook of Philosophy of Mind ,
pages 339–350. Oxford University Press, 2009.
S. M. Downes, P. Forber, and A. Grzankowski. LLMs are not just n ext token predictors.
Inquiry, forthcoming, 2024. Also arXiv:2408.04666.
S. Russell and P. Norvig. Artiﬁcial Intelligence: A Modern Approach . Prentice Hall, 2010.
Third Edition.
M. Shanahan. Embodiment and the Inner Life: Cognition and Consciousness in the Space
of Possible Minds . Oxford University Press, 2010.
M. Shanahan. Simulacra as conscious exotica. Inquiry, 2024a. https://doi.org/10.
1080/0020174X.2024.2434860 .
M. Shanahan. Talking about large language models. Communications of the ACM , 67
(2):68–79, 2024b.
5By way of counterpoint, (Shanahan et al., 2023) suggests that we can use anthropomorphic language
with a degree of impunity if we frame LLM behaviour in terms of role play .
3
M. Shanahan, K. McDonell, and L. Reynolds. Role play with large langua ge models.
Nature, 623:493–498, 2023.
L. Wittgenstein. Philosophical Investigations . Basil Blackwell, 1953.
4
