JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1
Model Compression and Efficient Inference for
Large Language Models: A Survey
Wenxiao Wang†, Wei Chen†, Yicong Luo†, Y ongliu Long†, Zhengkai Lin†, Liye Zhang†, Binbin Lin, Deng
Cai, Xiaofei He Senior Member, IEEE
Abstract —Transformer based large language models have achieved tremendous success. However, the significant memory and
computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained
devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic
perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can
still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language
models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even
retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning
or training. Therefore, many algorithms for large models, such as quantization and pruning, start to explore tuning-free algorithms. (2)
Large models emphasize versatility and generalization rather than performance on a single task. Hence, many algorithms, such as
knowledge distillation, focus on how to preserving their versatility and generalization after compression. Since these two characteristics
were not very pronounced in early large models, we further distinguish large language models into medium models and “real” large
models. Additionally, we also provide an introduction to some mature frameworks for efficient inference of large models, which can
support basic compression or acceleration algorithms, greatly facilitating model deployment for users.
Index Terms —Large language models, model compression, efficient inference, quantization, pruning, knowledge distillation, compact
architecture design, dynamic networks.
✦
1 I NTRODUCTION
LARGE language models (LLMs) has become an im-
portant and popular topic in the artificial intelligence
field. Compared with previous language models, LLMs ( e.g.,
ChatGPT, LLaMA, Claude) show much greater generaliza-
tion capability for their unseen data. Furthermore, they even
present many abilities that smaller models do not present
(i.e., emergent abilities), such as multi-step reasoning and in-
struction following abilities. These progresses demonstrate
great potentials of LLMs.
However, the forbidding memory and computational
budgets in the inference process also prevent the deploy-
ment of LLMs. For example, a 10B models with float32
weights consumes 37GB memory, needless to say that the in-
ference memory cost will further increase in a speed square
to the sequence length. To deploy the models on resource
constrained devices, or even mobile devices, many LLMs
resort to model compression methods such as quantization
to reduce the inference memory and computational cost.
Model compression for deep learning models is a field
that appear much earlier than LLMs. It assumes that we
have already a pre-defined (or even pretrained) model.
Model compression devotes to reducing the memory and
•†indicates equal contributions with specific .
•Wenxiao Wang, Yicong Luo, Yongliu Long, and Binbin Lin are with the
School of Software Technology, Zhejiang University.
•Liye Zhang is with School of Mathematical Sciences, Zhejiang University.
•Wei Chen, Zhengkai Lin, Deng Cai, and Xiaofei He are with State Key
Lab of CAD &CG, Zhejiang University.
Manuscript released in February 10, 2024.computational cost of the model in the inference process, so
that the model can run on various resource-constrained de-
vices. Algorithmically, common model compression meth-
ods include:
•Quantization transforms float32 weights or activa-
tions into lower-bit float or integer numbers. Less bits
means less memory requirement. Further, less bits
may indicate higher parallelism and faster inference
speed.
•Pruning devotes to removing unimportant compo-
nents ( e.g., neurons, layers, etc) in a pre-designed
model, thus reducing the memory and computa-
tional cost in the inference cost.
•Knowledge distillation introduces a pretrained
large model as a teacher and transfers its knowledge
to a new smaller model which is called a student
model. Then, the smaller model will share nearly the
same ability as the teacher and enjoy less memory
and computational cost.
•Compact architecture design designs new operators
with less cost to replace (often approximate) the
cumbersome operators in the original model. For
the Transformer models, self-attentions are the main
targets and are often replaced with other operators.
•Dynamic networks treat each inference sample dif-
ferently. The original model is a super-net, and each
sample only selects a sub-structure of the super-net
for inference. Mixture of experts (MoE) is a kind of
dynamic inference.
Besides, the above methods can also be combined forarXiv:2402.09748v1  [cs.CL]  15 Feb 2024
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2
further compression and speedup. Existing compression
methods have provided us with important cornerstones
and insights to compress LLMs. However, LLMs also bring
many new challenges for model compression:
1) Many previous model compression methods of-
ten require to finetuning models after compres-
sion. However, since the great budget to finetuning
LLMs, researchers have to explore finetuning free
or, at least, more efficient finetuning methods.
2) Instead of handling one single task such as neural
machine translation, large language models emph-
size versatility and generalization across various
tasks and unseen data, or even emergent abilities.
Thus, large language models after compression re-
quire more careful validation of their versatility and
generalization.
To face these challenges, many compression methods
specialized for LLMs are proposed. In this paper, we will
give a comprehensive survey about these methods. To better
present these methods, we further isolate language models
around one billion or less parameters, such as BERT, GPT-
2, and call them medium models, though they are usually
taken as large language models. And Models with over
one billion parameters, such as LLaMA, Claude, ChatGPT,
and etc, keep the name of large language models. The
reasons is that medium models are less affected by the above
two challenges, i.e., medium models are relatively easy to
finetune, demonstrate fewer emergent abilities. As a result,
many compression methods for medium models are still
similar to those for smaller models.
The following sections are organized as: Some prelimi-
naries will be introduced in Section 2. Then, we will discuss
pruning, knowledge distillation, quantization, compact ar-
chitecture design and dynamic networks in Section 3, 4, 5,
6, 7, 8, respectively.
2 P RELIMINARIES
In this section, we will introduce some essential preliminar-
ies about Transformer, large language models, parameter-
efficient training, and etc.
2.1 Transformer
Transformer is first introduced in [1], which is employed
for machine translation initially. Its fundamental structure is
depicted in Fig. 1. The input (a sentence) is often projected
through an embedding layer to a sequence of vectors (called
tokens) for a Transformer’s input. Each Transformer block
consists of an attention module and a multi-layer preceptron
(MLP) module.
Attention. For each token in the input sequence, it is first
mapped (often with a linear function) into vectors of query
(Q) and/or key-value pairs ( KandV). Then, the attention
module can be described as mapping a query and a set of
key-value pairs to an output. The output is computed as
a weighted sum of the values, where the weight assigned
to each value is computed by a compatibility function of
Fig. 1: The Transformer architecture drawn from [1].
the query with the corresponding key. The most common
attention module is scaled dot-product function:
Attention( Q, K, V ) = softmax(QKT
√dk)V, (1)
where the weight is computed through the dot-product of
QandK, and√dkis a constant scaling factor.
Multi-head Attention. Further, instead of performing
a single attention function with keys, values and queries,
Transformer employs a multi-head attention [1], as shown in
Fig. 1. It maps input tokens into hdistinct queries, keys and
values ( {Qi, Ki, Vi|i∈[1, h]}) with different linear layers.
Then, the final output becomes:
Muti-Head Attention = Concat(head 1,···,head h)Wo
head i= Attention( Qi, Ki, Vi),
(2)
where Wois a linear projection matrix.
Encoder and docoder. The first intial Transformer is
for neural machine translation, which employs an encoder-
decoder structure. The encoder first handles input sequnce
(e.g., wirtten in the source language) independently, and the
decoder takes the encoder’s output as input and predicts the
final output ( e.g., the target language). There are two core
differences between an encoder’s and a decoder’s attention
module: (1) The encoder employs a full attention, where
any two tokens in the input sequence are visible to each
other. On the other hand, the decoder employs a single-
direction attention. The reason is the decoder generates out-
put tokens one by one, and each token can only seen output
tokens before itself. (2) The encoder employs a self-attention
module, that is, Q, K, V are all from input tokens of the
source language. In contrast, the decoder employs a cross-
attention, where K, V are from the encoder’s output, while
Qis last output token of the decoder. As the development, in
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3
addition to the encoder-decoder models ( e.g., T5 [2]), plenty
of following language models also employ pure encoder
structure ( e.g., BERT [3]) and pure decoder structure such
as GPT-series [4], [5], [6] models.
While we brifly introduce some important concepts in
Transformer, more subtle introduction can be seen in many
previous surveys [7], [8].
2.2 Medium/Large Language Models
As the success of Transformer, more and more pure Trans-
former based language models emerge, and the parame-
ters of models also increase. Though there is no specific
threshold for large language models’ scale of parameter, it
is commonly accepted that “large” language models can
be dated back from BERT [3] and GPT-1 [4], which are
both proposed in 2018. Their scales of parameter both reach
several hundred million.
After that, more language models such as GPT-3 [9],
PanGu [10], T5 [2], CPM-2 [11], BLOOM [12], OPT [13],
GLM [14], PaLM [15], QWen [16], ERNIE [17], LLaMA [18],
and etcare proposed. Besides scale of parameter, the most
significant property that distinguish these models from the
previous are their emergence. As proposed in [19], large
language models utilize large-scale self-supervised pretrain-
ing to enable the models with many abilities ( i.e., emergent
abilities) that do not appear in smaller models, including
multi-step reasoning, instruction following, program exe-
cution abilities, and etc. For example, GPT-3, LLaMA and
many other LLMs can solve few-shot tasks through in-
context learning, and even zero-shot tasks. The breakout
of large language models present the surprising abilities
(called emergence) in solving a series of complex tasks over
smaller models.
To further emphasize this difference, we catorgorize
language models over hundred millions of parameters into
medium models and large models. Specifically, models with
around one billion or less parameters are called medium
models, while those with over one billion parameters are
called large models.
2.3 Parameter-efficient Finetuning (PEFT)
As we discussed in the introduction, many model compres-
sion algorithms such as knowledge distillation and pruning
require finetuning or even training for accuracy recovery
after compression. Nevertheless, full-parameter finetuning
or training is very costly for medium or large models. To this
end, many parameter efficient finetuning (PEFT) algorithms
are proposed. They devote to finetune as few parameters
or epochs as possible to lower the finetuning cost. In this
paper, model compression and acceleration algorithms in
the inference stage (rather than training/finetuning stage)
are mainly discussed, but we still supplement some PEFT
algorithms in the Appendix A.
3 Q UANTIZATION
Quantization refers to the process of mapping input values
in a large (often continuous) set to output values in a small
(often finite) set (see Fig. 2 for an example). It is the most
straightforward method to cut down memory cost andimprove inference speed for LLMs, especially on hardware
with support for fast operation of low-bit datatypes ( e.g.,
INT4). It should be noted that quantization has achieved
impressive success in both neural network training [20] and
inference, while the focus of this survey is only the inference
part.
Quantization methods have several advantages over
other compression methods, such as pruning and distilla-
tion. 1) High compression ratio : quantizing the weights in
LLMs from 32-bit float to 4-bit integer could drastically
compress the model size to approximately 1/8, essential for
memory-bound1processes like LLM inference. 2) Low cost :
a bunch of quantization methods doesn’t require re-training
the entire LLMs, making it more affordable to researchers
with limited computing resources. 3) High flexibility : quanti-
zation is compatible with most other compression methods,
which introduces exceptional opportunities for further im-
proving the performance.
To help readers better understand quantization methods,
we will first introduce the standard quantization method
and some basic concepts in Subsection 3.1. Then, in Section
3.2, we will briefly summarize some of the most important
works for medium-size Language models ( e.g., BERT, GPT-
2,etc.) before the emergence of LLMs. Section 3.3 and
Section 3.4 covers recent advances in quantization methods
that focus on LLMs inference. Considering the pains in re-
training a model with tens of billions of parameters, we
generally divide LLM quantization methods into two parts
based on whether the technique needs re-training. Methods
without re-training ( i.e.,post-training quantization, PTQ ) are
discussed in Section 3.3 while methods that demand re-
training ( i.e.,quantization-aware training, QAT ) is discussed in
Section 3.4. Finally, in Section 3.5, we discuss some advanced
topics showing potential for future research but not covered
in previous sections.
3.1 Basic Concepts
Quantization has a history that is much longer than neural
networks, and specific quantization methods vary a lot. To
give readers a clear grasp of the diverse quantization con-
cepts, we will first introduce the standard uniform quanti-
zation and the corresponding dequantization process. After
that, we explain several fundamental concepts frequently
mentioned in different quantization methods.
1)Uniform quantization. The most basic form of quanti-
zation is to separate a real-valued range into uniform, finite
intervals ( e.g.,2bintervals for b-bit integer) and then map
real values within the same interval to the same integer. The
formula of such a process is as follows:
Q(r) = ROUND(r
S) +Z (3)
where Q(·)is the quantization operator, ris the real values
to be quantized, Sis a real valued scaling factor, Zis an
integer zero point and ROUND( ·)is a rounding operation
(e.g., round to nearest). This method is known as uniform
1. ”memory-bound” means that the transfer between the device and
global memory nearly reaches the limitation or fetching data from the
memory is the bottleneck of the whole process. On the contrary, the
”compute-bound” process spends most of its time calculating and not
accessing memory.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4
-7-6-57…6-4-3-2-1Clipping range [𝛼,𝛽]FP16INT4Scaling factor 𝑆minmax
(a) Illustration of uniform quantization process
1.2-0.54.3-3.10.82.45.4Get clipping range [𝛼,𝛽]5.4Get scaling factor 𝑆1.32-1       6  -4    1         3       7
1.5-0.84.6-3.10.82.35.41.3Rescale to [−7,7]Divide by 𝑆FP16 TensorQuantized –INT4 TensorDequantized –FP16 Tensor
(b) Quantization and De-quantization of a FP16 tensor
Fig. 2: (a) Uniform quantization separates a real-valued
range into uniform, finite intervals and then maps real values
within the same interval to the same integer. (b) An FP16
tensor is quantized into INT4 format and then dequantized
back into FP16.
quantization since the length of each interval is the same
(equal to scale factor S) and the quantized values are
uniformly spaced ( e.g., integers 0,1,2, . . .).
The operation to recover real values from quantized
values is called dequantization :
˜r=S·(Q(r)−Z) (4)
It should be noted that the recovered values ˜rmay be dif-
ferent from the original real values rdue to the information
loss introduced by ROUND( ·)function.
2)Non-uniform quantization. The counterpart of uniform
quantization is non-uniform quantization , where quantized
values are not necessarily uniformly spaced, and the length
of intervals can be different. The general formula for non-
uniform quantization is:
Q(r) =Qi,ifr∈[∆i,∆i+1) (5)
where Qiis the candidate quantized values called quanti-
zation levels ,∆iand∆i+1defines an interval in which real
values would be mapped to corresponding Qi.
Given a fixed bit-width, non-uniform quantization can
often achieve higher accuracy and lower quantization er-
ror than its uniform counterpart, as the weights of neural
networks are generally not uniformly distributed. How-
ever, non-uniform methods may suffer low computation
efficiency because these methods usually involve a time-
consuming lookup operation that is not directly compatible
with parallel computation hardware like GPU.
3)Clipping range and calibration. An important factor
associated with uniform quantization is clipping range [α, β]
so that real values lower than αor higher than βwouldbe clipped to αorβrespectively. The clipping range also
directly influences the scaling factor Sin uniform quantiza-
tion:
S=β−α
2b−1(6)
In general, a wider clipping range results in fewer outliers
requiring clipping in the input data. However, this comes
at the cost of a larger scale factor, as shown in Equation (6).
Consequently, the quantization error for real values within
the clipping range would also be larger.
Choosing the clipping range is referred to as calibration .
Common choices of calibration involve using min/max
values ( i.e.,−α=rmin, β=rmax), using absolute max
values ( i.e.,−α=β=max(|r|)) or minimizing the infor-
mation loss ( i.e., KL divergence) between the real values and
quantized values.
4)Symmetric/Asymmetric quantization. When the clipping
range [α, β]is symmetric with respect to 0(α+β= 0 and
Z= 0), then corresponding method is often referred to as
symmetric quantization ; otherwise asymmetric quantization .
The clipping range for asymmetric quantization is
more compact, which is especially important for activa-
tions in neural networks whose range may be signifi-
cantly imbalanced ( e.g., after ReLU, all activations become
non-negative). Symmetric quantization, however, is more
compute-efficient at the dequantization step.
5)Quantization Granularity. A categorization criterion
specified for quantization of neural network is the granu-
larity , which corresponds to which weights/activations are
quantized together and share quantization parameters. We
list typical granularity from coarse to fine as follows:
•Layer-wise : Weights of all filters in a layer for con-
volution layers or the full weight matrix for linear
layers are quantized together.
•Channel-wise : Weights of a single filter for convolu-
tion layers are quantized together.
•Row/Column-wise : Weights of a single row/column
of weight matrix for linear layers are quantized to-
gether.
•Token-wise : Activations for each token are quantized
together.
•Group-wise : Several consecutive real values in
weights or activations are viewed as a group and
quantized together. The group size is typically small
(e.g.,64consecutive real values).
In general, finer granularity divides model weights or acti-
vations into smaller groups, which can reduce quantization
errors. However, finer granularity requires storing more
quantization parameters and introduces higher quantization
overhead during computation.
Note that different works may use different notations.
For clarity, in this survey, we set input X∈RN×Din, weight
matrix W∈RDin×Dout, where Nis the batch size, Dinand
Doutare input and output dimensions respectively. A linear
layer can be written as Y=XW∈RN×Dout.
6)Post-Training Quantization/Quantization-Aware Train-
ing. An effective way to reduce quantization error is
through training. Quantization methods can either require
re-training after quantization or not:
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5
•Post-Training Quantization, PTQ : methods without re-
training, quantized models can be directly used in
inference.
•Quantization-Aware Training, QAT : methods with re-
training, which helps recover the error introduced by
quantization.
Perturbation to parameters of a trained model caused by
quantization may push the model away from the point
where it had converged in floating-point-precision train-
ing. QAT addresses this issue through either simulating
the quantization process in re-training ( e.g., inject nodes in
the computation graph that simulate weight quantization)
or using additional parameters to finetune the quantized
model ( e.g., combined with Adapters or LoRA) so that the
model can learn to converge to a point which will have a
better performance after quantization. However, re-training
of full LLM with billions of parameters has an unbearable
cost to most researchers. Hence, PTQ methods are also
extensively studied to achieve better performance without
introducing additional computation budgets through spe-
cific non-training methods ( e.g., Optimal Brain Surgery).
Generally speaking, the rounding operation in quanti-
zation is a non-differentiable process, so QAT may seem
impossible at first sight. However, researchers found that
a simple method called Straight Through Estimator (STE)
works well under most circumstances, ignoring the round-
ing operation and approximating it with an identity func-
tion.
7)Static/Dynamic quantization. A key difference between
quantizing the weights and activations of a neural network
lies in the fact that weights are mostly fixed during inference
so that the statistics used in clipping range calibration can
be computed statically. It’s not the case for activations since
its range and statistics are unknown until runtime. Thus,
activation quantization can be divided into two categories.
•Static quantization refers to the methods whose quan-
tization parameters are pre-calculated before infer-
ence using some calibration inputs to find typical
activation statistics or learned jointly during neural
network training.
•Dynamic quantization calculates quantization parame-
ters dynamically at runtime, which is generally more
accurate than its static counterpart but can have high
overhead for calculating required statistics ( e.g., min,
max, etc.).
8)Simulated/Integer-Only quantization. Still, another cat-
egory criterion is whether quantized values are used for
actual operations ( e.g., matrix multiplication). For simulated
quantization (also called fake quantization ), the weights and
activations are only stored in low-bit datatypes ( e.g., INT4)
and have to be dequantized to high-bit datatypes ( e.g., float16)
to carry out actual operations. For Integer-only quantization ,
the operations can be carried out using low-bit datatypes.
Simulated quantization can reduce the memory cost and
data-moving time of neural networks, which is helpful since
several works have shown that LLM inference is memory-
bound rather than compute-bound1. In contrast, integer-only
quantization can further enjoy the acceleration of efficient
low-bit operations supported by specific hardware.9)Weight-only/Weight + Activation quantization. Whether
the quantization objective is only weights or both weights
and activations. Previous work [21] found that activation
quantization is generally more susceptible to weight quan-
tization, so weight-only quantization can reach lower bit-
width. However, quantized weights must be dequantized
before multiplying with activations, so weight-only quanti-
zation can not be integer-only and will introduce additional
computation overhead during inference.
We’ve briefly covered some of the most essential con-
cepts in quantization. These concepts are universally ap-
plied to all neural network quantization, and each specific
method may suit several different concepts ( e.g., auniform
symmetric dynamic layer-wise simulated quantization method
for LLMs). We categorize the main quantization methods for
LLMs according to these basic concepts in TABLE 1.
3.2 Quantization Methods for Medium-Size Language
Models
For ease of expression, we refer to models with sizes smaller
than or close to 1B as medium-size language models, repre-
sented by BERT, GPT-2, and BART.
Quantization methods for medium-size language mod-
els [22] mainly adopt the QAT framework instead of PTQ,
as the cost of re-training such a model is relatively accept-
able. The improvement in evaluation metric ( e.g., accuracy)
brought by re-training is significant, especially under ex-
treme low-bit settings ( e.g.,1-bit or 2-bit quantization). As a
result, we will first introduce mainstream methods, i.e., QAT
methods, for medium-size language models and then cover
the PTQ methods.
3.2.1 QAT for Medium-Size Language Models
Early works aim at quantizing weights of BERT-like models
into INT8. Q8BERT [23] applies the basic QAT framework
from [24] to quantize both weights and activations of BERT
into 8-bits without significant reduction in model perfor-
mance.
Some works enable quantization into bit-width lower
than 8-bit using more complicated methods [25], [26], [27],
[28], [29], [30], [31]. For example, Q-BERT [25] maintains
8-bit activations and mixed-precision weights down to 2/3-
bits. It uses the Hessian matrix to determine the bit-width
for weights of each layer so that more aggressive quantiza-
tion is performed for layers with smaller top eigenvalues
of the corresponding Hessian. Further, TernaryBERT [27]
restricts its weights to -1, 0, +1, using only 2 bits, and em-
ploys 8-bit activations. Knowledge distillation is adopted to
overcome performance degradation by minimizing the sum
of mean-square error (MSE) of the activations and attention
scores between the original and quantized model. Following
TernaryBERT, BinaryBERT [32] pushes the limit of BERT
quantization to weight binarization, i.e., restricts weights
in{−α,+α}. The authors propose to initialize BinaryBERT
by equivalently splitting from a half-sized TernaryBERT to
inherit the good performance of the ternary one. In addition,
BiBERT [28] is a full binarization of BERT ( i.e., 1-bit weight,
embedding, and activation). The authors identify the severe
performance degradation of the full binary model comes
from information degradation and optimization direction
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6
mismatch. A Bi-Attention structure and a DirectionMatch-
ing Distillation (DMD) scheme are proposed accordingly to
retain most of the ability of the original BERT.
Some works enable an automatic balance between
the model performance degradation and quantization bit-
width. Zhao et al. [29] leverages a Differentiable Neural Ar-
chitecture Search approach to assign precision for parameters
automatically. In detail, the weights and the bit assignment
of weights are optimized alternatively under an objective
function that combines the cross entropy loss with the
penalty of model size. The optimization process aims to
obtain a set of bit assignments for each group of parameters
close to optimal.
3.2.2 PTQ for Medium-Size Language Models
PTQ methods are carefully designed so they generally do
not require extra finetuning or re-training to compensate for
quantization errors. GOBO [33] quantizes the vast majority
of weights that comply with Gaussian distribution into 3 bits
using non-uniform quantization ( i.e., clustering) and saves a
few outlier weights separately in FP32. I-BERT [34] designs
integer-only approximation methods for specific non-linear
functions ( e.g., GeLU, Softmax, LayerNorm) to enable end-
to-end integer-only BERT inference without any floating
point calculation. Dai et al. [35] use finer granularity to
reduce quantization error. In detail, the authors quantize
weights and activations into 4 bits using group-wise quan-
tization ( e.g.,16∼64consecutive weights as a group). A
calibration set is used to determine the scaling factor for
each group.
Furthermore, it should be noted that the quantization
parameters obtained by elaborately tailored PTQ methods
can, in general, be a good initialization point for re-training
in QAT methods.
3.2.3 Quantize Generative Medium-Size Language Models
Despite the success of quantization approaches for BERT-
like models mentioned above, attempts to quantize gener-
ative language models ( e.g., GPT, BART) was scarce before
the emergence of generative LLMs [36]. The critical differ-
ence is that quantization error accumulates in the token-by-
token generation process, so quantizing generative language
models is generally a more complex problem.
According to Tao et al. [37], applying quantization meth-
ods that are designed for BERT-like models directly to
generative language models is hindered by homogeneous
word embedding and varied distribution of weights. Homo-
geneous word embedding refers to the problem where the
word embeddings of generative language models become
less distinguishable from each other after quantization. On
the other hand, varied distribution of weights means that
the weights of the model are highly skewed with outliers. To
tackle these challenges, the authors propose two solutions:
token-level contrastive distillation and module-dependent
dynamic scaling. DQ-BART [38] uses the QAT framework
and a distillation training objective to distill and quantize a
sequence-to-sequence model, i.e., BART, jointly. DQ-BART
adopts the standard symmetric uniform quantization as
shown in Equation (3) and sets the training objective as
minimizing the differences of the output logits, attentions,
and hidden states between the quantized and distilledlow-precision student model and the full precision teacher
model.
In this section, we only briefly cover the most impor-
tant works done on medium-sized language models. For a
more detailed summarization of quantization methods for
medium-sized language models, we refer interested readers
to [39], [40].
3.3 Post-Training Quantization for LLMs
The past few years have witnessed a remarkable surge in
post-training quantization methods (PTQ) for LLMs. This is
partly because PTQ doesn’t involve LLMs’ prohibitively ex-
pensive re-training process, so it’s a more feasible direction
for most researchers.
Further, we roughly divide PTQ works for LLMs into
two categories: weight-only quantization and weight + ac-
tivation quantization . We’ll discuss works related to these
categories respectively in the following parts.
3.3.1 Weight-Only Quantization.
In this part, we focus on the problem of only quantizing the
weights (but notthe activations) of LLMs. Generally, weight-
only quantization belongs to simulated quantization meth-
ods; the weights are only stored in low-bit datatype and
need to be dequantized before real computation. This means
such methods can decrease the overall size of LLMs and
decrease the time to move weights between memories but
cannot enjoy the accelerated low-bit operation supported by
specific hardware.
While the previous subsections have discussed various
methods that can be used to quantize medium-size language
models, LLMs present additional challenges due to their
unique characteristics. These challenges include:
1) LMs rely heavily on memory during the inference
process, especially when the inference batch size
is small [49]. This makes it crucial to minimize
memory usage and optimize data transfer between
different storage devices.
2) The activation patterns of LLMs are distinct, which
poses a challenge when applying quantization
methods that work well for medium-sized language
models. Systematic outliers [43] are one such unique
property of LLM activations that hinder the direct
application of such methods for weight-only quan-
tization of LLMs.
Some works directly apply uniform, round-to-nearest
quantization to LLMs with minor modifications [14], [21],
[50]. ZeroQuant-V2 [21] quantize OPT and BLOOM. It
shows that using 16-bit activations and directly quantiz-
ing weights of these models to 8-bit integers using row-
wise symmetric quantization results in negligible perplexity
degradation, while 4-bit weight-only quantization witnesses
a significant performance drop. To further push the limit of
low-bit quantization, ZeroQuant-V2 [21] propose the Low-
Rank Compensation (LoRC) method, which approximates
the error Ebetween the original weight matrix Wand the
quantized weight matrix ˆWusing a storage-efficient low-
rank matrix ˆEso that ˆW+ˆEwould be a better approx-
imation of the original weight W. However, Zeng et al.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7
TABLE 1: Detailed category of several strong baseline quantization methods for LLMs. ✓means that a quantization method
belongs to a specific category, ×vice versus, and ◦means a quantization method can be used in both circumstances. For
methods that work in different bit widths, we report the lowest effective bit width. For a detailed explanation of each
category, please refer to Subsection 3.1
Method #Bits Weight Activation Uniform Symmetric Static Re-Training Zero-shot Integer-Only
AWQ [41] 3 group-size − ✓ × ✓ × × ×
OPTQ/GPTQ [42] 3 column-wise − ✓ × ✓ × × ×
LLM.int8() [43] 8 column-wise row-wise ✓ ◦ × × ✓ ✓
ZeroQuant [44] 4 group-wise tokenwise-wise ✓ ✓ × ◦ ◦ ✓
SmoothQuant [45] 8 layer-wise layer-wise ✓ ✓ ◦ × × ✓
LLM-QAT [46] 2 column-wise tokenwise-wise ✓ ✓ × ✓ × ×
INT2.1 [47] 2 column-wise − ✓ × ✓ ✓ × ×
QLoRA [48] 4 column-wise − × ✓ ✓ ✓ × ×
[14] found that GLM-130B can be directly quantized into
4-bit weights using row-wise symmetric quantization with
negligible performance degradation, which is evaluated by
zero-shot accuracy on the LAMBADA dataset. The authors
ascribe the appealing 4-bit quantization property of GLM-
130B to its weight distributions being well-shaped and
not skewed compared to GPT-style models like OPT and
BLOOM.
Another line of research considers non-uniform methods
in weight-only quantization of LLMs. The critical insight
lies in the fact that the weight distribution of LLMs after
training is non-uniform, so it makes sense to let interval
[∆i,∆i+1)in Equation (4) also be non-uniform to push the
quantization even to lower bit-width. LUT-GEMM [51] (also
known as nuQmm) extends a non-uniform quantization
method, binary-coding quantization (BCQ) [52], which fac-
torizes the full-precision parameters into binary parameters
and a separate set of scaling factors. The authors add a bias
term to conventional BCQ methods to increase the represen-
tational capacity and use group-wise quantization to enable
a tradeoff between the compression ratio and model per-
formance. SqueezeLLM [49] verifies that the LLM inference
is memory-bound with extremely low arithmetic intensity
relative to other neural networks. Besides, SqueezeLLM
adopts sensitivity-based k-means centroids as the quantized
weight values for non-uniform quantization (see Xiin
Equation (5)). The sensitivity-based k-means method ap-
proximates the Hessian matrix of weights as the sensitivity,
highlighting the importance of minimizing perturbations for
weights with large Hessian values. SqueezeLLM has better
perplexity than standard uniform quantization methods
while achieving around 2×speedup compared to the FP16
baseline. Dettmers et al. [48] propose a new NormalFormat
(NF) datatype, which can also be viewed as non-uniform
quantization. The NF datatype builds on Quantile Quan-
tization [53], an information-theoretically optimal data type
that ensures each quantization interval has an equal number
of values assigned from the input tensor. The authors utilize
that pre-trained neural network weights usually have a
zero-centered normal distribution with standard deviation
σ, thus can be normalized to the standard normal distri-
bution N(0,1)by scaling σ.k-bit NormalFormat use k-
bit Quantile Quantization on standard normal distributionN(0,1)to find its quantized values Qi(See Equation (5)
for definition of Qi). In practice, weights to be quantized
are rescaled to range [−1,1]and then round to nearest
quantized values Xii.e., round-to-nearest (RTN) methods.
Above are zero-shot methods that only consider the min-
imize the difference between the original weight matrix W
and the quantized weight matrix Q(W),i.e., to minimize the
quantization error of weight matrix argmin ˆW||W−Q(W)||.
However, considering the high non-linearity of neural net-
works, a small distance in weight space doesn’t necessarily
mean a small difference between the output of the original
and quantized models. Thus, if given a small set of typical
examples C, called calibration set , there are some one-shot
methods [41], [42], [54], [55] consider to optimize the dif-
ference between the output activations of the original and
quantized layers:
argmin ˆW||XW−XQ(W)||forX∈C (7)
A typical work of one-shot methods for weight-only
LLM quantization is GPTQ (also known as OPTQ) [42],
which is built on an adaptive rounding method called Opti-
mal Brain Quantization (OBQ) [56]. OBQ handles each row
of the weight matrix independently, quantizing one weight
at a time while updating all not-yet-quantized weights to
compensate for the error incurred by quantizing a single
weight. However, OBQ is not explicitly designed for LLMs
and can be slow and inaccurate in practice. To fix these
problems, GPTQ quantizes weights of all rows in parallel
to improve efficiency, uses lazy batch updates to achieve a
higher compute-to-memory ratio in the quantization pro-
cess, and uses Cholesky reformulation to help numerical
stability. GPTQ can quantize OPT-175B or BLOOM-176B in
around 4 hours on a single NVIDIA A100 GPU with these
modifications. Further, GPTQ can provide reasonable accu-
racy under extreme quantization where weights are quan-
tized to 2-bit or lower. QuIP [55] defines a family of adaptive
rounding methods for optimizing the Equation (7) and
defines the optimal method within the pre-defined family of
methods, called LDLQ. LDLQ uses the LDL decomposition
of the second-moment matrix of vectors in the calibration
set to find the optimal way to update the not-yet-quantized
weights. The authors show that GPTQ is a particular case
of LDLQ. Further, QuIP proposes incoherence processing
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8
that can transform the weight matrix into a more suitable
form for quantization. Combining LDLQ and incoherence
processing, QuIP is the first LLM quantization method that
has viable results on 2-bit weight-only quantization. AWQ
[41] shows that preserving only 0.1%channels correspond-
ing to significant activation in FP16 and quantizing the rest
of the weight matrix can contribute to much better model
performance, meaning weights are not equally important.
Further, AWQ intends to reduce the quantization error
for the essential weights without using mixed-precision,
which is hard to implement efficiently. This is achieved by
activation-aware scaling, which automatically finds a per-
(input) channel scaling ratio swith an objective similar
to Equation (7) i.e.,argmins||(s−1·X)Q(W·s)−XW||
such that the salient weights with high scaling factor can be
better represented. In contrast, non-salient weights will not
be neglected. OWQ [57] is an advancement over OPTQ that
significantly enhances the quality of the quantized model. It
uses a mixed-precision quantization scheme, which applies
higher precision to the weights susceptible to quantization
caused by activation outliers. The sensitive weights are
identified using a sensitivity-based method similar to [49].
There are also some studies focusing on rounding cri-
teria in quantization. SignRound [58] suggests that as the
bit-width of quantization decreases, the quantization grid
broadens, thus emphasizing the importance of up and down
rounding. It extends previous work [59] to learn weight
rounding with signed gradient descent and can achieve
good results within 400 optimization steps.
3.3.2 Weight + Activation Quantization.
Quantizing both weights and activations of LLMs is a non-
trivial problem for several reasons. First, the range and
statistics of activation are unknown until actual runtime.
Second, quantizing weights and activations enables efficient
low-bit datatype operations on specific hardware. Third,
systematic outliers appearing in the LLM activations are
vital to the model performance, and shouldn’t be clipped in
quantization. While the first two reasons apply to all mod-
els, the third reason is unique to LLMs and differentiates
methods for LLMs from methods for previous models.
Similar to its weight-only counterpart, weight + activa-
tion quantization can also use basic uniform quantization
methods [21], [43], [44], [63] but with a special notification
of outliers in activations. LLM.int8() [43] emphasizes the
emergence of extreme outliers in LLM’s activations as the
model size scales up. The authors show that these out-
liers are highly systematic. Given input activation Xf16∈
RN×Dinto a linear layer, outliers occur systematically for
almost all Ntokens in a sequence. Still, they are limited
to specific feature/hidden dimensions ˆd∈ {1,2, . . . , D in}.
LLM.int8() thus propose to separate the outlier feature
dimensions O={ˆd|ˆd∈Z,1≤ˆd≤Din}which contains
all feature dimensions ˆdthat have at least one activation
outlier with a magnitude more significant than the threshold
α. The outlier dimensions are preserved in high-precision
datatypes ( e.g., FP16) while average values are quantized
using symmetric uniform quantization into low-precision
datatypes ( e.g., INT8). With Einstein’s notation, the matrixmultiplication thus becomes:
Xf16Wf16≈X
ˆd∈OXˆd
f16Wˆd
f16+Sf16·X
d/∈OXd
i8Wd
i8 (8)
where Sf16is the dequantization factor. The number of
outlier dimensions |O|is quite small, so this decompo-
sition would only consume more than 0.1% additional
memory typically. Instead of separating outliers into an
additional matrix, RPTQ [63] proposes to cluster and re-
order the dimensions of activation X∈RN×Dinbased on
the minimum and maximum of the dimension i, denoted
as(Xmin,i,Xmax,i ). The idea is to group dimensions with
outliers into the same cluster and perform cluster-wise
quantization. It should be noted that the statistics of each
activation dimension are measured on a calibration set so
that the clustering can be done before inference to find
the new order of dimensions. To further reduce latency,
RPTQ fuses the reorder operation into other operations: 1)
Combine with the LayerNorm operation to avoid additional
data movement and adjust. 2) Reorder columns of weight W
to reorder the dimensions of the output Y=XW.
Recently, low-bit floating-point formats ( e.g., FP4, FP8)
have emerged as promising alternatives for LLM quanti-
zation [64], [65], [66]. FP8 format has garnered support
from leading hardware vendors like NVIDIA despite its
potentially higher hardware costs. Intuitively, low-bit FP
formats can be viewed as a particular case of non-uniform
quantization, offering a typically more extensive data range
and higher precision for small values but lower precision for
large ones. Such characteristics of the FP format help solve
outlier problems in activations. MoFQ (Mixture-of-Formats
Quantization) [64], and ZeroQuant-FP [65] both show that
FP8 quantization is consistently better than INT8 when it
comes to activation quantization. MoFQ further provides an
algorithm to determine the optimal data format from some
candidates (INT4, INT8, FP4, FP8) for each layer based on
tensor error, layer output error, or model output error. Also,
MoFQ reallocates special NaN (Not a Number) and Inf (In-
finity) values in standard FP formats to normalized numbers
to enhance and let the FP format represent more values,
which is especially important for low-bit formats like FP4.
ZeroQuant-FP quantizes both weight and activation into FP
format. For cases using FP4 weights and FP8 activations,
ZeroQuant-FP proposes a bit-shifting method to cast FP4 to
FP8 to improve inference efficiency efficiently.
Another promising way is to suppress outliers that ap-
pear in the activation dimensions [45], [67], [68], [69], [70].
The general idea is that we can scale down the outlier
dimensions iin activations by factor siand scale up the
corresponding dimension in the weight matrix by factor si
without changing the output of the layer:
Y=XW= (ˆXdiag( s))·(diag( s)−1ˆW) =ˆXˆW (9)
so that the activation ˆXafter scaling would be quantization-
friendly. SmoothQuant [45] computes the per-dimension
scaling factor using:
si= max( |Xi|)α/max(|Wj|)1−α(10)
where αis a hyperparameter to control how much difficulty
will be migrated from activation to weights. Outlier Sup-
pression [67] discovers that γin LayerNorm acts as a sinful
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9
TABLE 2: The following table shows the perplexity of various strong baseline quantization methods for LLMs on Wikitext-
2 [60] and C4 [61]. Please note that the intention is not to compare the perplexity after quantization directly, as different
quantization methods may perform best on different models with different scales. This table only serves as a rough
comparison of the effects of different quantization methods. We strongly recommend readers refer to the original papers
for detailed results in different settings. The reported results are derived from the original papers, except for QLoRA, whose
result is derived from LoftQ [62].
Dataset Method#Bits
Weights#Bits
ActivationsModelPerplexity (↓)Speedup
FP16 Model Quantized Model
Wikitext-2AWQ [41] 3 16 OPT-66B 10.09 10.46 1.85 ×
OPTQ/GPTQ [42] 3 16 OPT-175B 8.34 8.68 3.24 ×
ZeroQuant [21] 4 8 BLOOM-176B 8.11 8.33 -
SmoothQuant [45] 8 8 LLaMA-65B 6.17 6.20 1.56 ×
LLM-QAT [46] 4 8 LLaMA-30B 7.00 7.50 -
INT2.1 [47] 2 16 LLaMA-7B 5.08 8.74 -
QLoRA [48] 3 16 LLaMA-13B 5.12 5.22 -
C4OPTQ/GPTQ [42] 3 16 OPT-175B 10.13 10.67 3.24 ×
LLM.int8() [43] 8 8 OPT-13B 12.45 12.45 1.22 ×
ZeroQuant [21] 4 8 BLOOM-176B 10.97 11.22 -
LLM-QAT [46] 4 8 LLaMA-30B 6.00 6.90 -
INT2.1 [47] 2 16 LLaMA-7B 7.52 12.52 -
amplifier for the outliers. Hence, it proposes the Gamma
Migration method, which uses γ−1in the previous layer as
the scaling factor s. Outlier Suppression+ [68] extends the
method through introducing additional shifting factor z:
Y=XW= (ˆXdiag( s) +z)·(diag( s)−1ˆW)
=ˆXˆW+zW=ˆXˆW+ˆb.(11)
The per-dimension shifting factor is computed as zi=
(max( Xi) + min( Xi))/2, which helps remove the asym-
metry in the activation ˆX= ( X−z)diag( s). FPTQ [69]
proposes a new offline logarithmic activation equalization
(LAE) method that moderates activation distributions in a
non-linear fashion, each channel of the scaling factor sis
computed as:
si= max( |Xi|)/log2(2 + max( |Xi|)) (12)
While the above methods use hand-craft quantization
parameters such as scaling factor s, Outlier Suppression+
in contrast proposes to find optimal scaling factor sby
optimizing the following objective using a calibration set:
min
sE[||XW−(Q(ˆX)Q(ˆW) +ˆb)||2
2]. (13)
Further, OmniQuant [70] proposes learning the clipping
range [α, β]to modulate the extreme values of weights.
QLLM [71] proposes a unique approach to handling outliers
in activations. The technique involves an adaptive channel
reassembly process that splits the outlier channels into mul-
tiple sub-channels. This ensures a more even distribution
of activation magnitudes. The process also merges similar
channels, maintaining the original channel number for effi-
ciency purposes.
There are also some recent studies employing methods
that do not fall into any of the previous paragraphs, so we
briefly cover them here for completeness. REx [72] quantizes
the quantization error, i.e.,W−Q(W), so that there is asmaller quantization error between the original value and
the dequantized value, which trades efficiency for higher
model performance. OilVe [73] employs the outlier-victim
pair (OVP) mechanism, which prunes some quantized low-
precision normal values to make extra space for the high-
precision outliers.
As a summary of PTQ methods for LLMs quantiza-
tion, we briefly compare and contrast the weight-only
and weight+activation quantization methods here. On the
one hand, weight-only quantization methods can push the
quantization limit to lower bit-widths (even to 3 bits or 2
bits), which significantly reduces the memory size of devices
required by LLMs. This is because model weights use most
of the memory. On the other hand, weight+activation quan-
tization can take advantage of the additional speedup that
comes with efficient low-bit arithmetic supported by specific
hardware and doesn’t introduce additional dequantization
overhead during inference. However, these methods often
require more bit-width ( ∼8 bits) to store the weights and
activations. Both weight-only and weight+activation quan-
tization methods have their strengths and weaknesses, and
they are both active research directions with great potential
and demand.
3.4 Quantization-Aware Training for LLMs
Quantization-Aware Training (QAT) is the method of re-
training a quantized model to recover from the performance
degradation caused by the quantization. As is illustrated in
the previous sections, QAT for models before LLMs ( e.g.,
CNN, medium-size LM) has achieved impressive success.
However, such methods often involve full-parameter re-
training of the entire model, which is too expensive for
LLMs, so there are also some attempts to combine quan-
tization with parameter-efficient training methods to signif-
icantly lower the cost of QAT on LLMs.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10
As a result, we divide current QAT methods on LLMs
into two categories: full-paramter re-training and parameter-
efficient re-training . We’ll discuss works in these two cate-
gories respectively in the following parts.
3.4.1 Full-Parameter Re-Training.
The primary concern of using the QAT framework in LLMs
is re-training them on a smaller dataset without hurting
their emergent abilities, such as in-context learning. Current
methods often combine QAT and distillation to preserve
these abilities of the original (teacher) model [74], [75].
LLM-QAT [46] directly applies the basic QAT framework
[24] to LLMs. To cope with the problem, LLM-QAT proposes
to use a data-free distillation method, where the data is
generated using the original LLM, and the quantized LLM is
trained to match the output distribution of the original LLM
on the generated data. Also, LLM-QAT enables quantization
and QAT of key-value caches, which takes up large memory
in the long sentence generation process.
To alleviate the unbearable cost of re-training full LLM,
ZeroQuant [21], [44], [65] proposes a layer-to-layer knowl-
edge distillation method. The method uses the original LLM
as a teacher and processes the weights of LLM in a layer-by-
layer order. Assume the LLM has Nlayers L1, L2, . . . , L N
and has some input dataset Xand the quantized ver-
sion of LkisQ(LK). After QAT and distillation of layers
L1, L2, . . . , L k−1, we use the following loss to update Lk:
Lk= MSE( Lk·Lk−1. . . L 1(X)−ˆLk·Lk−1. . . L 1(X))(14)
3.4.2 Parameter-Efficient Re-Training.
There are a bunch of works using parameter-efficient meth-
ods ( e.g., LoRA, Adapter, Prompt Tuning) to finetune the
LLMs, which will be discussed in Appendix A. In this sec-
tion, we discuss some methods that use parameter-efficient
methods in the re-training process of QAT.
Typical works [47], [48], [62], [71], [76], [77], [78] adopt
Low-Rank Adaption (LoRA) to re-train quantized LLMs
in a relatively acceptable compute budget. QLoRA [48]
quantize the weight of LLMs into 4-bit NormalFormat and
subsequently adopt LoRA with 16-bit BrainFloat to finetune
the quantized model on downstream tasks with cross en-
tropy loss. It further introduces a technique named double
quantization , which quantizes the quantization parameters
to compress further the model’s size in the trade of com-
putation speed. Combining all these techniques, QLoRA
enables finetuning a 65B LLM on a GPU with 30G memory
efficiently. Following QLoRA, QA-LoRA [78] proposes to
integrate group-wise quantization into QLoRA. The authors
suggest that quantization parameters in QLoRA are much
less than LoRA parameters, resulting in an imbalance be-
tween quantization and low-rank adaptation, while group-
wise operations can alleviate the problem by increasing the
number of parameters of quantization and decreasing that
of adaptation. Besides, LoftQ [62] finds the zero initializa-
tion of LoRA matrices in QLoRA inefficient for downstream
tasks. Instead, LoftQ proposes to initialize the LoRA ma-
trices with the singular value decomposition (SVD) of the
difference between the original and quantized weights, i.e.,
W−Q(W). LoftQ alternates between quantization and SVD
to obtain a better approximation of the original weights.LACos-BLOOM [76] quantizes the model weights using 8-
bit block-wise uniform quantization. The quantized model
is then finetuned using a scalable LoRA and 8-bit Adam
optimizer. INT2.1 [47] utilized GPTQ to quantize LLMs into
INT2 and found that the behavior of the quantized model
deviates significantly from the original full-precision coun-
terpart. INT2.1 integrates additional trainable parameters
(LoRA matrices) into the model and solely updates the
LoRA matrices with takes up of only 5%of total parameters.
The training objective combines a scaled Kullback-Leibler
divergence from the full precision model to the quantized
one and the cross entropy loss to encourage accurate next
token prediction. Their experiment indicates that an INT2
Large Language Model (LLM) finetuned with LoRA can
generate linguistically coherent English text and exhibit
adherence to prescribed instructions.
Other works [79], [80] freeze the quantization indices
and solely finetune quantization parameters ( e.g., scaling
factor Sin uniform quantization and quantization level
∆iin non-uniform quantization). AlphaTuning [79] works
by employing binary-coding quantization [52]. During the
adaptation phase, the binary values are frozen for all tasks,
while the scaling factors are fine-tuned for the downstream
task. PEQA [80] quantizes each fully connected layer of LMs
into a matrix of low-bit integers and a scalar vector using
uniform quantization. Subsequently, finetuning occurs on
the scalar vector for each downstream task.
Works also combine quantization with adapters [81] and
prompt tuning [82].
3.5 Other Topics for LLM Quantization
Some quantization-related works can not be categorized
into PTQ or QAT; we discuss such works in this section.
One important topic is co-designing efficient kernels
along with quantization algorithms [83], [84], designing
hardware-friendly quantization methods [85], [86] and in-
tegrating quantization methods in real-world applications
[87], [88], [89], [90], [91]. LUT-GEMM [51] is an efficient ker-
nel designed for an extended version of BCQ methods [52],
which can represent both uniform and non-uniform quan-
tization. Since weights are characterized by a binary vector
and scale factors in BCQ, LUT-GEMM can pre-compute and
store all possible combinations of full-precision activations
and binary patterns in a lookup table (LUT) to avoid re-
peated computation and remove dequantization of weights,
which accelerates the latency of OPT-175B model with 3-bit
quantization by 2.1×compared to conv entional simulated
quantization. Many uniform [21], [43], [44], [73] and non-
uniform quantization methods [49] discussed in the above
sections also design special kernels to reduce the overall
latency.
Other meaningful works study the intrinsic character-
istics of LLM quantizations [92], [93], [94]. For example,
Dettmers and Zettlemoyer [93] run extensive experiments
with 16-bit activations and k-bit weights ( 3≤k≤8)
at scales of 19M to 176B parameters across LLM families
BLOOM, OPT, NeoX/Pythia and GPT-2. The authors focus
on the tradeoff between zero-shot ability and total model
bits and show that 4-bit precision is almost universally
optimal for the tradeoff across different LLM classes and
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11
quantization methods. Liu et al. [94] aim to investigate
the impact of quantization on emergent abilities , which are
essential characteristics that distinguish LLMs from small
language models. Their empirical experiments show that
emergent abilities still exist in 4-bit quantization models,
while 2-bit models encounter severe performance degra-
dation on the test of these abilities. The authors conduct
further detailed experiments on enhancing the performance
of extremely low-bit models.
Some works [67], [95], [96] also focus on studying the
reasons behind the emergence of systematic outliers in
LLMs and looking for ways to suppress the outliers from the
source. Quantizable Transformer [95] ascribes the outliers in
activations to the behavior of attention heads that try not
to update residual. The authors designed clipped softmax
and gated attention accordingly to grant the model the
ability to produce minimal magnitude (or even exact zeros)
output of attention function without having outliers. Outlier
suppression [67], however, treats γin LayerNorm as the
sinful amplifier of outliers. There is still no consensus on
the source of activation outliers. However, Ahmadian et al.
[96] find that outlier dimensions may not be an inherent
product of scale as is thought in previous works [43], but
rather sensitive to the optimization conditions ( e.g., dropout
rate, weight decay, datatype) present during pre-training.
4 P RUNING
As a conventional technique employed for the compression
and acceleration of neural networks, pruning eliminates
non-essential weights or structures from models, while pre-
serving the performance of the networks at a level nearly
equivalent to their original state. Although pruning has
shown remarkable results in CNNs [97], its effectiveness is
less robust for LLMs when compared to other compression
techniques such as quantization and distillation. The reason
why pruning becomes less effective comes from the fine-
tuning process. The high cost of fine-tuning due to the
large number of model parameters makes it more difficult
to achieve the full effect of pruning. Nevertheless, pruning
is a crucial technique for compressing models, necessitating
further exploration to enhance and refine its effectiveness in
yielding improved results in LLMs.
In the following section, we will provide an overview
of pruning methods and basic concepts in Section 4.1. Sub-
sequently, in Section 4.2, we will expound upon pruning
techniques tailored for medium-size language models ( i.e.,
models with parameters in billions), given their structural
similarities with LLMs. Section 4.3 will delve into a detailed
exploration of pruning methodologies specifically designed
for LLMs. Finally, in Section 4.4, we will introduce some
auxiliary techniques that are not pruning methods but asso-
ciated with pruning to improve LLM pruning results, and
then discuss the challenges for future advancements in the
field of LLM pruning.
4.1 Basic Concepts
Numerous classification criteria exist for pruning. Never-
theless, the most significant things among them are two
fundamental problems: what to prune and how to prune.The answers to these two problems correspond respectively
to the pruning unit and metric. We will introduce these two
fundamental concepts and some other basic ones.
1)Pruning Unit. The first fundamental problem with
pruning is what kind of elements should be pruned. The
pruning units refer to the minimal pruning elements in the
pruning process, encompassing elements such as weights,
neurons, attention heads, layers, and etc. Based on pruning
units, pruning methods can be broadly categorized into un-
structured pruning and structured pruning. In unstructured
pruning, the pruning units focus on individual weights. The
weights to be pruned are zeroed out. Whereas in structured
pruning, the pruning units encompass broader network
structures, such as neurons, attention heads, and layers. The
structures to be pruned are removed from the networks.
Unstructured pruning tends to get a higher sparsity ratio
and maintain better performance as it is not limited to
the network structure and can prune individual weights.
However, the irregularly sparse patterns of weight matri-
ces, stemming from the non-systematically occurring zero
values, exhibit computational efficiency nearly equivalent
to dense matrices. Consequently, achieving significant gains
in inference speedup is infrequent in unstructured pruning.
Structured pruning makes it easy to achieve inference
speedup as it prunes network structures ( e.g., attention
heads, feed-forward network (FFN) neurons, and hidden
dimensions). Yet inevitably integrated structure deletions
may cause the performance descent of the model. To avoid
model collapse, the sparsity ratios of structured pruned
models are lower than unstructured ones.
Formally, a binary mask zusually covers the pruning
unit during the pruning process and is multiplied into the
model after pruned. For unstructured pruning, the pruning
process can be defined as a constrained optimization prob-
lem:
min
w,zL(w⊙z;D) = min
w,z1
NNX
i=1ℓ(w⊙z; (xi, yi)),
s.t.∥z∥0≤t,(15)
where ⊙corresponds to the element-wise product, w=
{w1, w2, ..., w M}is the network weights, Dis a dataset
composed of Ninput xiand output yipairs, and tis
the target non-sparsity ratio ( i.e., one minus sparsity ratio).
Similarly, the pruning process for structured pruning is as
follows:
min
w,zL(s⊙z;D) = min
w,z1
NNX
i=1ℓ(s⊙z; (xi, yi)),
s.t. f (z;s)≤t,(16)
where s={s1, s2, ..., s K}is the pruning structures com-
posed of w, and f(·)is the function to compute non-sparsity
ratio according to the binary masks and structures.
2)Pruning Metric. The second fundamental problem
with pruning is how to determine whether an element
is essential and should be pruned or not. Pruning metric
is the answer to this problem. The pruning metric is the
criterion to identify the importance of the pruning units. It
can be roughly divided into three parts: magnitude-based,
loss-based ( i.e., considering the first-order and second-order
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12
Pruning Fine-tuning Training (a)
(b)
(c) Pruning Fine-tuning TrainingTraining and Pruning Fine-tuning
Fig. 3: Three classes of static pruning methods. (a) Pre-
training pruning; (b) During-training pruning; (c) Post-
training pruning.
derivative information of the weighs belonging to the prun-
ing units), and regularization.
The magnitude-based pruning methods use the magni-
tudes ( i.e., absolute values) of weights and activation values
as a part of the pruning metrics. The fundamental principle
underlying this class of methods is that the magnitude of
weight or the activation value from the pruning unit intu-
itively reflects its importance. The magnitude of the weight
alone can serve as a pruning metric, constituting a well-
known foundational pruning method known as Magnitude
Pruning [98]. Magnitude Pruning is the vanilla magnitude-
based pruning method. In this method, a threshold is set to
zero out weights with smaller magnitude and the threshold
typically is derived from sparsity ratio. Despite the defini-
tion of importance score being quite heuristic, Magnitude
Pruning demonstrates efficacy across various models.
In addition to the intuitive magnitude-based metric,
another more sophisticated kind of metric is the loss-based
metric. The loss-based metric is designed to attribute the
importance of a pruning unit to its impact on loss. If the loss
increases significantly after pruning an element, it indicates
that the element should not be pruned. More precisely,
following the pruning of an element, the greater the increase
in loss, the more crucial the importance of that element
becomes. However, examining the loss after pruning indi-
vidual elements one by one is resource- and time-intensive.
In contrast, employing the Taylor expansion provides a
more convenient expeditious method for elucidating the
loss alteration. The alteration in loss after the pruning can
be quantified using a Taylor expansion, incorporating the
first-order or second-order derivatives of the pruning units
with respect to the loss and higher-order ones, which are
usually ignored. In comparison to the resource- and time-
intensive approach of evaluating loss after pruning each
element individually, the computation of the first-order and
second-order derivatives emerges as a more efficient and
time-saving alternative.
Besides, regularization methods encompass L0,L1, and
L2regularization. While L1regularization is known for
inducing sparsity in weights, L0regularization is a more
commonly employed regularization approach in the context
of pruning.
3)Dynamic/Static Pruning. To enhance adaptability
to diverse inputs, a kind of pruning method, referred to
asdynamic pruning , constructs the network in a manner
contingent upon the specific input characteristics. We will
these later in Section 7. In contrast, static pruning methods
prune the model at training time and fix the architectureafter pruning, thus different inputs share the same pruned
network. Static pruning methods can be classified as pre-
training pruning ,during-training pruning and post-training
pruning according to the pruning period, as shown in Fig. 3.
•Pre-training pruning : prunes the initialized network
first and then trains the sparse network.
•During-training pruning : trains and prunes the dense
network at the same time, where regularization
methods are representative.
•Post-training pruning : is the most popular type of
pruning pipeline, prunes the trained dense network
to get a sparse network, where they usually follow
a training, pruning, and fine-tuning paradigm as we
mentioned before.
4)Iterative/One-shot Pruning. As pruning damages
model performance inevitably, a popular paradigm of prun-
ing pipeline consists of three steps: training, pruning, and
fine-tuning, as shown in Fig. 3 (b), (c). The initial step
involves training the network to ascertain the importance
of individual pruning units. Subsequently, the second step
entails the removal of non-essential pruning units through
pruning, and the third step focuses on fine-tuning to recover
the performance of the model post-pruning.
Given the potential for the fine-tuning process to render
initially zero-valued weights as non-zero, the final two steps
are subject to iterative repetition until the targeted sparsity
ratio is achieved. This iterative design underscores that each
pruning step is succeeded by a fine-tuning step, thereby
facilitating the preservation of the model’s performance.
These methods containing iterative pruning and fine-tuning
rounds are classified as iterative pruning .
However, as the model parameters get huger, the itera-
tive pruning and fine-tuning process is expensive and time-
consuming. Thus more pruning methods tend to prune the
network only once to the target sparsity ratio, discarding the
iterative pruning and fine-tuning rounds. These methods
are classified as one-shot pruning .
5)Global/Local Pruning. The early pruning approaches
compare all the pruning units to identify and eliminate
those less essential. Given that the comparison scope in
these methods encompasses the entire network, they are
categorized as global pruning approaches. However, global
pruning permits distinct sparsity ratios for individual local
regions. It might result in excessive pruning of a specific
region ( e.g., a layer, a column), and exert a notable influence
on the overall performance of the model. The resolution of
this issue lies in the application of local pruning method-
ologies. Local pruning imposes constraints on the sparsity
of each region, thereby ensuring that the sparsity ratios
within each region do not reach excessively low thresholds,
consequently mitigating the risk of model collapse.
6)Data-driven/Data-free Pruning. The categorization of
pruning methods into data-driven and data-free modalities
distinguishes the reliance on data for pruning decisions.
Specifically, data-driven pruning methods, exemplified by the
majority of pruning techniques, derive pruning decisions
from available data. Conversely, data-free pruning methods,
such as Magnitude Pruning [98], execute network pruning
independent of data input. In general, data-driven pruning
methods tend to exhibit superior performance, given their
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13
dependence on data-driven insights, while data-free prun-
ing methods are less effective but data-independent.
7)Upstream/Downstream Pruning. The training of lan-
guage models involves two main stages—pre-training and
fine-tuning. Pruning methods can be classified based on
when they are applied. Techniques identified as upstream
pruning involve the pruning of the model before the fine-
tuning stage. In contrast, downstream pruning methods are
characterized by the simultaneous execution of pruning
alongside the fine-tuning process. Accordingly, upstream
pruning retains the adaptability of the pruned model for
multiple tasks, ensuring its versatility. Conversely, down-
stream pruning directs the pruned model to concentrate on
a specific, well-defined task.
4.2 Pruning Methods for Medium-Size Language Mod-
els
Language models, such as GPT-2 and BERT, are initially
trained on extensive corpora and exhibit applicability across
various downstream tasks after fine-tuning. Specifically, the
pruning of language models distinguishes itself from the
pruning methodologies employed in Convolutional Neural
Networks (CNNs) or Recurrent Neural Networks (RNNs) in
three key aspects. First and foremost, the sheer magnitude
of parameters in language models surpasses that of CNNs
or RNNs. For instance, the BERT-large model encompasses
335 million parameters, whereas the parameters of a typical
RNN are in the range of tens of millions [99]. The increased
number of parameters amplifies the temporal and compu-
tational demands of the fine-tuning phase. Consequently,
language model pruning necessitates addressing the chal-
lenges posed by this substantial parameter abundance.
Secondly, language models have the potential to undergo
fine-tuning for a multitude of downstream tasks. Certain
upstream pruning methodologies necessitate the retention
of the language model’s capacity to function as a multi-
task solver. Thirdly, transformer-based language models
exhibit a distinctly different structural composition. Hence,
in light of the model’s architecture, certain structured prun-
ing methods may require reconfiguration to align with the
structure of the model. In conclusion, there exist specialized
designs of pruning methodologies for language models that
are tailored to their unique characteristics, deviating from
conventional pruning approaches.
We will introduce these pruning techniques for medium-
size language models in the following, including ap-
proaches that are specially designed for Transformer-based
models and generic to plenty of models with different
architectures. In consideration of the fundamental features
of pruning methods ( i.e., the determination of what to
prune and how to prune), we shall introduce these pruning
methods in the order of pruning unit and metric. Initially,
we classify pruning methods into two primary components:
unstructured and structured ones. Subsequently, based on
the sequence of pruning criteria, we will expound upon each
of the three pruning methods: magnitude-based pruning,
loss-based pruning, and regularization.4.2.1 Unstructured Pruning for Medium-Size Language
Models
Unstructured pruning methods zero out non-essential
weights without any specific constraints. We will introduce
unstructured pruning methods for medium-size language
models in a systematic order based on specific metrics,
including magnitude-based pruning, loss-based pruning,
and regularization.
1) Magnitude-based Pruning
Magnitude-based pruning, characterized by its simplic-
ity and efficacy, incorporates the magnitudes of weights and
activation values into its pruning metrics. In this section
on magnitude-based pruning for medium-size language
models, we find that all of the related methods exclusively
focus on the magnitudes of weights. Consequently, we will
introduce these magnitude-based pruning methods with
weights.
Magnitude Pruning [98], recognized as the most com-
monly utilized pruning method, has been examined in the
context of medium-size language models [100], [101], [102],
[103]. Gordon et al. [100] conducted a study focusing on
the compression of BERT through Magnitude Pruning. The
findings reveal that approximately 30−40% of the weights
are non-essential and can be discarded without affecting
BERT’s performance. Furthermore, fine-tuning BERT for a
specific task does not contribute to an enhancement in the
ultimately achievable sparsity ratio. This implies that BERT
can undergo pruning once during the pre-training phase,
obviating the need for separate pruning for each task, all
while maintaining performance integrity. Based on this,
Prune Once for All [104] is to prune models once for all
tasks before fine-tuning.
Magnitude pruning, characterized by the direct pruning
of the model sparsity ratio to the target ratio, may result in a
substantial deterioration of model performance. Compared
to Magnitude Pruning, Gradual Magnitude Pruning (GMP)
[105] introduces a sparsity ratio schedule, gradually reduc-
ing the sparsity ratio throughout the pruning process. Prune
Once for All [104] and GMP ⋆[106] are both implementations
of GMP specifically applied to language model pruning.
Besides, GMP ⋆introduces an initial substantial pruning step
to better adapt to a high target sparsity ( e.g., 97%). This ap-
proach allows for more recovery time in subsequent pruning
steps, ultimately leading to improved performance, outper-
forming most pruning methods including Prune Once for
All [104].
2) Loss-based Pruning
While magnitude-based pruning is easy to implement,
the magnitude alone may not accurately reflect the im-
portance of weights in some instances. The magnitude
of certain weights may be diminutive, yet their contribu-
tion remains essential [107]. Therefore, a more scientifically
grounded approach involves assessing these weights within
the context of a specific task. The methods in this section
adopt the loss-based pruning strategy tailored for medium-
size language models. These approaches align with a more
nuanced evaluation based on the performance. Given that
the model’s training process is inherently geared towards
minimizing this loss, the loss undoubtedly stands out as the
most reliable measure of the model’s performance.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14
The first major category of loss-based pruning methods
integrates information about the gradients within the spe-
cific metrics. The universal expression by which these meth-
ods evaluate the importance of weights can be articulated
through the negative gradient-weight product, expressed as
follows:
I=−w∇L(w) (17)
The first interpretation of this expression pertains to the
weight change. The negative gradient direction of the
weights signifies the direction in which the weights are
intended to increase. Consequently, if the weight direction
aligns with the direction of weight growth, it indicates
the importance of that weight in the specific task, as the
task necessitates the continued increase in its magnitude.
Alternatively, the second interpretation of this expression
can be simplistically conceived as the first-order term of the
Taylor expansion of loss alteration, with higher-order terms
being disregarded.
Many methods have implemented their improvements
based on this universal expression. Movement Pruning [108]
accumulates multiple updates of the negative gradient-
weight product. Accumulating such information aids in
minimizing fluctuations during pruning. Among the first-
order methods, Movement Pruning stands as a pioneering
one, upon which many extensions have been developed
[109], [110]. To mitigate the substantial variability and un-
certainty introduced by mini-batch sampling and intricate
training dynamics, PLATON [111] employs a weight prun-
ing strategy that considers both the importance and uncer-
tainty associated with individual weights. The uncertainty
originates from changes in importance. To enhance stability,
both importance and uncertainty undergo exponential mov-
ing averaging. The final importance score of each weight
is determined by the product of smoothed importance and
uncertainty. Parameter-efficient Sparse Training (PST) [112]
and LoRAPrune [113] add the magnitude of weight and the
accumulated negative gradient-weight product to derive the
final importance score.
The second major category of loss-based pruning meth-
ods integrates information about the second-order deriva-
tive within the specific metrics. The variation of the loss,
when employing the Taylor expansion and expanding up to
the second-order term while neglecting higher orders, can
be expressed in the following manner:
L(w)− L(w∗)≃(w−w∗)⊤∇L(w∗)
+1
2(w−w∗)⊤HL(w∗)(w−w∗),(18)
where HL(w∗)is the Hessian matrix. These methods are
post-training pruning methods and always prune a well-
trained network w∗. Therefore, the gradient ∇L(w∗)can
be neglected, getting a universal expression to represent the
importance of weights:
I=1
2(w−w∗)⊤HL(w∗)(w−w∗). (19)
The Optimal Brain Damage (OBD) [114] and the Opti-
mal Brain Surgeon (OBS) [115] represent the second-order
pruning approaches in early works. Both of them utilize the
Hessian of the loss function to selectively eliminate specificparameters while minimizing the impact on loss. To stream-
line calculations, both methods simplify the computation of
the Hessian matrix to some extent. However, OBD computes
solely the diagonal entries of the Hessian matrix, whereas
OBS also considers the impact of off-diagonal entries. These
methodologies have served as inspiration for numerous
subsequent approaches. The Optimal BERT Surgeon [116]
extends the principles of the OBS to the context of BERT,
yielding better results when compared to some magnitude-
based pruning methods [98], [104], [106] and the first-order
pruning methods [108], [111].
3) Regularization
In addition to the aforementioned methods, regular-
ization techniques find many applications in medium-size
language models. L1andL2regularization are popular
methods employed to counteract network overfitting. Both
introduce a regularization term into the loss function. Be-
sides, L1regularization has the additional effect of inducing
sparsity in weights. However, when it comes to directly
pruning the network, L1regularization is not always the
most suitable choice. This is attributed to the fact that
L1regularization imposes more substantial penalties on
larger weights, deviating from the original pruning objec-
tive of eliminating unimportant connections, where smaller
weights are often situated.
Instead, L0regularization [117] is a more versatile prun-
ing method than L1andL2regularization. L0regularization
incorporates the L0norm of weights into the loss function.
Similar to L1andL2regularization, L0regularization pe-
nalizes non-zero weights. However, it distinguishes itself by
applying equal penalties to all non-zero weights, aligning
precisely with the pruning objective of equitably penalizing
all the existing connections.
The training objective of the pruning process for all three
of these regularizations can be expressed by the following
formula:
min
w,z1
NNX
i=1ℓ(w⊙z; (xi, yi)) +λ∥w∥p, (20)
where λrepresents the regularization factor, ∥w∥pdenotes
theLpnorm of the weights, and zis a binary mask indicat-
ing whether the weights are pruned. Consequently, the L0
norm of the weights can be equivalently represented by the
summation of binary masks, i.e.,∥w∥0=PM
i=1zi.
However, the discrete nature of zposes challenges for
efficient gradient-based optimization. To this end, the hard
concrete distribution serves as an approximation to the
binary masks, allocating approximately half of its mass
to{0, 1}and the remaining half to the interval (0, 1),
thereby bridging the gap between discrete values 0 and 1
with continuous probability mass, as shown in Fig. 4. The
formulation of the hard concrete distribution is as follows:
u∼ U(0,1),
s= Sigmoid((log u−log (1−u) + log α)/β)
¯s=s(ζ−γ) +γ,
z= min(1 ,max(0 ,¯s)),(21)
where U(·)is a uniform distribution, logαis the location
parameter, βis the temperature parameter, (γ, ζ)is the
“stretch” interval with γ < 0andζ > 1. Given that the
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013
/uni0000005d/uni00000013/uni00000015/uni00000017/uni00000019/uni0000001b/uni00000014/uni00000013/uni00000014/uni00000015/uni00000033/uni00000055/uni00000052/uni00000045/uni00000044/uni00000045/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000027/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c/uni00000024/uni00000053/uni00000053/uni00000055/uni00000052/uni0000005b/uni0000004c/uni00000050/uni00000044/uni00000057/uni00000048/uni00000003/uni00000033/uni00000055/uni00000052/uni00000045/uni00000044/uni00000045/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000027/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000002b/uni0000004c/uni00000056/uni00000057/uni00000052/uni0000004a/uni00000055/uni00000044/uni00000050/uni00000003/uni00000052/uni00000049/uni00000003/uni0000005d/uni00000003/uni0000000b/uni00000030/uni00000052/uni00000051/uni00000057/uni00000048/uni00000003/uni00000026/uni00000044/uni00000055/uni0000004f/uni00000052/uni0000000c
/uni0000002b/uni0000004c/uni00000056/uni00000057/uni00000052/uni0000004a/uni00000055/uni00000044/uni00000050/uni00000003/uni00000052/uni00000049/uni00000003/uni0000005d
Fig. 4: The approximate probability density histogram of
hard concrete distribution by using Monte Carlo simulation.
The parameters of this hard concrete distribution are logα=
0,β= 0.5,γ=−0.1, and ζ= 1.1. Under this specification
the hard concrete distribution assigns, roughly, half of its
mass to {0, 1}and the rest to (0, 1).
reparameterized variable zis not strictly binary after train-
ing, many pruning methods adopt a threshold to discretize
zinto binary values in the end. For values of zbelow the
threshold, the value is set to 0, while for values above the
threshold, the value is set to 1.
While L0regularization finds broader applications in
pruning, L1regularization also has some pertinent use
cases, and certain methods strive to enhance L1regular-
ization. For example, Reweighted Proximal Pruning (RPP)
[118] builds upon L1regularization and introduces im-
provements and refinements. RPP comprises reweighted L1
regularization and the proximal operator. The reweighted
L1regularization dynamically reallocates penalty factors,
assigning greater penalties to weights approaching zero.
The proximal operator facilitates the separation of the spar-
sity pattern search and the back-propagation-based gradient
update of the training loss, enabling an easier sparse pattern
search.
4) Others
Among the unstructured pruning methods discussed
above, numerous approaches demonstrate an ability to
uphold satisfactory model performance even with high
sparsity. However, they encounter challenges in achieving
efficient inference speedup due to the irregular nature of the
sparse matrices they generate. To address this predicament,
unstructured pruning methods can be integrated with N:M
sparsity [119].
The principle underlying N:M sparsity mandates that
within each group of Mconsecutive weights in the neural
network, no more than Nweights should exhibit non-zero
values. This implies that within each group of Mconsec-
utive weights, there are N−Mweights with zero values.
Thus the underlying hardware can compress the regularly
occurring zero values within it. This kind of compression
relies on unique architectures of the hardware, such as
sparse tensor cores. For instance, the Nvidia Ampere A100
is equipped with sparse tensor cores to accelerate the 2:4
sparsity.
For N:M sparsity, the pruning metric is not a restrictedfactor. It can be seamlessly integrated with unstructured
pruning methods, providing the inference speedup that
pure unstructured methods may lack. For example, the
determination of the sparsity pattern can be initially pred-
icated on the magnitudes of the weights [120]. Serving as
a generic sparsity methodology, 2:4 sparsity demonstrates a
notable twofold acceleration in computational speed with-
out compromising performance.
5) Discussion
Among all these unstructured pruning methods for
medium-size models, the Optimal BERT Surgeon [116]
demonstrates superior performance compared to various
magnitude-based pruning methods [98], [104], [106] and the
first-order pruning methods [108], [111] in the conducted
experiments [106], [116].
Nonetheless, Magnitude Pruning [98] remains the most
widely adopted pruning method. Because it is simple to
implement, yet achieves competitive results with many
intricate methods [121]. Crucially, the pruning process of
Magnitude Pruning operates independently of any specific
dataset, thereby addressing challenges in some scenarios
where datasets may be unavailable.
4.2.2 Structured Pruning for Medium-Size Language Mod-
els
Indeed, numerous unstructured pruning methods have
demonstrated the capability to achieve a high sparsity ratio
while maintaining performance levels comparable to their
dense counterparts. However, it’s noteworthy that unstruc-
tured sparse patterns do not necessarily lead to inference
speedup on normal hardware. Consequently, there is an in-
creasing focus on research dedicated to structured pruning.
In the context of structured pruning methodologies ap-
plied to medium-size language models, the selection of
appropriate pruning units assumes significance alongside
the choice of pruning metric. Pruning units commonly
considered encompass attention heads, FFN neurons, hid-
den dimensions, and etc. Notably, employing architecture-
related structures as pruning units tends to yield more
favorable outcomes compared to structures unrelated to
model architecture, such as weight blocks.
This observed superiority may be attributed to the
preservation of fundamental principles inherent in the
model’s construction when reducing architecture-related
structures. For instance, after the pruning of attention heads,
the resultant model retains the essential characteristics of a
transformer-based model, featuring a reduced number of
attention heads.
In the following, we will delve into the realm of struc-
tured pruning, encompassing magnitude-based pruning,
loss-based pruning, and regularization techniques.
1) Magnitude-based Pruning
Intuitively, the aggregation of weight magnitudes for
pruning units serves as a meaningful representation of
importance, which is widely applicable to convolutional
kernels in CNNs. Similarly, it can be extended to medium-
size language models. For instance, the weight magnitudes
could be aggregated with L2norm to represent the corre-
sponding importance in attention heads, FFN neurons [122],
and weight blocks [123]. The less important structures are
then removed based on the order of their importance scores.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16
2) Loss-based Pruning
Within loss-based pruning methodologies, considerable
attention has been directed towards the exploration and
analysis of attention heads [124], [125], [126], [127]. This
focus emanates from their proclivity to become redundant,
and that the rest of the heads frequently could demonstrate
an aptitude for assuming the functional roles previously
carried out by the pruned heads. Michel et al. [124] proposed
an iterative pruning approach based on head importance
scores. The attention heads are covered with binary mask
variables. Thus, the head importance scores are computed
through the examination of gradients on the binary mask
variables. The results indicated that 20-40% of heads Trans-
former heads could be pruned without significantly com-
promising test accuracy on the target task.
However, Differentiable Subset Pruning (DSP) [125]
demonstrated that Michel et al. [124] significantly under-
estimated the number of Transformer heads that could be
pruned. The experiments showed that DSP could prune up
to 90% of heads without causing much degradation in test
performance. (Pruning up to 90% of heads means around
20% of the model size shrinkage as the parameters of heads
are just part of the whole model.) DSP treats Transformer
head pruning as a subset selection problem. To ensure the
differentiability of the subset pruner, the Gumbel–Softmax
trick [128] and its extension to subset selection is applied to
DSP . The results indicated superior accuracy and inference
speedup of DSP compared to other head pruning methods
[124], [129].
In addition to attention head pruning, Block Move-
ment Pruning [130] is a block pruning method. It extends
structured methods by considering blocks of any size and
integrates these structures into the Movement Pruning [108].
The matrix within the model undergoes partitioning into
fixed-sized blocks, with larger block sizes yielding greater
inference speedup. Furthermore, the combination of this
approach with the pruning of neurons in the FFNs results in
the best overall performance. Similarly, numerous method-
ologies prune neurons in the FFNs and attention heads
simultaneously [131], [132], [133], [134], [135], [136].
In addition to the above methods designed for the
Transformer structures, some structured pruning methods
can be generalized because the pruning units in them are
neurons [137], [138], [139]. For instance, Low-Rank and
Sparse approximation (LoSparse) [137] prunes the weight
matrix in neuron level ( i.e., the columns of weight matrix).
Considering the sensitivity of parameters defined in PLA-
TON [111], the importance of each neuron is defined by the
cumulative sensitivity of parameters within a given column.
3) Regularization
In addition to the loss-based pruning methods, regu-
larization methods constitute another category within the
spectrum of structured pruning techniques applicable to
medium-size language models. Diverging from unstruc-
tured pruning approaches, the regularization term in struc-
tured pruning encompasses binary masks associated with
specific structural components, as opposed to individual
weights. Except for the pruning units, other details closely
resemble those in unstructured pruning.
Nevertheless, among these regularization methods, L0
regularization stands out as the most extensively employedtechnique. The main variability among these L0regular-
ization methods resides in their respective approaches to
the selection of pruning units. Voita et al. [129] introduced
L0regularization to attention head pruning, specifically
selecting a subset of attention heads. McCarley et al. [140]
incorporated L0regularization to prune attention heads and
FFN neurons. Factorized Low-rank Pruning (FLOP) [141]
integrates Low-Rank Factorization with L0regularization.
This methodology involves the reparameterization and fac-
torization of the matrix Winto the product of two smaller
matrices, denoted as W=PQ, where pkand qkrepresent
thek-th column of Pandk-th row of Qrespectively. The
pruning unit is the combination of pkand qk. Additionally,
an augmented Lagrangian method is introduced to regulate
the sparsity ratio in the context of FLOP . Coarse- and Fine-
grained Pruning (CoFi) [142] jointly prunes coarse-grained
and fine-grained modules using L0regularization, includ-
ing attention and FFN layers, individual attention heads,
FFN neurons, and hidden dimensions for Transformer-
based models. Notably, the mask over the hidden dimension
is shared across all Transformer layers and an augmented la-
grangian method is adapted. By combining with a layerwise
distillation approach, CoFi achieves models with more than
10× speedups while exhibiting only a marginal decrease in
accuracy in the conducted experiments.
In addition to L0regularization, L1regularization also
gets relevant research. SIMPLE [143] introduces L1regu-
larization to structured pruning, encompassing attention
heads, intermediate neurons of the FFN, and the hidden
dimension as compressible components. The mask over
the hidden dimension is shared across layers, akin to the
approach employed in CoFi [142]. Through the learning of
masks for these compressible components via a sparsity-
induced objective, various-sized pruned models can be
obtained. These pruned models can subsequently be fine-
tuned with a causal distillation objective to enhance perfor-
mance.
4) Others
Beyond the classification based on metrics, certain struc-
tured pruning methods exhibit notable similarities when
their designated pruning units are identical.
The first class among other structured pruning is layer
pruning [144], [145], [146]. The aforementioned pruning
units, such as attention heads and neurons, are characterized
by their relatively diminutive scale, necessitating a more
detailed pruning scheme to determine which should be
pruned. Conversely, when dealing with substantially larger
pruning units, such as entire layers, numerous methodolo-
gies tend to engage in direct experimentation with multi-
ple pruning schemes before determining the most effective
network configuration. This practice stems from the lower
testing costs associated with a smaller number of layers.
In addition to layer pruning, there is a body of re-
search dedicated to token pruning [147], [148], [149], which
does not alter the underlying network architecture. Token
pruning involves the removal of unimportant tokens from
a sequence during inference to reduce computational re-
quirements. Learned Token Pruning (LTP) [148] represents
a straightforward and effective approach to adaptively re-
move unimportant tokens as an input sequence traverses
through transformer layers. The pruning metric for each
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17
token is determined by the sum of normalized attention
probability from the Transformer block.
Extending beyond the pruning units previously men-
tioned, structured pruning encompasses a myriad of di-
verse units. For instance, Spectral-Normalized Identity Prior
(SNIP) [150] employs a strategy to prune attention and
FFN sublayers by transforming residual connections into
strict identity mappings. SNIP sets specific thresholds for
activation vectors, and those falling below the thresholds
result in the pruning of residual blocks ( i.e., the attention
and FFN sublayers).
4.3 Pruning Methods for LLMs
In the last section, we introduced the pruning methods for
medium-size language models with parameters numbering
less than 1 billion. Most of these methods adopt full fine-
tuning after pruning to improve the performance. However,
as the parameters increase, full fine-tuning becomes more
difficult or even infeasible. This discrepancy underscores
a significant challenge in the field of research dedicated
to pruning techniques tailored specifically for LLMs. To
handle this problem, on the one hand, certain pruning
methodologies opt to incorporate parameter-efficient tuning
techniques to reduce fine-tuning costs. On the other hand,
alternative approaches abandon the fine-tuning process,
relying on an optimized pruning procedure will inherently
lead to retained model performance. The viability of these
alternative approaches is partly attributed to the huge num-
ber of parameters in LLMs. The higher number implies a
higher likelihood of redundancy within the model.
In this section, we will introduce the pruning methods
for LLMs, mirroring the sequence established in the section
devoted to pruning methods for medium-size language
models. Pruning methods for LLMs adhere to a parallel
approach to those employed for medium-size language
models, with some distinctions in certain methods primarily
arising in omitting the fine-tuning process. To facilitate
a more comprehensive comparison of these methods, we
consolidate the characteristics of these pruning methods, as
shown in TABLE 3.
4.3.1 Unstructured Pruning for LLMs
Attributed to the greater capacity of unstructured pruning
methods to preserve model performance compared to struc-
tured alternatives, all of the unstructured pruning method-
ologies in this section for LLMs adopt an approach of
eschewing the fine-tuning process as shown in TABLE 3. The
experiments have demonstrated that these methodologies
can attain a sparsity ratio of 50% with a relatively modest
compromise in model performance.
The two pioneer unstructured pruning methods for
LLMs are SparseGPT [154] and Wanda [151], which become
the baselines for many subsequent methods for compari-
son. The subsequent unstructured pruning methods demon-
strate their capability to outperform SparseGPT and Wanda
across various NLP tasks, thereby attaining superior results.
Though unstructured pruning methods get hardly inference
speedup, they can easily be combined with N:M sparsity
[119] to accelerate inference speed, which is also experi-
mented in SparseGPT and Wanda.These unstructured pruning methods require minimal
calibration data. The minimal calibration data is for a single
forward pass of the model, specifically aiming at acquiring
activation values or gradients to calculate the importance of
weights, which remains a contributing factor to the outcome
of the pruning [166].
In the following, we will introduce these unstructured
pruning methods in LLMs in the order of pruning met-
rics. In this investigation, no regularization-related methods
have been identified, thus this section will be divided into
introductions of methods based on magnitude and methods
based on loss.
1) Magnitude-based Pruning
When directly applying Magnitude Pruning [98] to
LLMs, the outcomes are not very competitive even with
parameter-efficient fine-tuning strategies [167], [168]. There-
fore, in magnitude-based pruning methods, compared to
only using the magnitude of weights as the pruning met-
ric in medium-size language models, more magnitude-
based pruning methods in LLMs combine the magnitude
of weights and activate values as the pruning metric. For
instance, Wanda [151] and RIA [152] use the magnitude of
weight and activation metric. In addition to the magnitude
of weight and activation, E-Sparse [153] also introduces the
information entropy into the metric.
Wanda (Pruning by Weights and activations) [151] intro-
duces a novel pruning metric, considering both the magni-
tude of weights and activate values. The motivation is that
the significance of weights should not solely be evaluated in
isolation but rather in consideration of its product with the
corresponding activation value. To illustrate, let’s consider
a fully connected layer with weights represented by W
with dimensions (Cout, Cin). In the context of language
models, this linear layer receives input activation Xwith
dimensions (N×L, C in), where NandLdenote the batch
and sequence dimensions respectively. For each weight, its
importance is quantified as the product of its magnitude and
the corresponding input feature norm. Concretely, the score
Sijfor the weight Wijis defined as:
Sij=|Wij| · ∥Xj∥2, (22)
where ∥Xj∥2evaluates the L2norm of j-th features aggre-
gated across N×Ldifferent tokens. Remarkably, the results
indicate that Wanda achieves comparable performance to
SparseGPT but in a significantly shorter time.
Similar to Wanda [151], RIA (Relative Importance and
Activations) [152] also jointly considers the weight and
activation. The primary distinction lies in its approach to
alleviating channel corruption ( i.e., the rows and columns
of the weight matrix pruned integrally). RIA replaces the
magnitude of weights with relative importance. This rela-
tive importance is calculated as the magnitude of individual
weights divided by the sum of the magnitude of weights
in their corresponding row and column. Therefore, the
comparison among different rows and columns becomes
relatively equitable by utilizing the relative importance, mit-
igating potential biases introduced by the variations in their
magnitudes. RIA can be further combined with channel
permutation, which maximally preserves important weights
under N:M sparsity to get practical speed-up on specific
hardware.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 18
TABLE 3: A summary of various pruning methods for LLMs.
Methods Unit Metric Iterative/One-shot Finetuning Global/Local
Wanda [151] Unstructured Magnitude-based One-shot No Local
RIA [152] Unstructured Magnitude-based One-shot No Local
E-Sparse [153] Unstructured Magnitude-based One-shot No Local
SparseGPT [154] Unstructured Loss-based One-shot No Local
ISC [155] Unstructured Loss-based One-shot No Local
GBLM-Pruner [156] Unstructured Loss-based One-shot No Local
PGZ [157] Unstructured Loss-based One-shot No Local
FLAP [158] Structured Magnitude-based One-shot No Global
SliceGPT [159] Structured Magnitude-based One-shot PEFT Local
LLM-Pruner [160] Structured Loss-based One-shot PEFT Global
LoRAShear [161] Structured Loss-based Iterative PEFT Global
APT [162] Structured Loss-based Iterative PEFT Global
Sheared LLaMA [163] Structured Regularization One-shot Yes Local
Compresso [164] Structured Regularization Neither PEFT Global
LLM Surgeon [165] Both Loss-based Iterative PEFT Global
In addition to the magnitude of weight and activation
as Wanda and RIA, E-Sparse (Entropy-based Sparsity) [153]
introduces information entropy from hidden state features
into the pruning metric. The entropy serves as a measure of
information richness, with higher values indicating richer
information. Consequently, entropy is incorporated along-
side standard weight magnitude and input feature norm
in the pruning metric, enhancing the evaluation of channel
information activation.
2) Loss-based Pruning
In loss-based approaches, it is observed that the prun-
ing metrics involve the first or second-order derivatives of
weights with respect to the loss. The second-order methods
discussed in this subsection are all inspired by two earlier
second-order loss-based pruning methods, namely, Optimal
Brain Damage (OBD) [114] and Optimal Brain Surgeon
(OBS) [115].
SparseGPT [154], a second-order pruning method, incor-
porates OBS [115] technique into the GPT-family models.
It is the first pruning method that works efficiently at
models with 10-100+ billion parameters. The SparseGPT
pruning methodology is delineated by two main compo-
nents: mask selection and weight reconstruction processes.
Initially, the mask selection identifies weights for pruning
based on a metric, such as weight magnitude. Subsequently,
the unpruned weights undergo optimization using the OBS
method to reconstruct the compressed model ( i.e., update
the remaining parameters) to compensate for the pruned
weights. The pruning procedure in SparseGPT requires min-
imal calibration data. These data undergo a single forward
propagation, during which the unpruned weights are up-
dated only once. The results of this approach demonstrate
that LLMs can be compressed to high sparsity through
weight pruning in a single pass, without necessitating
the fine-tuning process. Importantly, this compression is
achieved with a low loss of accuracy, as assessed by perplex-
ity and zero-shot performance metrics. Similarly, the LLM
Surgeon [165] extents OBS but is generic for unstructured
and structured pruning.
Building upon the concepts of OBS and OBD, Shao et
al. [155] introduced a novel pruning metric termed theImproved Saliency Criterion (ISC). ISC is devised by adding
the metrics derived from OBS and OBD directly. This new
metric aims to provide a comprehensive and refined as-
sessment of the importance of model parameters for the
pruning process. In addition to proposing ISC, Shao et al.
put forward to allocate sparsity ratio individually to each
matrix. In this way, pruning targets are selected adaptively
within each weight matrix.
In addition to the aforementioned second-order meth-
ods, there has been corresponding research into first-
order methods [156], [157]. Gradient-based Language Model
Pruner (GBLM-Pruner) [156] is a first-order pruning
method. The importance of weights is defined by the prod-
uct with the magnitude of weights and the normalization
of the corresponding gradients across different samples,
which can be seen as an extension of the traditional first-
order method ( i.e., gradient-weight product). Furthermore,
the feature activations can be integrated into the pruning
metric to enhance performance.
4.3.2 Structured Pruning for LLMs
In contrast to unstructured pruning, structured pruning is
not constrained by hardware limitations, enabling the real-
ization of inference acceleration on conventional hardware
following the pruning process. However, these methods
might result in more performance degradation than un-
structured ones due to the alteration of network structures,
necessitating a fine-tuning process to recover performance.
Therefore, while fine-tuning is abandoned in unstructured
pruning for LLMs, it is widely employed in structured
pruning for LLMs but in a parameter-efficient way. Similar
to unstructured pruning, structured pruning for LLMs has
its pioneer method, LLM-Pruner [160], which serves as a
baseline for subsequent methods and facilitates meaningful
comparisons.
The discussion of these structured pruning methods for
LLMs will be presented in the following section. Similarly,
we will introduce these structured pruning methods in
LLMs in the order of pruning metrics, including magnitude-
based pruning, loss-based pruning, and regularization.
1) Magnitude-based Pruning
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 19
Magnitude-based pruning methods for LLMs consider
rows or columns as pruning units [158], [159], [169]. For
instance, the pruning units of FLuctuation-based Adaptive
Structured Pruning (FLAP) [158] are columns. The impor-
tance score of each column of the weight matrix is mea-
sured by the ”fluctuation metric”. This metric is the sample
variance of each input feature which is weighted with the
squared norm of the corresponding column of the weight
matrix. Furthermore, in its pursuit to obviate the necessity
for fine-tuning, FLAP incorporates bias compensation mech-
anisms aimed at mitigating the adverse effects stemming
from the removal of components.
2) Loss-based Pruning
In the realm of loss-based structured pruning methods
applied to LLMs, gradients remain pivotal information, akin
to their significance in medium-size models. The following
methods utilize gradient information in different ways [160],
[161], [162], [170], such as defining pruning structures, se-
lecting pruning targets, and etc. The most notable departure
of these methods from traditional approaches lies in their
avoidance of predefined pruning units ( e.g., attention heads,
neurons). Instead, some of these methods dynamically iden-
tify and designate pruning units.
For instance, LLM-Pruner [160] removes non-critical
coupled structures during the pruning process. These cou-
pled structures are automatically identified and extracted
through the definition of structure dependency ( i.e., connec-
tion dependencies between neurons). A coupled structure
comprises a group of weights. The importance of individual
weights is formulated as the change in loss, expanded using
Taylor expansion to the second order. The diagonal of the
Hessian matrix in the second-order term is approximated by
the Fisher information matrix using first-order information.
Ultimately, the importance of a group of weights is aggre-
gated through summation, production, or other methods to
determine the group’s overall importance. After evaluating
the importance of each group, those with lower importance
are pruned based on a predefined pruning ratio. The fine-
tuning process in LLM-Pruner applies some parameter-
efficient tuning techniques, such as LoRA. This facilitates
rapid and effective fine-tuning of pruned models using a
small amount of data. The experimental results showcase
that when 20% of the parameters are removed, the pruned
model maintains the performance of the majority of the orig-
inal model. However, a more aggressive pruning strategy,
involving the removal of 50% of the parameters, results in a
substantial decline in model performance. This observation
also underscores the difficulty of achieving high sparsity
ratios through structured pruning while maintaining model
performance.
Similar to LLM-Pruner, LoRAShear [161] discovers the
minimal removal structures in the dependency graph. How-
ever, LoRAShear specifically constructs dependency graphs
over LoRA modules, considering their learnable nature.
The analysis of knowledge distribution is then utilized to
identify crucial structures, marking them as unprunable.
A distinctive feature of LoRAShear is the introduction of
LoRA Half-Space Projected Gradient (LHSPG) for progres-
sive structured pruning. LHSPG leverages information from
LoRA modules to identify and remove redundant structures
while preserving the knowledge stored in the importantstructures. This is achieved through the projection of redun-
dant structures onto zero, transferring the knowledge to the
crucial structures.
In contrast to the manual design of pruning features,
Ji et al. [170] proposed a novel approach by employing
a non-neural model, specifically a gradient boosting de-
cision tree (GBDT), as an accuracy predictor. The use of
this accuracy predictor enables further optimization of the
search space and search process for identifying the optimal
pruned model automatically. By training the GBDT as an
accuracy predictor, the model gains the ability to assess and
predict the impact of different pruning configurations on the
accuracy of the neural network, facilitating more efficient
and automated selection of the optimal pruned model.
3) Regularization
In the context of regularization methods applied to
LLMs, contemporary approaches predominantly adhere to
the principles established for earlier medium-size language
models, incorporating some generic refinements and opti-
mizations.
Sheared LLaMA [163] can be viewed as an extension
of CoFi [142]. This approach involves the joint pruning of
coarse-grained and fine-grained modules in Transformer-
based models using L0regularization. The modules sub-
jected to pruning include layers, individual attention heads,
FFN neurons, and hidden dimensions as in CoFi. Sheared
LLaMA introduces two novel and significant components.
The first component is targeted structured pruning, which
frames pruning as a constrained optimization problem. This
formulation aims to learn pruning masks that search for
a subnetwork matching a pre-specified target architecture
while maximizing performance. The second component is
dynamic batch loading, a strategy that loads training data
from each domain in proportion to its rate of loss reduc-
tion. This approach efficiently utilizes data and accelerates
overall performance improvement during training. In a full-
resource setup, Sheared LLaMA achieves compact counter-
parts that outperform models of equal sizes trained from
scratch.
Compresso [164] integrates LoRA into the L0regulariza-
tion. The L0regularization is employed to optimize binary
masks that cover modules including heads, FFN intermedi-
ate neurons, and hidden dimensions. Simultaneously, model
parameters are updated through LoRA in the instruction
tuning process. An innovative aspect of Compresso is the
introduction of a collaborative pruning paradigm where the
pruning algorithm and target LLM work together through a
collaborative prompt to learn the optimal pruning decisions
during the instruction tuning process. The prompt explains
the concept of pruning and its purpose, informs the LLM
that it is undergoing pruning, and encourages the LLM
to better adapt to the pruning process. By incorporating
this informative prompt, Compresso aims to enhance the
LLM’s understanding and cooperation during pruning, con-
tributing to improved performance and adaptation to the
modified model structure.
4.4 Other Topics for LLM pruning
4.4.1 Enhancing Pruning Efficacy for LLMs
Several auxiliary techniques have been developed to en-
hance the efficacy of pruning methods tailored for LLMs,
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 20
including the sparsity ratios tailored for subregions [171],
[172], post-pruning fine-tuning methods [89], [167], [173],
[174], [175], and hardware optimization [176], [177] While
not constituting a novel pruning method, these auxiliary
techniques can readily be integrated with existing pruning
methods for LLMs to enhance overall pruning outcomes.
One such method of tailored sparsity ratios is Outlier
Weighed Layerwise sparsity (OWL) [171]. The experiments
in OWL indicate that the appropriate layerwise sparsity
ratios have a strong correlation with the emergence of
outliers. Therefore, the sparsity ratio of OWL is directly
proportional to the outlier ratio observed within each layer.
Consequently, in contrast to the prevailing LLM pruning
strategies that uniformly apply sparsity levels across all
layers, OWL introduces a customized set of non-uniform
layerwise sparsity ratios. Another approach of post-pruning
fine-tuning methods is Dynamic Sparse No Training [174],
which introduces a training-free fine-tuning method for
sparse LLMs. This allows for slight updates to sparse LLMs,
enabling further refinement without the need for a complete
fine-tuning process. Without the expensive backpropaga-
tion, Dynamic Sparse No Training minimizes the recon-
struction error between the dense and sparse LLMs, in the
fashion of performing iterative weight pruning and growing
on top of sparse LLMs.
The experimental results demonstrate that these tech-
niques can significantly improve the performance of existing
pruning methods, such as Wanda and SparseGPT. These
findings suggest that the potential enhancements to the
performance of pruning can be achieved through various
means unrelated to the cores of the pruning methods.
4.4.2 Future Works of Pruning for LLMs
While the field of pruning for LLMs has yielded fruitful
results, it continues to grapple with significant challenges.
Two primary issues stand out as particularly crucial.
Firstly, the integration of pruning with other methodolo-
gies, such as quantization [154] and knowledge distillation
[163], is essential for achieving competitive performance.
Relative to the achievements of pruning in the domain of
visual models in the past, the current outcomes in LLM
pruning are comparatively less satisfactory. Therefore, a piv-
otal challenge lies in augmenting the inherent effectiveness
of the pruning method, ensuring its proficiency even when
employed independently.
Secondly, the fine-tuning cost is a significant challenge
in the pruning of LLMs. Many pruning methods for LLMs
adopt one-shot pruning without fine-tuning to minimize
the computational burden. Alternatively, some approaches
incorporate parameter-efficient tuning techniques to reduce
training costs. However, such strategies inevitably compro-
mise the performance of the pruned model. Researchers
and practitioners in the field must persist in addressing
the challenge of the inability to execute full fine-tuning,
particularly when dealing with LLMs aiming to enhance the
performance of pruning.
In conclusion, addressing these challenges is imperative
for advancing the effectiveness and practicality of pruning
techniques.5 K NOWLEDGE DISTILLATION
Knowledge Distillation (KD) is a common technique for
compressing and speeding up models. The specific im-
plementation process involves transferring the knowledge
acquired by a complex teacher model to asimpler student model ,
thereby enabling a more concise and efficient representation
of the teacher model’s knowledge.
In Section 5.1, we will introduce some fundamental con-
cepts of knowledge distillation and provide a brief classifica-
tion of knowledge distillation methods. Then we will sum-
marize various knowledge distillation methods employing
medium-size language models (the language models with
around 1 billion parameters) in Section 5.2, and we will clas-
sify them into three groups based on whether distillation oc-
curs during the pretraining phase, the finetuning phase, or
both. We finally provide a detailed overview of knowledge
distillation for large language models (the language models
with over 1 billion parameters), categorizing them as black-
box distillation and white-box distillation in Section 5.3.
5.1 Basic Concepts
Understanding the core of knowledge distillation involves
answering three questions: what is knowledge, between
whom is knowledge transmitted, and how is knowledge
transmitted. Knowledge, in simple terms, is summarized
as the abilities the models possess (classification, reasoning,
etc). In the distillation process, the source of knowledge is
the teacher model, and the recipient of knowledge is the
student model. In other words, a well-trained teacher is
essential, and our goal is to enable the student to acquire
or reinforce the abilities the teacher possesses. However, the
key lies in how knowledge is transmitted. The pioneers of
knowledge distillation, Hilton et al. [178], first used the out-
puts of the teacher and student’s softmax layers to transmit
knowledge. They designed the following loss function to
train the student model, thereby achieving the transfer of
knowledge:
L=α·LD(p(zt, T), p(zs, T))+(1−α)·LS(y, p(zs, T))(23)
where LD(p(zt, T), p(zs, T))represents the difference in
output of the softmax layers between the student and
teacher, LS(y, p(zs, T))represents the difference between
the output of the student’s softmax layers and the ground-
truth labels. Both of them utilize the cross-entropy loss. α
represents the weight coefficient, and the specific expression
forpiis as follows:
pi=exp(zi/T)
Σjexp(zj/T)(24)
where T is employed to amplify the impact of incorrect
labels on the transmission of knowledge, thereby enabling
the student model to acquire more knowledge from a single
sample.
Subsequent researchers have employed a variety of
methods to achieve knowledge transfer, primarily falling
into the following four categories: logit-based KD, feature-
based KD, relation-based KD and black-box KD. In Fig. 5,
we also provide a brief overview of these distillation meth-
ods and their relationships.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 21
Hint Layer 1Hint Layer 3
Hint Layer 2…
Input LayerHint Layer NOutput LayerPrediction
InputHint Layer 1Hint Layer 3
Hint Layer 2…
Input LayerHint Layer MOutput Layer
Input
Teacher StudentMatrix T Matrix SBlack-box KD
Logit -based KD
Feature-based KDRelation -based KDPrediction
White-box KD
Fig. 5: Taxonomy of knowledge distillation
1)Logit-based KD . As the name suggests, logit-based
KD is a distillation paradigm that involves the transfer of
knowledge using the logits from the teacher model. We can
write down the general form of the logit-based knowledge
distillation loss function:
Llogit=L(p(zt), p(zs)) (25)
where L(·)indicates the cross-entropy loss [178], Kullback-
Leibler divergence (KLD) loss [179] and so on.
Clearly, Hilton et al.’s method is a example of logit-based
knowledge distillation.
2)Feature-based KD . Due to the limited knowledge ac-
quired by the student in logit-based knowledge distilla-
tion, researchers aim for better emulation of the teacher’s
behavior. Hence, they introduced feature-based knowledge
distillation. Specifically, this involves matching the outputs
of intermediate layers in both the student and teacher mod-
els, requiring the student not only to know the results but
also to understand the underlying processes. The following
is the general form of the loss function for feature-based
knowledge distillation:
Lfeature =L((ft(x), r(fs(x))) (26)
where ft(·)andfs(·)represent the feature maps of the
teacher model and the student model. L(·)is the function
used to fit features, and r(·)is applied to make feature maps
of the teacher model and the student model have the same
shape.
For example, FitNet [180] leverages feature maps from
intermediate layers of both the teacher and student models
to adjust the parameters of the student model. It also uses
mean squared error (MSE) along with a learnable matrix as
L(·)andr(·).
3)Relation-based KD . Furthermore, researchers aim for
the student to learn how the teacher handles relationships
between different data, leading to the proposal of relation-
based knowledge distillation. This relationship is primarily
manifested in two aspects: the relationship between outputs
at different layers for the same sample and the relationshipbetween outputs for different samples. The general form of
its loss function is as follows:
Lresponse =L((ft(ti, tj), fs(si, sj)) (27)
where ti,tjandsi,sjare feature representations from the
teacher model and the student model. They can represent
outputs from different layers or outputs from different sam-
ples. ft(·)andfs(·)represent the similarity functions.
For example, FSP [181] uses feature maps of the same
size as feature representations, and employs Gram matrix
and MSE as f(·)andL(·).
4)Black-box KD . The three distillation methods men-
tioned above rely on the premise that internal information
of the teacher model is accessible, so they all fall under
the category of white-box distillation (distillation method that
requires access to internal data of the teacher model during
the training process). However, many contemporary closed-
source large models have inaccessible internal information,
and we can only obtain the model’s predictions. The distil-
lation pattern where knowledge is transmitted through the
predictions of the teacher model is referred to as black-box
knowledge distillation.
5.2 KD for Medium-Size Language Models
With the emergence of the transformer architecture, var-
ious medium-size language models based on the trans-
former structure (e.g. BERT, GPT-2, etc), have been pro-
posed. These language models are trained through two
training processes: pretraining and finetuning. Specifically,
in the pretraining phase, we train the model on a large-
scale unlabeled dataset to learn the general features and
structure of language. Subsequently, during the finetuning
process, we further train the model on labeled data to
adapt it to the specific features and requirements of the
given task. Consequently, unlike previous distillation meth-
ods, distillation for these models is categorized into two
classes: finetuning distillation and pretraining distillation.
The student model can receive knowledge transmitted from
the pretrained teacher during the pretraining period or
from the teacher fine-tuned for a specific task during the
finetuning period. We will separately introduce these two
distillation paradigms. Additionally, we have created the
Table 4 to illustrate the training stage, knowledge source
and loss function for various medium-size model distillation
methods mentioned below.
5.2.1 Finetuning Distillation
Finetuning distillation is primarily aimed at compressing
models for specific tasks. Generally, teachers in finetun-
ing distillation are models that have been fine-tuned for
specific tasks. For example, Distilled BiLSTM [182] is the
earliest method to employ knowledge distillation on BERT.
It transfers the knowledge of fine-tuned BERT to BiLSTM
by learning from logits. Therefore, this is a successful imple-
mentation of logit-based knowledge distillation on medium-
size models. Subsequently, many feature-based knowledge
distillations [183], [184] have also been implemented on
medium-size models. They distill knowledge in the embed-
ding layer, transformer layers, and prediction layer, allow-
ing the student model to learn the knowledge mastered
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 22
TABLE 4: A summary of various KD methods for BERT. Embed., Attn., Hidden., and Pred. represent knowledge is from
embeddings, attentions, hidden layers, and model’s prediction, repectively.
KD Method Training stage Embed. Attn. Hidden. Pred. New Knowledge Source
Distilled BiLSTM [182] Finetuning MSE
PKD [183] Finetuning MSE CE
DynaBERT [184] Finetuning MSE MSE CE
Metadistil [185] Finetuning CE
AD-KD [186] Finetuning CE Attribution map (MSE)
AdaBERT [187] Finetuning CE CE Model efficiency
MixKD [188] Finetuning CE MixUp data (CE/MSE)
Meta-KD [189] Finetuning MSE MSE MSE CE Transferrable knowledge (MSE)
ReAugKD [190] Finetuning CE Similarity matrix (KL)
DistilBERT [191] Pretraining COS CE
MiniLM [192] Pretraining KL Value-relation (KL)
MobileBERT [193] Pretraining KL MSE MSE
HomoBERT [194] Pretraining MSE MSE MSE KL
TinyBERT [195] Finetuning and pretraining MSE MSE MSE CE
TED [196] Finetuning or Pretraining KL Filters (MSE)
by the teacher model from various aspects. For example,
PKD [183] introduced a hidden state loss. It selects a subset
of outputs from the intermediate transformer blocks of
both the teacher and student for distillation. Additionally,
it designed two alignment modes, namely PKD-Skip (the
student learns from every k layers of the teacher) and PKD-
Last (the student learns from the last k layers of the teacher),
with experimental evidence demonstrating the superiority
of the former. DynaBERT [184] also takes into account the
width of the model, and it incorporates the idea of pruning.
To be specific, It sets a parameter, the width multiplier
mw∈(0,1), and retains the most important mwattention
heads in the Multi-Head Attention (MHA) layer of the
transformer, as well as the most important mwneurons in
the Feed-Forward Network (FFN), to initialize the student
model DynaBERT w. Then it transfers knowledge from the
teacher model to the width-adaptive DynaBERT wthrough
the embedding layer, hidden states, and the prediction
layer. Following that, it uniformly selects transformer layers
from DynaBERT wusing the depth multiplier md(similar
to PKD-skip) to initialize the student model DynaBERT .
Knowledge is then transferred from DynaBERT wto both
the width-adaptive and depth-adaptive DynaBERT using
the same knowledge source as in the width-adaptive pro-
cess. Metadistil [185] points out two common issues in
general distillation: the teacher cannot perceive the stu-
dent’s abilities, and a strong teacher may not necessarily
be effective in teaching good students. To address these
problems, it proposes a novel distillation approach: first
distill a copy S’ of the student S on training data, then use
the updated S’ to update the teacher model on quiz data,
allowing it to learn to teach. Finally, use the updated teacher
model to distill S on training data. AD-KD [186] focuses on
the importance of each token to the prediction results. It
aims for the student model to understand which tokens the
teacher model prioritizes when generating predictions, thus
learning the rationale behind the teacher model’s reasoning.
Some methods [183], [184] mentioned above can be
applied to pretraining distillation from the perspective ofoperational feasibility, but Turc et al. [197] has demonstrated
that simple pretraining distillation methods result in signif-
icant distillation losses. Therefore, the effectiveness of the
student models distilled using these methods may not be
ideal. Besides, some methods [185], [186] have not been
utilized in pretraining distillation. The applicability of these
methods to pretraining distillation remains to be explored.
Considering the fact that finetuning distillation is tai-
lored for specific tasks, many other methods also utilize pro-
prietary knowledge sources, enabling students to acquire
knowledge these specific tasks need from the teacher model
more efficiently. So, these methods cannot be applied to
pretraining distillation.
For instance, AdaBERT [187] employs a search space to
enable adaptive changes in the student’s structure. Specifi-
cally, the search space consists of multiple layers, with each
layer comprising input nodes, output nodes, and hidden
internal nodes that form a directed graph. The edges of this
graph represent candidate operations selected from a series
of lightweight operations based on CNN. Considering the
size and efficiency of the student model, AdaBERT incor-
porates not only soft and hard targets for distillation but
also includes the normalized parameter size and number of
floating-point operations of the student model in the loss
function. Ultimately, this loss function is used to choose
appropriate CNN-based operations. However, MixKD [188]
starts with the dataset and applies MixUp [198] to KD in or-
der to address the issue of limited training samples leading
to insufficient knowledge acquisition by the student. It uses
zero padding to make all sentences the same length, and
then interpolates the word embeddings and labels of two
training samples to obtain an augmented sample. Then it in-
corporates the loss of mixup samples into the loss function.
Meta-KD [189] recognizes that when a student is learning in
one domain, they may benefit from auxiliary knowledge in
other domains. For example, a physics student may find
it easier to grasp physics equations under the guidance
of a teacher proficient in both physics and mathematics.
Hence, training an ”all-purpose teacher” model for domain-
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 23
specific student models becomes essential. More precisely, it
constructs a learnable sub-network using the output of the
last hidden layer for each instance. This sub-network is ca-
pable of distinguishing the domain of each instance, making
the knowledge transferable and not restricted by domain
limitations. During the distillation process, the teacher is
tasked not only with conveying knowledge encompassed
by input embeddings, hidden states, attention matrices, and
output logits but also with transmitting this transferable
knowledge. ReAugKD [190] take the inference phase into
consider. It uses an external memory derived from rel-
evant task-specific knowledge of the teacher to enhance
the effective capacity of the student. In the distillation
phase, it adds a linear projection head, which has been
fine-tuned for downstream tasks, on top of the teacher
model’s encoder to generate the teacher embedding and
obtains the student embedding from the last transformer.
Then it trains with a relational KD loss that minimizes
the divergence between teacher-teacher and teacher-student
embedding distributions. They found that this distillation
method can effectively enhance the student model’s ability
to retrieve external information. In the inference phase, it
constructs a knowledge base with the teacher’s soft labels
and predictions. Then, it processes the top-k data entries
from the knowledge base that are most similar to the student
embedding. The final prediction is obtained by weighting
and combining the student’s predictions with these pro-
cessed entries from the knowledge base.
Besides, Enhanced KD [199] proposes a new distillation
loss function by expanding the loss in a Taylor series,
which allows for effective distillation even when the teacher
model is not fine-tuned for a specific task. This approach re-
duces a significant amount of training cost and architecture-
agnostic.
5.2.2 Pretraining Distillation
The primary objective of pretraining distillation is to obtain
a pretrained model with fewer parameters and good gen-
eralization capabilities. So some of them [191], [193], [194]
utilize the loss function employed during the training of
BERT. DistilBERT [191] is the first to introduce pretraining
distillation for BERT. It transfers the idea of PKD-skip [183]
(the student learns from every k layers of the teacher) to
pretraining distillation and employs the cosine similarity
loss function to facilitate the transfer of knowledge within
hidden states. MiniLM [192] places the emphasis of distilla-
tion on the last transformer layer. It utilizes the self-attention
distributions and self-attention value-relation (dot-product
of the value matrix with itself) from this layer to acquire
knowledge and perform distillation. This approach cleverly
allows the student to have more flexible layer numbers
and hidden dimensions. Hence, it can straightforwardly
distill the teacher into a teacher assistant [200] with smaller
hidden dimensions and then distill the teacher assistant
into a student model with fewer layers, thereby enhancing
the performance of the student model. MobileBERT [193]
and HomoBERT [194] put emphasis on model width as
DynaBERT [184], but they just alter models’ width while
preserving their depth because Turc et al. [197] proves that
the impact of depth on model performance is more signif-
icant. MobileBERT adds bottleneck and inverted-bottleneckto both the teacher and student models to alter the hidden
dimensions. However, the practical implementation of this
approach may disrupt the balance of parameters between
Multi-Head Attention (MHA) and Feed-Forward Network
(FFN). Therefore, the authors address this issue by adopting
a stacked FFN approach. Then it distills knowledge through
the attention and hidden states of transformer layers. Ho-
moBERT utilizes the concept of pruning as DynaBERT. But
it initializes the student with the teacher model so that it
can maintain small discrepancy compared to the teacher
model. Then it derives the distillation loss function using
input embeddings, hidden states, attention matrices, and
output logits as the pruning objective. In each iteration, it
removes the least important neurons from the student based
on importance scores and guides the student’s training
using the distillation loss. This process is iteratively repeated
throughout the entire training until the student reaches the
target size. TinyBERT [195] combines pretraining distillation
and finetuning distillation so that TinyBERT can capture the
general-domain as well as the task-specific knowledge in
BERT. It also distills various knowledge from the embedding
layer, hidden states and attention matrices of transformer
layers, and the prediction layer. But the ablation studies
show that finetuning distillation has a more significant
impact than pretraining distillation. TED [196] equips each
layer with a task-aware filter (a neural network with a task-
specific head) to extract knowledge from the hidden repre-
sentation of this layer. It has achieved promising results in
both pretraining and finetuning scenarios.
5.2.3 Discussion
Finetuning distillation is computational costly because
switching to a new task always requires the training of a
task-specific teacher. So many finetuning knowledge distil-
lation methods [189], [190], [199] are proposed to reduce
the computational cost of the finetuning process. But in
pretraing distillation, student is distilled from a teacher
pretrained on open-domain data and can be efficiently fine-
tuned on various downstream tasks, which reduces the
computational cost associated with distillation for multiple
specific tasks to a certain extent. However, pretraining distil-
lation also comes with many new challenges. For example,
teacher models have larger capacity and stronger represen-
tation capabilities than student models, it is challenging for
students to produce predictions that match the teacher’s on
a large amount of open-domain training data. Therefore, for
general methods, the choice between pretraining distillation
and finetuning distillation depends on the trade-off we
make between model size and performance.
5.3 KD for Large Language Models
Recently, an increasing number of large language mod-
els(LLMs) have been developed. However, many of these
large models are closed-source, which imposes significant
limitations on knowledge distillation for such models. While
the student model cannot acquire knowledge from internal
information, we can still use the teacher model’s responses,
the remaining source of knowledge, to transfer information
to the student model. Depending on whether the source of
knowledge for the student model is solely the answers pro-
vided by the teacher model, distillation for large language
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 24
models can be categorized into black-box distillation and
white-box distillation.
5.3.1 Black-box Distillation
Even though conventional distillation methods may no
longer apply, some unique properties of LLMs allow us
to find a breakthrough. Researchers have found that when
the models’ parameter is large enough, they exhibits sur-
prising emergent abilities, enabling it to tackle intricate
tasks. Many black-box distillation methods leverage that
abilities, and there are typically three methods commonly
in use: Instruction-Following, Chain-of-Thought (CoT) and
In-Context Learning.
1) Instruction-Following
Instruction-following capability means that the LLMs
can generate corresponding outputs based on a specific
instruction (directing the model on what task to accomplish)
and the input (data required to fulfill that instruction).
Due to the fact that black-box distillation can only transfer
knowledge through datasets, it necessitates a sufficiently
comprehensive dataset. Therefore, the common effort in
this method [201], [202], [203], [204] involves constructing a
large dataset (comprising instructions, inputs, and outputs)
to enable the student models to learn as much as possible
from the teacher models. Specifically, SELF-INSTRUCT [201]
employs a self-distillation approach, where the model serves
both as the teacher and the student. It starts by obtaining
a manually curated small-scale task pool, where each task
consists of an instruction and a corresponding input-output
pair. Subsequently, it selects a subset of instructions from the
task pool as in-context examples for the model to generate
new instructions and matching inputs and outputs for them.
Finally, it filters out data with excessive redundancy or
content that cannot be handled by language models, placing
the qualified data back into the task pool. This iterative
process continues to generate an extensive dataset for fine-
tuning the student model. This has become a paradigm for
instruction-following distillation, and the 13B open-source
models Alpaca [205], Vicuna [206] and GPT4All [207] were
trained with some adjustments based on this paradigm.
Also, following this idea, LLaMA-GPT4 [202] and LaMini-
LM [203] construct their respective instruction sets and
fine-tune smaller models. Compared to SELF-INSTRUCT,
their breakthroughs are as follows: LLaMA-GPT4 generates
a 52K instruction-following dataset in both English and
Chinese using GPT-4, and fine-tunes two student models,
LLaMA-GPT4 and LLaMA-GPT4-CN. Additionally, it trains
a reward model specifically for evaluating the quality of
model responses. LaMni-LM enriches the types of models
used for generating instructions and the topics of instruc-
tions, constructing a massive dataset of 2.58M for finetuning
smaller-parameter student models, which achieves good
results. However, in the methods mentioned above, the
student model is not involved in the selection of the dataset,
so the teacher model cannot receive timely feedback from
the student model during the dataset generation process. In
response to this issue, Lion [204] adopts adversarial knowl-
edge distillation, where the student model not only learns
from the teacher model’s responses but is also evaluated
by a referee to assess its difference compared to the teacher
model. This helps to identify ”hard” instructions where thestudent model’s performance falls short, thus generating
new ”hard” instructions so that teacher models can achieve
feedback in the learning process. PERsD [208] evaluates the
student’s attempt with unit test cases and gets execution
feedback. Then it prompts the teacher model to refine the
student’s attempt so that the student can be trained on
personalized data.
Some work focuses on task-specific instruction-
following distillation. For instance, UniversalNER [209]
conducts in-depth research on Named Entity Recognition
(NER) tasks. So, unlike the methods mentioned above that
increase the diversity of instructions, its emphasis is on
enhancing the diversity of inputs to improve the model’s
generalization across multiple domains. To be specific, it
directly samples inputs from a large corpus across diverse
domains, and then uses a LLM to generate outputs. After
obtaining the data, it trains the student model using a
conversation-style tuning format, enabling it to identify
entities of each entity type contained in the input text.
Furthermore, this approach of using large language
models to construct reinforced datasets for finetuning stu-
dent models is not unique to instruction-following but
rather a common method of black-box distillation.
2) Chain-of-Thought
Chain-of-Thought capability refers to the ability of a
large language model to provide better answers to questions
based on the rationale within the given prompts. The typical
paradigm of CoT [210], [211], [212], [213] distillation utilizes
large models to generate reinforced datasets containing ra-
tionales, which are then used to fine-tune the student model.
Hence, the issues of interest revolve around how to generate
high-quality rationales for training [210], [214], [215], [216],
[217], [218], [219], [220], [221], [222], [223] and how to en-
sure that students effectively leverage these rationales [210],
[212], [215], [216], [217], [223], [224].
Li et al. [210] systematically explores three explanation
generation approaches from LLMs and three multi-task
learning with explanations methods. Finally it finds that
CROP (Chain of Thought with Rationalization Prompting
backup) and MT-CoT (Multi-task Learning with Chain of
Thought) are outstanding methods. In detail, CROP refers to
a process where, for a dataset containing questions and an-
swers, the teacher model first produces an explanation and
an answer based on the question. If the answer is correct,
the explanation is retained. If the answer is incorrect, the
teacher model generates an explanation based on the ques-
tion and the correct answer. Ultimately, a dataset is obtained
with questions, explanations, and answers for finetuning
the student model. MT-CoT refers to a training process for
the student model with two tasks. The model is not only
required to learn predicting answers but also to provide ex-
planations. Moreover, in the task of providing explanations,
the model needs to arrive at the correct answer through
the reasoning steps it takes. Further, Distilling Step-by-Step
[212] demonstrates that good results can be achieved even
when the original dataset only consists of questions with-
out answers. Fine-tune-CoT [214] applies existing zero-shot
CoT prompting to generate rationales from large teacher
models, and uses them to fine-tune smaller student models.
It also proposes diverse reasoning to augment the training
data for student models so that student models can have
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 25
better performance. Besides, SCoTD [220] and MCC-KDc
[221] also conducts in-depth explorations on the diversity of
rationales. Fu et al. [225] found that it is indeed possible to
transfer the student model’s capabilities from general tasks
to tasks specific by employing CoT distillation. SOCRATIC
CoT [215] decomposes a question into several sub-questions
to guide the generation of rationales. It starts by selecting
a subset of data from the dataset, manually decomposing
questions, and providing answers for each sub-question.
These serve as examples given to the LLM to generate sub-
questions and answers for the remaining data. The resulting
dataset is reinforced by filtering based on the correctness of
the final results. Two student models are then trained using
this dataset, one for questioning and one for answering
questions. SCOTT [216] takes into account two issues in
CoT. Firstly, the rationale generated by the teacher model
may not match the answer or be meaningful. Secondly, the
student model may struggle to connect rationale and answer
during learning. To address these challenges, SCOTT em-
ploys contrastive decoding during the rationale generation
process to make the model pay more attention to the answer.
This requires the teacher model’s decoding process to be
adjustable. In the training process of the student model,
SCOTT introduces counterfactual rationales to guide the
student in obtaining different answers, thereby establishing
a closer relationship between rationale and answer. KARD
[217] addresses the issue of limited memory capabilities
in small models by retrieving information from external
knowledge bases. Program Distillation [218] and PaD [219]
both leverage programs as rationales and have achieved
promising results on math word problems. DOCTOR [222]
utilizes a teacher model to generate question-answer-style
rationales containing commonsense knowledge, and then
filters and selects high-quality multi-hop reasoning for train-
ing students. Wang et al. [223] build an interactive multi-
round learning paradigm, where the student first provides
its learning status to the teacher LLM who then can pro-
vide customized rationales as the feedback to the student.
They also exploit the reasoning potential of smaller LM by
eliciting it to take self-reflection on the mistakes.
3) In-Context Learning
In-context learning (ICL) is also a manifestation of the
emergent capabilities of large models, referring to the ca-
pacity of large models to generate correct outputs for new
inputs based on some input-label examples without up-
dating model parameters. Based on it, In-context Learning
Distillation [226] utilizes two few-shot learning paradigms,
namely Meta In-context Tuning (Meta-ICT) and Multitask
In-context Tuning (Multitask-ICT), to transfer the in-context
learning capabilities of teacher models to student models
by distillation. In Meta-ICT, it enables the student model
to adapt to unseen tasks through in-context learning and
assistance from the teacher. But in Multitask-ICT, it treats
all target tasks as training tasks and directly employs ex-
amples from target tasks in in-context learning distillation.
The results demonstrate that multi-task in-context tuning is
more effective, although it comes with higher computational
costs. LLM-R [227] initially trains a reward model based on
LLM feedback to evaluate the quality of candidate exam-
ples, followed by knowledge distillation to train a retriever
that can identify high-quality in-context examples for LLMs.4) Others
In addition to the three paradigms mentioned above,
there are other methods that generate specific reinforcement
datasets to enable the student model to acquire specific
capabilities. For instance, Symbolic Knowledge Distillation
[228] utilizes a LLM to gather data and filter it, thereby ob-
taining high-quality Commonsense Knowledge Graphs for
training a Commonsense Model. DISCO [229] uses a LLM to
obtain counterfactual data and employs a large teacher NLI
model for filtering, thus obtaining a high-quality dataset
to improve students’ abilities in natural language inference
(NLI) tasks. PubMedBERT [230] conducts a case study on
adverse drug event (ADE) extraction and proposes a novel
framework that simultaneously handles adverse event (AE)
entity extraction and ADE relation extraction to reduce com-
putational requirements. Promptmix [231] utilizes LLMs
to mix and relabel text data for classification problems in
proportion, aiming to obtain a stronger dataset for training.
However, Gudibande [232] demonstrates that continu-
ally increasing imitation training data can lead to the model
simply imitating without understanding, thus enhancing
the capabilities of the base model is also an indispensable
aspect of black-box distillation.
5.3.2 White-box Distillation
Compared to black-box distillation, the work on white-
box distillation is relatively limited, but there is still some
exploration. For example, MINILLM [233] and GKD [234]
both focus on the loss function and they find that forward
KL divergence overestimates the void regions of the teacher
distribution in language generation tasks when the student
model distribution is insufficiently expressive to cover all
the modes of teacher distribution. But reverse KL divergence
focuses on the major modes, allowing the student to learn
the main part of the teacher’s distribution. Furthermore, it
doesn’t force the student to exactly match the teacher’s dis-
tribution but aims to leverage the information provided by
the teacher to assist in the student’s training. So MINILLM
samples from the student distribution and uses the Policy
Gradient Theorem to calculate the reverse KL divergence.
Also due to the high variance and reward hacking policy
gradient suffers from, it comes up with the single-step
regularization, teacher-mixed sampling and length normal-
ization to solve these problems. Similar to MINILLM, GKD
utilizes reverse KLD and Jensen-Shannon divergence (JSD)
to enhance the student’s expressive capacity. But it uses on-
policy KD to alleviate the distribution mismatch between
training and evaluation, which involves sampling from
the student distribution without backpropagating through
student’s sampling process—something that MINILLM re-
quires. It’s proved that this gradient handling approach is
relatively simple yet effective. Padmanabhan et al. [235]
generate a transfer set by prompting a language model to
generate continuations from the entity definition and then
update the model parameters so that the distribution of
the student matches the distribution of the teacher on the
transfer set. TSLD [236] utilizes logit distillation to reform
intermediate representations and applies token-wise logit
scaling, reducing the errors introduced when QAT is applied
to generative language models. MiniMA [237] finds that the
optimal distillation effect occurs when the student model
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 26
is approximately 40% of the size of the teacher model’s
parameters. It utilizes LLaMA2-7B for structured pruning
and logit-based knowledge distillation to train a 3B MiniMA
model.
Due to the limitations imposed by the closed-source
nature of large language models, white-box distillation
has faced constraints. However, with the emergence of
increasingly diverse open-source large language models
(e.g. Alpaca, Vicuna), white-box distillation holds significant
promise for the future.
6 C OMPACT ARCHITECTURE DESIGN
Compact architecture design is a philosophy that pursues
efficiency and streamlining, and it aims to achieve a signifi-
cant increase in model efficiency by optimizing the network
structure and algorithms while reducing the consumption
of computational resources and memory usage. Specifically,
it can be divided into two levels of research: micro and
macro. This section will focus on optimizing the attention
computation and the Transformer architecture design. Since
the Transformer layer is currently the main component of
the LLM, and it makes no difference for large and medium-
size models, so we will not specifically categorize methods
by model size here.
6.1 Efficient Attention
The standard self-attention mechanism of the Transformer
has a time and space complexity of O(N2)for sequence
length N, which significantly limits its further expansion in
various fields and prevents it from handling long-sequence
problems. To solve this problem, many works have emerged
to improve attention, many of which have focused on im-
proving computational and memory efficiency. We refer to
these works as Efficient Attention . Based on the starting
point and method characteristics, we divide these works
into three categories: Sparse Attention ,Linear Approxi-
mate Attention ,and Flash Attention . There are also some
unique works, such as Transformer-XL [238], which do not
improve within the attention operator and, therefore, will
not be discussed here.
6.1.1 Sparse Attention
The Sparse Attention approaches [239], [240], [241], [242],
[243], [244], [245], [246], [247], [248], [249], [250] allow each
token to attend only locally or predominantly relevant items
to implement the sparse attention pattern, thus reducing
computational and memory complexity. Based on the char-
acteristics of these methods, we categorize them into stride-
based, window-based, and data-based methods.
1) Stride-based Methods
The stride-based methods [239], [240], [241] reduce com-
putational complexity by having each token attend to sev-
eral preceding tokens of length stride to achieve sparse
attention patterns.
[239] is an earlier work. It offered two ways to de-
compose attention: strided (Fig. 6 (b)) and fixed attention
patterns. These ways allowed each query to attend only to
preset positions, reducing the complexity of self-attention to
O(N√
N). However, this method has limited applicability
(a) Full attention (b) Sparse attention (strided)
(c) Window attention (d) Global attentionFig. 6: Comparing the sparse attention patterns. (a) full self-
attention, (b) strided attention, (c) window attention, (d)
global attention.
unless we can design appropriate sparse attention kernels
for various scenarios. By observing the distribution of at-
tention across different heads in a standard Transformer,
[240] found that not all heads attend to the entire context
(some heads only focus on the nearest tokens) and proposed
learning dynamic attention spans for each head to reduce
computational and storage costs. However, both previous
works can only attend to past consecutive spans of tokens.
To solve this problem, [241] introduces α-entmax to replace
softmax, allowing low-rated words to receive precisely zero
weight, thus enabling more flexible sparse attention.
2) Window-based Methods
Unlike the above approaches, the window-based meth-
ods divide the input into individual localized windows so
that each token only attends to items inside the window
(Fig. 6 (c)), thus reducing the computational complexity.
An early method [243] achieves a complexity of O(N2
n)
by dividing the Q,K, and Vmatrices into nblocks (padding
is used if not divisible) and calculating attention within each
block by shifting one position. This method is more straight-
forward to implement than Sparse Transformer. However, it
is worth noting that nis typically set to 2or3in practice
to maintain performance, which results in poor actual accel-
eration. To solve this problem, [244] achieves O(N(k+m))
complexity by using a dilated sliding window to increase
the receptive field without increasing the computation and
improving the performance by adding ”Global Attention”
(Fig. 6 (d)) to the pre-selected input positions. Here, k
represents the size of the sliding window, and mrepre-
sents the number of pre-selected positions. However, its
extended implementation requires efficient banded matrix
multiplication support, and using the naive CUDA kernels
can only have the running speed as standard self-attention.
Therefore, in practice, it only has a consistent memory foot-
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 27
print with theoretical complexity, but there is a gap between
actual running speed and theoretical complexity. Similarly,
[246] established a sparse attention pattern consisting of
three main components:
•Global attention: A global token set gwhere tokens
within the set attend to the entire sequence, and all
tokens in the sequence attend to set g.
•Local attention: All tokens attend to a set wof sur-
rounding windows around themselves.
•Random attention: All tokens attend to a random
token set r.
It is worth noting that there is also a gap between BigBird’s
actual running speed and theoretical complexity, similar to
[244].
In addition to the methods described above, some meth-
ods [242], [249] let each token attend directly to close and
indirectly to distant locations. This method is very similar
to the window-based methods introduced above but with
a different indirect way to implement ”Global Attention.”
Therefore, we present them here.
[242] proposes a new architecture, BP-transformer
(BPT), based on the prior observation that elements closer
together have higher attention scores, while elements fur-
ther away have lower attention scores. It treats attention
calculation as a graph neural network and partitions the
input sequence into different multi-scale spaces through
binary partitioning (BP), constructing a binary tree-based
attention pattern. Each leaf node represents a token, and
each token focuses on different scale nodes based on the
target distance, thereby reducing the complexity of attention
toO(kNlog (N/k))where kis a hyperparameter controlling
the density of attention. The core idea of [249] is very similar
to BPT in that it enables each token to attend to all other
items directly or indirectly. The difference is that it treats the
attention mechanism as a conditional expectation problem.
3) Data-based Methods
Unlike the above methods that need to design sparse
patterns manually, data-based methods [245], [247], [248],
[250] make each token automatically and quickly find the
most relevant items to compute attention using appropriate
algorithms. The most significant advantage of these meth-
ods is data-awareness, which effectively avoids the disad-
vantage of having to re-design the sparse patterns manually
in the case of different tasks and data, and it isn’t easy to
obtain the optimal solution.
Reformer [245] achieves efficient sparse attention com-
putation by using locally sensitive hashing to find similar
vectors quickly, reducing the complexity to O(Nlog (N)).
At the same time, Reformer also uses techniques such as re-
versibility layers and chunking in FFN layers to significantly
reduce memory usage during training. However, this trade-
off may also slow down the training speed. In addition,
to avoid hash errors, Reformer requires multiple rounds of
hashing, weakening its final efficiency benefits. Similarly to
Reformer, [250] views self-attention as a routing problem.
Specifically, it is based on k-means clustering, which allows
queries and keys to cluster on the same set of cluster center-
of-mass vectors by letting the model learn to select sparse
clusters of word examples. So that each query Qiattends
only to the keys that belong to the same cluster as it does.To ensure performance, it sets the number of clusters to√
N,
which reduces the attention complexity to O(N√
N).
Other works related to sparse attention based on input
are SAC [248] and SSA [247]. Among them, SAC regards
the input as a graph and uses an LSTM edge predictor to
learn the edges between tokens. The nodes in the graph
represent tokens, and the edges represent attention rela-
tions. It also uses reinforcement learning to train this edge
predictor. However, LSTM has limitations, such as a lack
of parallelism and a limited ability to express long-term
dependencies. There may be better methods available for
building an edge predictor. On the other hand, SSA is based
on the differentiable sorting of internal representations and
introduces a meta-sorting network that can learn to gen-
erate potential orderings on sequences. It allows us to use
only local windows for quasi-global attention after a given
ordering sequence, improving the memory efficiency of the
attention module.
6.1.2 Linear Approximate Attention
The standard attention can be represented as:
Attention( Q, K, V ) = softmax( QKT)V (28)
Since QKTis quadratic in sequence length and memory
complexity, this severely limits applying attention to long
sequence scenarios. Therefore, several methods devoted to
linearized attention computation have been proposed to
address this dilemma. Based on the characteristics of these
methods, we categorize them into associativity-based and
low-rank-based methods.
1) Associativity Based Methods
The natural idea is that if we can calculate KTVfirst
utilizing the associativity of matrix multiplication, we can
achieve linear complexity in attention computation. How-
ever, due to the presence of softmax, we cannot easily
implement this. For each row iin the attention result, we
can equivalently represent it as:
Attention( Q, K, V )i=Pn
j=1sim(qi, kj)vjPn
j=1sim(qi, kj)(29)
Where sim(qi, kj) =eqikT
j, it is actually a weighted average
ofvjwith weights given by eqikT
j. A natural thought is that
if we can find two functions ϕ1(x)andϕ2(x)such that:
sim(qi, kj) =ϕ1(qi)ϕ2(kj)T(30)
and satisfy sim(qi, kj)>= 0 all the time, and also satisfy:

ϕ1(qi)ϕ2(kj)T
vj=ϕ1(qi)
ϕ2(kj)Tvj
(31)
Then, we can achieve linear attention. Building on this idea,
many different approaches to linear attention have been
proposed [251], [252], [253], [254], [255], [256].
Specifically, [251] achieves this by constructing:
ϕ1(x) =ϕ2(x) =elu(x) + 1 =1 +x x⩾0
exx <0(32)
Performer [252] also achieves linear attention through a
kernel function method. It proposes a FAVOR+ method
that cleverly uses random projection to project the input
features orthogonally. Without relying on any prior and
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 28
without loss of accuracy, it successfully realizes the linear
attention. Specifically, by taking ϕof the following form
for functions f1, ..., f l:R→R, function g:Rd→R
and deterministic vectors ωiorω1, ..., ω miid∼ D for some
distribution D ∈ P (Rd):
ϕ(x) =h(x)√m(f1(ω⊤
1x), ..., f 1(ω⊤
mx), ..., f l(ω⊤
1x), ..., f l(ω⊤
mx))
(33)
To better describe fi,handωiinϕ, for the element A(i, j) =
exp(qikT
j)in the ith row and jth column of the original
attention matrix A, we give it a generalized definition:
SM(x,y)def= exp( x⊤y) (34)
In fact, as early as [257] there was an approximate expres-
sion for SM(x,y)with h(x) =exp(||x||2
2),l= 2,f1=sin,
f2=cos. Since the previous methods appear to have sin
and cos trigonometric functions and instabilities such as
negative numbers may appear in the computed results,
Performer proposes another, more stable, approximation:
SM(x,y) =Eω∼N(0,Id)
exp
ω⊤x−∥x∥2
2
exp
ω⊤y−∥y∥2
2
= ΛEω∼N(0,Id)cosh( ω⊤z)(35)
where Λ = exp( −∥x∥2+∥y∥2
2),x,y∈Rd,z=x+yand cosh
is hyperbolic cosine. This is equivalent to making:
h(x) =exp(−||x|||2
2), l= 2, f1(x) =exp(x), f2(x) =exp(−x)
(36)
However, to ensure accuracy, the number of random sam-
ples mis usually larger than the feature dimension d,
which means that when dealing with short sequences, the
Performer may not perform as well as the standard Trans-
former. Only when the sequence is relatively long can its
advantage be fully leveraged. Similarly, [256] achieves linear
approximate attention through a double softmax approach:
Attention( Q, K, V )≈softmax 1(Q)softmax 2(K)V (37)
Where, softmax 1,softmax 2refer to softmax operations in
the first (N)and second (d)dimension, respectively. How-
ever, directly softmaxing Q, KTseparately, i.e., without
similarity (inner product) computation, gives the impres-
sion of running counter to the attention mechanism. [253]
builds on this by first considering Q, K asnd-dimensional
vectors, respectively, and then clustering them into matrices
consisting of mcluster centers ˜Q,˜K∈Rm×d. In addition,
it inserts a matrix M∈Rm×min the middle such that the
final attention computation can be represented as
Attemtion( Q, K, V )≈softmax
Q˜K⊤

softmax
˜Q˜K⊤−1
softmax
˜QK⊤
V(38)
which is closer to the standard attention.
Recently, HyperAttention [255] simplified the existing
algorithm based on Kernel Density Estimation (KDE), iden-
tified the main entries in the attention matrix through
Hamming-ordered locally sensitive hashing, and proposed
a simple linear time attention approximation algorithm. Thisalgorithm can achieve a wide range of linear approximation
attentions while ensuring the spectral properties of atten-
tion and supporting causal masks. It is worth noting that
the acceleration effect of HyperAttention can be tens of
times different in two cases of using causal masks and not
using causal masks. At the same time, if HyperAttention
completely replaces all layers of attention, the model per-
formance will be significantly reduced, so this method still
needs to balance speed and performance.
2) Low-rank Based Methods
Other methods [258], [259] to achieve linear attention
are through the utilization of low-rank property. Linformer
[258] observed that the normalized cumulative singular
values of the attention matrices in the Transformer model
exhibit low-rank properties across multiple tasks. Based on
this observation, Linformer preserves the original Scaled-
Dot Attention formulation but projects KandVwith two
matrices E, F ∈Rm×nbefore computing attention, en-
abling linear approximate attention computation, formally
expressed as:
Attention (Q, K, V )≈softmax (Q(EK)T)FV
In order to maintain its performance, it’s essential to set
the value of mhigh enough. However, this can result
in Linformer processing short sequences relatively slowly.
Consequently, there might be a significant difference be-
tween the theoretical complexity and the practical usage of
Linformer.
Recently, Transformer-VQ has adopted a unique per-
spective, performing Vector Quantization (VQ) on the key
matrix Kin attention calculation. This is achieved by mak-
ing each vector in Kclosest to the vector in C, where C
is the training parameter and the VQ codebook. VQ can
be mathematically represented as: bK=V Q(K, C), K∈
Rn×dk, C∈Rc×dkSince each vector in bKis from C, we
can first calculate QCT, which is linear due to the fixed size
ofC. Transformer-VQ cleverly constructs a △ ∈ { 0,1}n×c
such that:
exp(QKT)V=exp(QCT△T)V=exp(QCT)(△TV)(39)
The computational complexity of this calculation is
O(ncdk+ncdv+ncdv) =O(n), achieving linear attention.
Moreover, this method can naturally be applied to autore-
gressive tasks, making it a promising approach.
Unlike the previous approximate attention methods,
FlashAttention [260] focuses its improvements on reducing
the memory access overhead with great success. It achieves
acceleration of training and inference while reducing the
memory footprint of attention. More importantly, it is not an
approximate attention method, meaning its computational
results are precisely equal to the standard attention results.
At its core is tiling, which is the chunked transfer of
matrices involved in computation to shared memory to im-
prove overall read and write speed. For example, as shown
in Fig. 7 , suppose we now need to compute the upper half
of the result of multiplying a matrix Awith a matrix B,C0.
For standard matrix multiplication there is C0= (A0B0+
...+A0B3)concat (A1B0+...+A1B3). And for Tiling matrix
multiplication, C0= (A0B0+A0B1)concat (A1B2+A1B3)
reduces the memory access to half of the standard matrix
multiplication.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 29
Fig. 7: (a):Standard matrix multiplication, (b):Tiling matrix
multiplication.
In fact, in the Naive version of matrix multiplication,
only one row and one column of the two matrices are
read from memory each time for computation. The memory
access efficiency is very low, and the two examples here
read the same number of elements from the matrices A and
B, respectively, just for comparability. For matrix multipli-
cation, we can use Tiling directly by chunking. Still, there
are softmax operations in attention, and the denominator of
softmax contains the summation term associated with all the
elements, so the real difficulty in applying tilting to attention
lies in the chunking of softmax.
For standard attention, we usually use the numerically
safe softmax:
Softmax (xi) =exi−m
PN
j=1exj−m(40)
where mdenotes the maximum of all xi. To get the final
result, we need three rounds of iterations:
•Iterate over all xito find the maximum value m
•Iterate through all xito find the sum =PN
j=1exj−m
•Calculating each Softmax (xi)
Since each round of iteration depends on the results of
the previous iterations, the computation cannot be done
independently in chunks. One existing method is to define
a single sequence l′:l′
i=Pi
j=1exj−mi, thus having:
l′
i=iX
j=1exj−mi=
i−1X
j=1exj−mi
emi−1−mi+exi−mi
=l′
i−1emi−1−mi+exi−mi(41)
It’s a matter of cobbling together aPi−1
j=1exj−miout and
replacing it with an incremental computation of l′
i−1, and
it’s clear that, after we get to this point, our sequences canbe computed in the same round of iterations as l′andm,
and in the end, l′
nwill be equivalent to l, and in that way
we’ll be able to reduce the three rounds of iterations to two
rounds of iterations. However, the two-step iteration is still
coupled and cannot be chunked for separate computations.
Inspired by the previous derivations, FlashAttention derives
methods to obtain the final O-matrix after one round of
iterations. A row in matrix Ois a weighted summation of V
and Softmax results, which can then be expressed as:
oi←NX
j=1exj−mN
lNV[j,:]
(42)
Using and the same trick, introduce a sequence of o′alone
and let it participate in the computation using the local mi
andl′
i:
o′
i←iX
j=1exj−mi
l′
iV[j,:]
(43)
It is easy to see that for N,oiis equal to o′
N, and the
problem translates into figuring out how to cobble together
aPi−1
j=1
exj−mi−1
l′
i−1V[j,:]
out of the formula replacing it
with o′
i−1:
o′
i=iX
j=1exj−mi
l′
iV[j,:]
=
i−1X
j=1exj−mi−1
l′
i−1V[j,:]
emi−1
emil′
i−1
l′
i+exi−mi
l′
iV[i,:]
=o′
i−1l′
i−1emi−1−mi
l′
i+exi−mi
l′
iV[i,:]
(44)
Calculating Attention requires only one round of iterations
with the above formula, so we can chunk the calculation to
find the final result. FlashAttention-2 [261] improves on this
by improving the formula 44 as follows:
o′
i=o′
i−1l′
i−1emi−1−mi+exi−miV[i,:] (45)
Compared to the original o′
i, we only need to divide l′
N
by one more l′
Nin the final computation o′
Nto get the
correct result, thus avoiding the intermediate multistep
scaling division operation. It also reduces the memory write
overhead. Specifically, in FlashAttention, it is fixed Kj, Vj
enumeration of Qi, Oi, l′
i, mifor computation; in this way,
for each computed Oiwe need to write it back to memory,
which requires O(N2d2M−1)write complexity, where M
denotes the size of the shared memory. In FlashAttention-
2, we fixed Qi, Oi, l′
i, mito enumerate Kj, Vj, so that the
final result of Oican be computed at once and then written
back to memory, and the complexity of writing is reduced to
O(Nd). In addition, it also parallelizes the dimension of se-
quence length; when the batch size and the number of heads
are small, it increases the parallelism on the sequence length
to improve the GPU occupancy, significantly improving the
computation speed.
In general, efficient attention optimization methods
mainly include sparse attention, linearized attention, and
FlashAttention. However, there is often a gap between the
practical and theoretical effects of many efficient attention
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 30
methods, for example, many sparse attention methods are
difficult to achieve the theoretical effects in practice due
to the discontinuous memory accesses, which is mostly
because we do not take into account the characteristics of
the existing hardware when improving the methods.
6.2 Neural Architecture Search.
Although there have been significant advances in compres-
sion and acceleration methods for LLMs, many current hy-
perparameters that determine the final shape of the model
still need to be determined by hand design. This hand
design approach often requires a great deal of specialized
knowledge and experience on the part of the designer, and
it also has the problems of requiring long training time and
high cost. In this dilemma, one promising solution is Neural
Architecture Search (NAS) [262], [263], [264], [265], [266],
[267], [268], [269], [270]. For simplicity’s sake, next, we will
present a representative work from one of them.
The high computational cost of the Transformer model
makes it difficult to deploy on some hardware devices and
to realize the low-latency of inference on hardware devices
with limited resources, HAT [262] has emerged. The idea
of HAT is to search for the best-performing model structure
parameter that satisfies the requirement (given the hardware
conditions and resources) for a given latency requirement.
However, searching out the model structure and training
and evaluating it from scratch is costly and slow. It avoids
expensive retraining by constructing a Super Transformer
such that it approximately contains all Sub Transformer
models in the search space by sharing weights. Meanwhile,
HAT trains a delay predictor to predict the delay through
an offline method, which further speeds up the search. In
addition, it observes several important properties:
•First, focusing on multiple encoding layers is benefi-
cial for decoding layers.
•Second, different hardware has different preferences
for the model, with GPUs preferring shallow and
wide Transformers and ARM CPUs preferring nar-
row and deep Transformers.
Overall, HAT provides an efficient NAS scheme for trans-
former models under different hardware conditions and
latency requirements. At the same time, it can be well com-
bined with other compression acceleration methods because
it finds suitable model structure parameters for a given
condition without changing the model’s architecture.
7 D YNAMIC NETWORKS
Scaling up the size of language models has been proven to
be an effective approach for enhancing their performance
on NLP tasks [271], [272]. However, the substantial compu-
tation costs and memory demands associated with scaling
present a major challenge in the advancement of LLMs.
To address these issues while still harnessing the benefits
of scaling, dynamic neural networks (DyNNs) engage only a
subset of the network for processing each input, making the
entire model more flexible and efficient in meeting computa-
tional demands under resource-constrained environments.
In the field of NLP and the domain of LLMs, current
research on DyNNs primarily encompassess the followingthree methodologies: early exit, cascade inference and mixture
of experts (MoE) .
Early exit is designed to dynamically terminate the
inference process at the early layers of deep neural networks
(DNNs), thereby reducing computational costs and improv-
ing response time [273]. The intuition is that the predictions
for less complex words can often be accurately accom-
plished in earlier layers of the network [274]. These methods
typically integrate a series of internal classifiers within the
network, which provide signals for early exiting during
inference. Various exit criterions have been proposed [275],
[276], [277], [278], [279], [280], [281], [282]. This line of work
mainly focuses on and is applied to small or medium-size
language models, such as Bert. And the accuracy may not
be sufficient enough to support the application of general
LLMs in more complex and realistic scenarios.
Casacade inference utilizes a series of language models
of varying sizes to process requests with different levels of
complexities. Tabi [283] proposes an inference system with
multi-level inference models and a probability-based dis-
patcher to determine the handling strategy for input queries
and balance both accuracy and efficiency. FrugalGPT [284]
learns to adaptively triage quries from diverse datasets and
tasks and direct them to an appropriate combination of LLM
APIs. Both EcoAssistant [285] and [286] employ a query
cache to reference historical data for faster responses and a
hierarchy of LLMs to handle those mismatched new queries.
Mixture-of-Thoughts [287] considers the consistency of an-
swers from weaker LLMs as an indicator of the question
difficulty to decide whether to leverage stronger LLMs.
Generally, this line of works has emerged recently and
demonstrates a promising direction for the development of
more efficient LLM systems.
Compared to the two types of methods above, the study
ofMoE has an extensive history spanning multiple machine
learning fields including NLP . MoE horizontally extends a
feed-forward network (FFN) with multiple sub-networks,
of which only one or few will be activated during a single
forward pass. It is widely incorporated into the architectures
of today’s LLMs [288], [289] to provide both efficient and
powerful services. So in the remainder of this section, we
will delve into the realm of MoE. Section 7.1 begins with an
introduction to the basic concepts of MoE, followed by an
extensive survey of contemporary research on incorporating
MoE into LLMs, which includes algorithmic and architec-
tural design, training strategies and pratical applications.
Section 7.2 offers a concise review of some representative
studies on integration of MoE with previously dicussed
model compression and acceleration techniques, highlight-
ing its potential in the development of more comprehensive
and cost-efficient LLM systems.
7.1 Mixture of Experts
The earliest concept of MoE dates back to three decades
ago [298], [299] but firstly demonstrates its effectiveness
in massively improving model capacity without paying
a proportional computation overhead using sparse gating
[292]. In sparse MoE models, a subset of model parameters
are partitioned into a set of Nexpert networks {Ei(·)}N
i=1,
each operates independently on the input with unshared
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 31
TABLE 5: A summary of various Mixture-of-Experts (MoE) methods. For models of the largest size, we present the total
number of paremeters along with the number of experts per MoE layer. For methods utilizing shared experts [290], [291],
we include both (the number of experts used for sharing + the number of experts used for routing).
Methods Base Model Sparsity Largest Model Size
(Params / Num. experts)Load Balance
Sparsely-Gated [292] LSTM top-k 137B / 131072 Noisy top-k gating and auxiliary loss term.
GShard [293] NMT top-2 600B / 2048 Local group dispatching and auxiliary loss term.
Switch [294] T5 top-1 1571B / 2048 Auxiliary loss term.
Expert Choice [295] Transformer Expert Choice 143B / 64 Expert choice routing.
DeepSpeed-MoE [290] GPT-3 Residual-MoE 52B / (1+127) Multi-expert and multi-data parallelism.
M6-T [296] M6 ktop-1 10003B / 960 -
Brainformer [297] Non-uniform Expert Choice 158B / 64 Expert choice routing.
Mixtral 8x7B [289] Mistral 7B top-2 47B / 8 -
DeepSeekMoE [291] DeepSeek Shared Experts 145B / (4+128) Auxiliary loss terms.
Fig. 8: Illustration of a transformer block with an integrated
MoE layer.
weight. During training and inference, each input example
x(i.e., a token representation in language models) would be
routed to specific expert(s) via gating function G(·)whose
input is also xand the output is a sparse n-dimensional
vector. The final output yof MoE module is a weighted
combination which can be written as:
y=NX
i=1G(x)iEi(x) (46)
Based on the sparsity of G(x), we can skip the computation
ofEi(x)wherever G(x)i= 0 . Since only part of model
parameters are activated for each input and all experts have
the potential to be utilized by different samples, MoE model
theoretically enjoys a competitive learning ability with a
much faster inference speed comparing to its dense coun-
terpart. As Transformer [1] has become the standard choice
for language model, the most common way to introduce
MoE into Transformer is to replace the feed-forward layer
of certain block(s) with MoE layer, of which each expert
is itself a regular Transformer feed-forward network. An
example of MoE layer is given in Fig. 8. By increasing the
number of experts, the parameter size could grow from
hundreds of millions to hundreds of billions or even trillions
to match the size of LLMs. There are mainly three keyelements to characterize one MoE method: (1) Routing
method decides where and how each input will be routed
in the MoE layer, which is the most critical element of MoE
algorithms. (2) MoE model architecture discusses common
or specific design choices towards building scaled models
that are more performance-effective and parameter-efficient.
(3) Special training strategies sometimes are needed to
accommodate the uncertainty raised from learning-based
routing methods. We summarized some representative MoE
methods in TABLE 5.
7.1.1 Routing Method
For an MoE system, the most crucial factor affecting its per-
formance and the primary design concern is the ensurance
ofload balancing among the experts.
In a standard distributed training settings, experts from
the same layer are scattered across multiple devices. Each of
them is a simple feed-forward network (FFN) in most cases
and computes in parallel. And the maximum number of
tokens each expert can process during a single forward pass,
also known as the expert capacity , is limited by the memory
of device it resides. Generally, without specific algorithm or
architecture design, tokens from the same batch are assigned
unevenly among experts by the gating function due to its
sparsity. Therefore, if too many tokens are routed to the
same expert, surpassing its capacity, this will lead to an
overflow issue. The computation for the overflown part will
be skipped and those tokens will be passed directly to the
next layer via a residual connection. Thus unbalance loads
across experts lead to under-processing of tokens and a
waste of computation and memory for experts with empty
slots.
Besides performance decline, imbalanced loads across
experts could also lead to a self-reinforcing phenomenon that
inherently limits the capabilities of MoE system through
training. This phenomenon manifests as the gating net-
work converging to a state where it always produces large
weights for the same few experts, which rapidly trained
with large amount of data and are favored by the gating
network even more. Consequently, the remaining experts
remain undertrained and underutilized, which results in
the original MoE network collapses to a smaller network
comprising only the few active experts.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 32
Alongside the problem of load imbalance, there are
also methods dedicated to mitigate other adverse effects
stemming from the sparse nature of MoE, such as unstable
routing or the trade-off between sparsity and accuracy.
Based on the primary problem each method address, we
categorize the existing routing methods into the following
two groups and begin our review.
Towards load balancing. Most routing methods apply
an additional learnable gating network in each MoE layer.
A simple choice yet adopted by most works is to use one
linear layer with trainable matrix W∈Rd×N, where dis
model dimension and Nis the number of experts, then
followed by a non-linear function like softmax or sigmoid.
TheNcolumns of W{w1,···, wN}can also be seen as
embeddings of Nexperts respectively and readers may find
this expression in some works. For each token x∈Rd, the
routing score between xandi-th expert is given by the dot-
product similarity metric si=x·wi. To add sparsity to the
gating network, i.e., to use only kexperts ( k≪N) each MoE
layer, only experts with the highest top- kvalues of {si}N
i=1
(set of indices T) will be selected for token x. In general, if
expert iis activated, its gating value is given by
G(x)i=

exp(si)/P
j∈Texp(sj),softmax gating, k >1
exp(si)/PN
j=1exp(sj),softmax gating, k= 1
σ(si), sigmoid gating
(47)
where σ(·)is the sigmoid function. Note that for softmax
gating, top- 1methods formulate sightly differently to make
G(x)inon-trivial. The above idea is first proposed in [292]
and applied to LSTM models [300]. To mitigate the issue
of self-reinforcing, they add a tunable Guassian noise to
the routing scores and employ two additional loss terms
to encourage more balanced routing scores and selection
rates across the experts. GShard [293] integrates MoE into
transformers by replacing every other feed-forward layer
with an MoE layer using top-2 gating. They propose several
strategies to ensure load balancing, including (1) partion-
ing tokens into groups and limiting the number of tokens
that each experts can receive from a single group; (2) an
auxiliary loss term which has been widely adopted by later
works [294], [301], [302] and (3) a random routing strategy.
To further align the computational cost with that of a vanilla
transformer, Switch Transformer [294] routes each token to
only one expert with top-1 gating.
The aforementioned studies primarily suggest interme-
diary strategies, such as auxiliary loss terms, to promote
load balancing during training. Other researches aim at
improving the gating function to directly regularize or
even guarantee perfectly balanced loads across experts.
BASE Layer [303] formulates token-to-expert allocation as
a linear assignment problem, which maximizes the scores
between expert and their assigned tokens while subject to
a constraint that each expert must receive an equal amount
of tokens. Expert choice [295] is another routing strategy
guaranteeing perfect load balancing with no additional
regularization required. Instead of letting tokens select the
top-kexperts, each expert chooses top- ktokens which also
based on routing scores. Consequently, each expert can have
exactly same workloads and each token can be processed
by a variable number of experts. Clark et al. [304] propose arouting algorithm using reinforcement learning, in which
each router is seen as policy with actions and rewards
defined by the selection of experts and the predicted proba-
bility of the correct output token respectively. In addition to
employing learnable gating networks, some studies imple-
ment non-learnable strategies to regulate the load distribu-
tion across experts. Hash layer [305] employs a parameter-
free hash function to replace dynamic gating with a pre-
defined fixed mapping from tokens to specific experts,
which consequently eliminates load imbalance. MoWE [306]
routes each word to one specific expert based on auxiliary
vocabulary mapping and ensures that the words assigned
to each expert are of approximately the same frequency
in the pretraining data. Inspired by Hash layer, PanGu-
Σ[307], [308] deploys a two-level routing that first maps
each token to a group of candidate experts by domain and
then uses random hash to choose a particular expert from
that group for processing. THOR [309] also simplifies the
gating process with a parameter-free approach by randomly
selecting a pair of experts from each layer during a training
iteration and dispatching the whole batch of tokens to those
experts.
The methods we have described so far can only be
utilized to the fullest extent with sufficient device resources
(i.e., no overflow issues are encountered). In more realistic
senarios, one must take additional strategy to handle over-
flow. The simplest solution is to stop assigning tokens to
experts with full workloads. However, in this way only
either the prefix of the input sentences or the sentences
with small indices in the batch dimension will be processed,
based on whether the batch is flattened along the sequence
length or the batch dimension, which leads to a biased
selection and underutilization of training or inference data.
To address this issue, Z-code [310] and BASE [303] pro-
pose to shuffle the input tokens in the flattened batch to
disentangle the probability of a token being selected from
its position. In the domain of vision transformer, V-MoE
[311] introduces Batch Prioritized Routing algorithm (BPR),
which additionally compute a priority score for each token
(e.g., the maximum of routing scores between the token and
all experts) and sort tokens accordingly before allocation.
Therefore only insignificant tokens will be discarded. ST-
MoE [312] finds that BPR benefits language models as
well, especially in low-resource regimes where the expert
capacity is even less than the average number of tokens
each expert receives. Note that BPR can only be applied to
encoder side of encoder-decoder model since the inputs of
encoder are not autoregressive thus are allowed to see each
other, otherwise model could cheat by using future token
information.
Effective routing with sparsity. Although sparsity
bounds the computational cost in a large-scale MoE system,
it generally limits the network’s capability and impedes
convergence due to unstable routing dynamics. DeepSpeed-
MoE [290] observes that increasing the number of experts
each token goes through helps accuracy. In order to leverage
this property while keeping the computation costs as top-
1 gating, they propose Residual-MoE by fixing one expert
and varying the second expert for each token to achieve the
benefit of using 2 expert. DeepSeek-MoE [291] employs the
same methodology, utilizing a fixed set of shared experts
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 33
to capture and consolidate common knowledge, augmented
by a distinct set of routed experts dedicated to knowledge
specialization. M6-T [296] also notices the advantage of top-
kgating over the top- 1. They propose to split expert into k
groups and perform ktop-1routing in parallel to match
the efficiency of top- 1gating. To ensure each expert can
receive rich and diverse tokens under the sparse settings,
MoEC [302] encourages experts to form clustered structure
by closing the routing probability among neighbor experts
with designed loss term and randomly drops some experts
in each cluster before the global top-1 gating.
Besides sparsity, DSelect-k [313] notices the discontinuity
in top-k gating methods and suggest this could lead to
convergence and performance issues when training with
gradient-based methods. They propose a continuously dif-
ferentiable and sparse gating function, which densely se-
lects experts at beginning of training but fast converges to
sparse expert selection by adding an regularization term. X-
MoE [301] points out that current routing mechanisms tend
to push token representations towards expert embeddings
which potentially harms the representation capacity. They
propose to calculate the routing scores between tokens and
experts in a low-dimensional space with additional nor-
malization and learnable gating temperature. ST-MoE [312]
conducted an extensive study on the training stability and
fine-tuning quality of sparse MoE models. They framed
the training process of sparse models as a quality-stability
trade-offs: various stability techniques, such as dropout and
gradient clipping, could enhance the training stability but
often come at the expense of model quality. Therefore they
propose router z-loss to address both the instability issue
and the problem of quality degradation.
7.1.2 MoE model architecture
In this part we discuss how to arrange MoE layers into a
Transformer-based model, such as the frequency of expert
layers and the number of experts, which could significantly
affect the scale of models and the overall performances.
In transformer, sparse model usually starts with a dense
model and scales up by substituting or inserting MoE lay-
ers at a fixed interval or heuristically. A common design
deployed in most large sparse expert models [293], [294],
[295], [314], [315] is to replace the feed-forward component
of every other Transformer block with a MoE layer (i.e., at a
frequency of 0.5). Other frequencies are also adopted, such
as 0.25 (i.e., substituting every fourth FFN layer) in [312] and
1.0 (i.e., placing in every layer). In general, experiments [304]
suggest a frequency at 0.5-1.0 and lower frequency under-
mines the performance. However, there are also works [301],
[302], [303], [305], [316] introducing a fixed number of expert
layers to baseline models by spreading MoE layers unevenly
across the network. For instance, BASE [303] inserts a large
MoE layer consisting of stacked FFNs only after middle
layer.
As for the number of experts per-layer, although using
more experts continuously brings quality gains in most
cases, diminishing returns in the improvements are also
reported in earlier works [293], [294]. Further analyses [304],
[312] also points out the drastically-diminishing incremental
benefits from routing as the base model size increases. Afixed number of experts per-layer in {64,128}is recom-
mended by [304] and also have been practiced in a lot of
sparse large language models [312], [314], [317]. Moreover,
DeepSpeed-MoE [290] questions the standard MoE architec-
ture putting the same number of experts in all MoE layers.
Their experiments suggest that a large number of experts in
deeper layers boost the performance more effectively. There-
fore they introduce a pyramid structure by utilizing more
experts only in the last two layers and achieve comparable
results as standard MoE models but with fewer parameters.
Above works are all built upon uniform transformer
blocks and by interleaving dense and sparse layers, Brain-
former [297] explores a non-uniform architecture with
sparse layer inspired by the success of EfficientNet [318]
and sandwich transformer [319]. An evolutionary search
algorithm is applied to explore the best Brainformer block
architecture in the search space consisting of different layer
types (namely self attention, MoE and dense FFN sub-
layers), interleaving orders and hyperparameter settings
such as model dimension and number of attention heads.
The whole network is constructed by stacking a variable
number of blocks according to different scales. Experiment
results demonstrate a clear advantage in terms of both
efficiency and capacity over its GLaM [314] counterpart and
Primer [320] dense model produced by NAS [321].
7.1.3 Training Strategies
Existing learning-based routing methods usually train both
the gating and expert networks jointly from scratch. As
the parameters of MoE layers are randomly initialized, the
routing behavior at the beginning stage of training can be
seen as random routing and the correspondences between
tokens and experts are highly unstable . As a result, MoE
models take a longer time to converge with a potential risk
of reinforcing improper routing behavior which eventually
limits the model quality.
To handle this problem, a two-stage training strategy is
introduced in [316], [322] to separate the training of the
gating network and expert networks. In the first stage,
StableMoE [316] learns a balanced and cohesive routing
strategy following a standard MoE training process with ad-
ditional balance loss. Throughout the second stage, the rout-
ing strategy is freezed to provide a stable token-to-expert
assignment for the training of the rest of model. Experiments
confirm that the consistency of routing strategy boost both
the convergence speed and final performance. Conversely,
EvoMoE [322] starts from training then diversifying from
one common expert at stage one before learning the gating
network and sparsifying the network in the second stage.
In this way experts are able to get sufficient training in the
early stage and more suitable routing strategy can be built
on the exploitation of specialized experts.
Another line of works set out to alleviate the overfitting
problem raised from the imbalance between vast number
of parameters and limited training examples via special
dropout [323] mechanism at MoE layer. For instance, Switch
Transformer [294] increases the dropout rate solely inside
the experts, named as expert dropout , to aid the performance
on downstream tasks with very few training data. Gating
Dropout [324] further pushes traditional dropout to another
level to reduce the communication cost for dispatching
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 34
tokens across devices and also improve the performance
during training. Specifically, they permit tokens to ignore
the assignment from gating network with certain probability
and instead route them to the experts on the same device.
This also encourages experts to function more robustly and
learn a generalization ability. The results demonstrate that
Gating Dropout indeed accelerate the convergence of sparse
MoE models in terms of wallclock time and enhance the
model quality.
7.1.4 MoE Applications
The success of MoE models promotes a series of works
deploying sparse MoE algorithms in actual LLM applica-
tions or combining with other model compression and ac-
celeration techniques in pursuit of greater efficiency. CPM-2
[325] integrates BASE Layer [303] into their largest Chinese-
English bilingual models with 198 billion parameters. Fol-
lowing GShard [293], GLaM [314] trains a family of decoder-
only sparse MoE models, the largest of which has 1.2T pa-
rameters and yields better zero, one and few-shot learning
abilities in comparison with its dense GPT-3 [9] counter-
parts. Seeking to close the performance gap between high
and low-resource languages and break the 200 language
barrier, a Sparsely-Gated MoE model with 54.5B parameters
is developed by [317], following the optimaztion process
in [293], and casts light on a promising approach towards
a universal translation system. A revealing article on the
technical details of GPT-4 [288] confirms the deployment
of a MoE model consisting of 16 experts inside GPT-4.
Each expert is tuned to specialize in a specific domain or
task, thereby endowing GPT-4 with the multi-task ability.
Mixtral [289] builds upon Mistral 7B and replaces all FFN
sub-blocks by MoE layers, each consisting of 8 experts and
a top-2 gating network. The resulted Mixtral 8x7B only
uses 13B active parameters during inference but surpasses
Llama 2 70B [18] and GPT-3.5 on several benchmarks.
DeepSeekMoE [291] proposes a series of MoE models with
sizes of 2B, 16B, and 145B as well as aligned versions, to
demonstrate the adaptability and versatility of their MoE
architectures. OpenMoE [326] also releases a suite of open-
sourced MoE models, building upon the architectures of ST-
MoE [312] and Residual-MoE [290]. In addition, they offer a
comprehensive study and a few insights on MoE’s routing
behavior throughout training.
7.2 Combineing MoE with other efficient techniques
The Mixture-of-Experts approach inspires the field as an
alternative pathway for buidling more powerful and effi-
cient LLMs. Given that MoE is akin to an art of architecture
design and is orthogonal to most model compression and
acceleration techniques, there are also works exploring ways
to merge its inherent sparsity with other optimization strate-
gies, such as pruning, distillation, and PEFT. In this section,
we will examine the most representative studies in this area
and highlight the potential it holds for future research.
7.2.1 Model Compression
In the realm of sparse MoE models, most existing works can
be viewed as trading memory consumption for model qual-
ity. To reduce the memory footprint while retaining mostof their capabilities, researchers have explored several ways
to introduce traditional model compression techniques into
MoE models.
Switch Transformer [294] made the first attempt to distill
large sparse models into small dense models. Their findings
indicate that it is possible to preserve approximately 30% of
the quality gains achieved through scaling when distilling
to a FLOP-matched dense variant for both pre-training and
fine-tuning tasks. DeepSpeed-MoE [290] studies the poten-
tial of distilling a large teacher MoE model into a smaller
student MoE model with shallower expert networks. Ad-
ditionally, they suggest a stage-KD training strategy ( i.e.,
halting distillation at specific steps) to mitigate the under-
fitting issue stemming from the student model’s limited
expert capacity.
As another prominent tool for parameter reduction, the
pruning of MoE models aims to remove redundant or less
influential components, usually a subset of less important
expert networks, with minimal impact on the performance.
The hypothesis behind pruning, as suggested by Z-code
[310], is that different experts can specialize in distinct as-
pects of the task during training, making a subset of experts
competent enough for a given task to a certain extent. Z-
code tried two methods for expert selection: random selec-
tion and selection based on utilization rates in the validation
set. Chen etc[327] observe the long-tailed distribution of
expert contributions in downstream tasks. Different from Z-
code’s approach, they propose to progressively prune away
most experts throughout the fine-tuning process, leaving
only the most professional one for the target downstream
task. The experiment results highlight the effectiveness of
their pruning strategy, preserving 99.3% of the benefits from
MoE while enjoying the same resource consumption as
vanilla dense models during inference. As an alternative
approach, MPOE [328] introduce a parameter-efficient MoE
architecture by decomposing the parameter matrix of each
expert into central and auxiliary tensors. The central tensor is
believed to encode the majority of the information present
in the original matrix, which is likely to be similar across
experts and thus suitable for sharing among them. On the
other hand, the auxiliary tensors capture the individual
characteristics and serve as a complement to the central
tensor. This parameter-sharing method has been shown to
be effective, achieving a 27.2x reduction in total parameters
while yielding better performance comparing to the Switch
Transformer.
Witnessing the great success of MoE models, there are
also efforts to introduce sparsity into a standard transformer
model with the purpose of reducing the number of param-
eters involved in computation while retaining the repre-
sentation power. MoEBERT [329] adapts the feed-forward
networks in a pre-trained BERT [3] into multiple experts
and activates only one expert during inference to increase
the speed. To preserve the original representation power,
they share the most important neurons in the FFNs among
the experts based on the importance score [330] when ini-
tializing MoEBERT. The training of MoEBERT incorporates
layer-wise distillation, leading to a resulting model that out-
performs other task-specific distilling methods. MoEfication
[331] aims to generalize the conversion from FFNs to MoE
layers for various Transformer models. The idea is driven
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 35
by the insight that only a tiny fraction of neurons of FFNs
will respond to most inputs. To split the feed-forward layer
into experts, neurons that often activates simultaneously are
grouped into the same expert network. And the routing
strategy is learned by approximating the calculation of the
original model. To further reduce the computational and
memory demards of standard transformers, σ-MoE [332]
and SwitchHead [333] introduce additional sparsity to the
FFN and attention components, drawing on the principles
of the MoE methodology.
7.2.2 Efficient Finetuning
In search of more efficient and powerful model architec-
tures, researchers are also exploring the combination of MoE
methods and other cost-effective techniques such as Mixer
[334] and PEFT methods. These collaborative approaches
primarily leverage the expressiveness provided by MoE
while aggressively reducing the training and computation
cost. Sparse Mixers [335] replaces most of the self-attention
sublayers with mixing and FFNs with MoE sublayers. SMLP
[336] goes one step further by substituting the self-attention
modules with linear transformations, which also employs
a MoE mechanism with routing in the feature dimension
to ensure tokens from the same sentences are delivered
to the same expert. AdaMix [337] proposes a mixture of
adapters [338] or a mixture of low-rank decomposition
matrices [339] with stochastic routing mechanism [309] as
a novel fune-tuning technique to enhance the downstream
performance. The result illustrates that AdaMix surpasses
SOTA parameter-efficient fine-tuning and even full model
fine-tuning algorithms on both NLU and NLG tasks. Based
on a similar idea, MixDA [340] also utilizes a set of domain
adapters to inject domain-specific knowledge in parallel and
then train a mixture-of-adapters gate to dynamically fuse
the knowledge from different domain adapters. This plug-
in approach showcases its scalibility and efficiency on sev-
eral domain tasks. The same methodology is also adopted
by [341], [342], [343], [344] to achieve efficient finetuning on
domain-specific or instruction datasets and to mitigate the
catastrophic forgetting arising from continual learning.
8 A CCELERATION FRAMEWORK
With the rapid development of Transformer-based models,
various models have emerged. Because of different applica-
tion scenarios, they have additional requirements in terms
of latency, throughput, memory, etc, making it difficult for
us to deploy the models. In this section, we introduce some
recently developed inference acceleration frameworks for
LLM, which effectively improve the efficiency of the models
in different scenarios, as shown in TABLE 6. We classify
the general framework and specialized framework based on
the generalization. Here are some more acceleration frame-
works [351], [352], [353], [354], [355], [356], [357] specific to
training, and since this paper focuses on inference, we will
not discuss them specifically. If you want to deploy trained
models to get efficient inference quickly, you can refer to
these frameworks [358], [359], [360], [361], [362], [363].8.1 General Framework
In this section, we will introduce some relatively general-
ized frameworks [345], [346] proposed recently. No matter
what kind of scenarios the models are deployed in, we can
consider using them or combining their ideas to accelerate
the inference and thus obtain higher efficiency. Since most
big models are still deployed and run on GPUs, our gener-
alization here refers to generalization under GPU hardware.
Operator fusion is a common method to accelerate
model inference by eliminating unnecessary intermediate
results, lowering memory requirements, and reducing un-
necessary memory IO and kernel startup overhead, thus
improving the utilization of computational resources such
as GPUs, CPUs, and registers. Meanwhile, operator fusion
is an essential optimization for many state-of-the-art DNN
compilation frameworks, such as TensorFlow XLA [364],
TVM [365], MNN [366], PyTorch JIT [367], and so on.
However, these frameworks have stringent requirements
for operator fusion, e.g., TVM uses relatively fixed sched-
ule templates, resulting in missing many potential fusion
opportunities. DNNFusion [345] has better coverage and
fusion identification capabilities through algebraic simpli-
fication and rational classification of operators. In addition,
it further improves the efficiency of operator fusion by elim-
inating unnecessary operators through heuristic methods.
Recently, Microsoft proposed DeepSpeed Inference [346],
an efficient integrated inference system for the increasingly
diverse Transformer model, which reduces latency by a
factor of 7.3 in state-of-the-art latency-oriented scenarios
and increases throughput by more than 1.5 in throughput-
oriented scenarios. It includes the following two compo-
nents:
•Multi-GPU Inference Solution: It minimizes latency
while maximizing throughput in dense and sparse
Transformer models with GPU memory aggregation.
•Heterogeneous Inference Solution: Besides GPU
memory and computation, it utilizes CPU and NVMe
memory to achieve high inference throughput for
large models that do not lend themselves to aggre-
gated GPU memory.
Many strategies have been used to maximize the training
throughput, such as tensor parallelism, pipeline parallelism,
ZeRo, expert parallelism, and so on. However, inference
with small batch size suffers from the following problems
due to insufficient parallelism:
•A smaller amount of data is processed at a time,
which results in the need to read model weights from
the HBM and call the kernel frequently, incurring a
significant overhead.
•Each kernel call writes data to global memory, which
the GPU has to re-read on the next kernel call, adding
additional communication overhead.
•The current cuBLAS and CUTLASS GeMM libraries
are not optimized for small batch sizes and have low
memory bandwidth utilization.
On the other hand, regular operator fusion can only
be done for element-wise operators. In contrast, operators
in the Transformer structure introduce data dependencies
across thread blocks, making it challenging to do operator
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 36
TABLE 6: A summary of various acceleration frameworks.
Framework/Passage Generalization Method
DNNFusion [345] General Operator fusion
DeepSpeed Inference [346] General Operator fusion, tensor parallelism, inference pipeline
TurboTransformer [347] Specialized Operator fusion, scheduling optimization
ByteTransformer [348] Specialized Operator fusion, scheduling optimization
FlexGen [349] Specialized Offloading system
Power-Infer [350] Specialized Offloading system
NormQuery
Key
ValueQ_bias
K_bias
V_bias+
+
+ V-transK-transQ-transAttn
ScoreSoftmax
Attn
Context
TransformAttn
OutputAll
Reduce
+ +Attn_bias
Norm
Intermediate
FFGlue +All
ReduceOutput
FF+ +
Feed-Forward I_biasY
O_bias3x speedup1.2x speedup1.5x speedup2.9x speedupQKVAttention
MLP
Bias_add
Fig. 9: Deep-Fusion strategy for the small-batch inference.
fusion. This is because if another consumes data generated
by one thread block on GPUs, a global memory synchroniza-
tion is needed to invoke a new kernel. To avoid the need for
global synchronization, Deep-Fusion tiles the computational
space along the dimensions of the iteration space so that no
cross-block data dependencies arise. In addition, it custom-
designed GeMM for small batch sizes and implemented
it to be fusible with Deep-Fusion for maximum memory
bandwidth utilization. Deep-Fusion does four operator fu-
sions in a Transformer layer for small batch sizes to obtain
four customized kernels, as shown in the red dashed box in
the Fig. 9 below. For large batch sizes, the operator fusion
strategy is the same, with the difference that the GeMM in
cuBLAS is used directly instead of the custom GeMM.
8.2 Specialized Framework
In this section, we will introduce some specialized frame-
works that have been proposed recently. They are cus-
tomized for specific scenarios and needs and can be tailored
to meet different requirements. If you deploy models indi-
vidually in high demand in certain aspects, consider using
them.
Compared to other scenarios, efficiently deploying the
Transformer model to servers needs to meet the service’s
low latency and high throughput requirements, which
presents a significant challenge. In addition, due to the
unpredictability of requests and the fact that NLP tasks
employ variable-length sentences, the variability of input di-
mensions poses a severe problem for effective memory man-
agement and service optimization. TurboTransformer [347]
proposes a sequence-length-aware memory allocation algo-
rithm and a batch scheduling algorithm that aims to max-
imize the response throughput by treating it as a dynamic
programming problem. Efficient memory reuse of variabledimensional intermediate tensor and optimal batch schedul-
ing scheme are realized. TurboTransformer also proposes a
parallel approximation algorithm for high-frequency opera-
tors such as Softmax and LayerNorm, significantly improv-
ing efficiency. However, TurboTransformer’s active group-
ing approach still introduces non-eliminable padding over-
heads. Based on this, ByteTransformer [348] proposes a
padding-free algorithm to free the whole Transformer from
the redundant computation of zero-padded tokens. In ad-
dition, ByteTransformer optimizes multi-head attention for
the zero-filling algorithm so that the attention is no longer
faced with redundant computation of useless tokens, further
improving performance.
Unlike the previous work, FlexGen [349] sacrifices the la-
tency of the inference computing service almost completely
to polarize the design of an LLM computing system that
focuses only on throughput. Thus, it is only suitable for
offline computing. Every aspect of the LLM accelerator can
be reconsidered when pursuing only throughput, including
storage management, latency-hiding design of memory ac-
cesses in cross-domain memory hierarchies, and paralleliza-
tion strategies. FlexGen proposes a new offloading-based
inference system based on a zig-zag parallelization strategy,
which achieves more than 40 times higher throughput than
DeepSpeed Zero-Inference. We believe the most inspiring
aspect of this work is that it highlights the importance of
divergent thinking and emphasizes the need to dig deeper
into the details of the problem and explore alternative
solutions.
Recently, the open-source inference framework Power-
Infer [350] recently made LLM Inference 11 times faster.
Without quantization and with FP16 precision, it allows 40B
models to run smoothly on an RTX4090 PC; if quantization
is added, a 2080 Ti can also run 70B models smoothly. It is
based on highly localized sparse activation based on LLM,
i.e., a small fraction of neurons hot neurons are activated
all the time on input, while the majority of neurons cold
neurons respond according to a specific input. PowerInfer
exploits this feature and the fact that CPUs are good at con-
ditional computation and GPUs are good at simple parallel
computation to develop an innovative GPU-CPU hybrid
inference engine. This means hot neurons are preloaded
into the GPU for fast access. In contrast, cold neurons are
computed on the CPU, dramatically reducing the memory
requirements of the GPU and the amount of data transferred
between the CPU and the GPU. In addition, PowerInfer in-
corporates an adaptive predictor and neuron-specific sparse
optimizations to improve the sparsity efficiency of neuron
activation and computation. Overall, PowerInfer enables PC
users to run advanced LLM locally without needing expen-
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 37
sive specialized hardware. This facilitates the popularization
of AI applications and provides unprecedented opportuni-
ties for hobbyists, researchers, and small businesses.
There are already more accelerators built for large model
multi-GPU distributed inference, but relatively few acceler-
ators are built for edge device deployment. As the demand
for deploying AI large models on edge devices grows, this
will become a pressing problem.
9 C ONCLUSIONS
In this paper, we conducted a comprehensive investigation
of compression and efficient inference for large language
models from an algorithmic perspective, including quan-
tization, pruning, distillation, compact architecture design,
dynamic networks. Additionally, we introduced some pop-
ular compression and acceleration frameworks tailored for
large language models. However, as we mentioned in the
introduction, compression and acceleration of large models
face more challenges compared to smaller models. While
existing algorithms have made significant efforts to address
these challenges, many algorithms still rely on frameworks
designed for compressing small models, and challenges in
compressing large models persist. In the future, further
exploration is needed to develop more efficient and effective
compression algorithms while ensuring the versatility and
generalization of large models.
CONTRIBUTORS
Wenxiao Wang is responsible for this paper’s overall struc-
ture, content arrangement, the writing of Section 1 and
Section 2, and refinement of each section in this paper.
Wei Chen, Yongliu Long, Zhengkai Lin and Liye Zhang
are responsible for the surveys and writing of quantization
(Section 3), pruning (Section 4), dynamic networks (Sec-
tion 7), and knowledge distillation (Section 5), respectively.
Yicong Luo is responsible for surveys and writing compact
architecture design and acceleration framework (Section 6
and Section 8). All co-first authors are listed in alphabetical
order of their surnames. Binbin Lin, Deng Cai, and Xiaofei
He participate in the comprehensive discussion and provide
many great insights.
REFERENCES
[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
Advances in neural information processing systems , vol. 30, 2017.
[2] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P . J. Liu, “Exploring the limits of transfer
learning with a unified text-to-text transformer,” The Journal of
Machine Learning Research , vol. 21, no. 1, pp. 5485–5551, 2020.
[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language under-
standing,” arXiv preprint arXiv:1810.04805 , 2018.
[4] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al. ,
“Improving language understanding by generative pre-training,”
2018.
[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever
et al. , “Language models are unsupervised multitask learners,”
OpenAI blog , vol. 1, no. 8, p. 9, 2019.[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright,
P . Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman,
J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P . Welinder,
P . F. Christiano, J. Leike, and R. Lowe, “Training language models
to follow instructions with human feedback,” in Advances in
Neural Information Processing Systems 35: Annual Conference on
Neural Information Processing Systems 2022, NeurIPS 2022, New
Orleans, LA, USA, November 28 - December 9, 2022 , S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds.,
2022.
[7] T. Lin, Y. Wang, X. Liu, and X. Qiu, “A survey of transformers,”
CoRR , vol. abs/2106.04554, 2021.
[8] S. Islam, H. Elmekki, A. Elsebai, J. Bentahar, N. Drawel, G. Rjoub,
and W. Pedrycz, “A comprehensive survey on applications of
transformers for deep learning tasks,” CoRR , vol. abs/2306.07303,
2023.
[9] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P . Dhari-
wal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell et al. ,
“Language models are few-shot learners,” Advances in neural
information processing systems , vol. 33, pp. 1877–1901, 2020.
[10] Y. Wang, H. Chen, Y. Tang, T. Guo, K. Han, Y. Nie, X. Wang,
H. Hu, Z. Bai, Y. Wang, F. Liu, Z. Liu, J. Guo, S. Zeng, Y. Zhang,
Q. Xu, Q. Liu, J. Yao, C. Xu, and D. Tao, “Pangu- π: Enhancing
language model architectures via nonlinearity compensation,”
CoRR , vol. abs/2312.17276, 2023.
[11] Z. Zhang, X. Han, H. Zhou, P . Ke, Y. Gu, D. Ye, Y. Qin, Y. Su,
H. Ji, J. Guan, F. Qi, X. Wang, Y. Zheng, G. Zeng, H. Cao, S. Chen,
D. Li, Z. Sun, Z. Liu, M. Huang, W. Han, J. Tang, J. Li, X. Zhu,
and M. Sun, “CPM: A large-scale generative chinese pre-trained
language model,” AI Open , vol. 2, pp. 93–99, 2021.
[12] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow,
R. Castagn ´e, A. S. Luccioni, F. Yvon, M. Gall ´e, J. Tow, A. M. Rush,
S. Biderman, A. Webson, P . S. Ammanamanchi, T. Wang, B. Sagot,
N. Muennighoff, A. V . del Moral, O. Ruwase, R. Bawden, S. Bek-
man, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier,
S. Tan, P . O. Suarez, V . Sanh, H. Laurenc ¸on, Y. Jernite, J. Launay,
M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji,
A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue,
C. Klamm, C. Leong, D. van Strien, D. I. Adelani, and et al.,
“BLOOM: A 176b-parameter open-access multilingual language
model,” CoRR , vol. abs/2211.05100, 2022.
[13] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,
C. Dewan, M. T. Diab, X. Li, X. V . Lin, T. Mihaylov, M. Ott,
S. Shleifer, K. Shuster, D. Simig, P . S. Koura, A. Sridhar, T. Wang,
and L. Zettlemoyer, “OPT: open pre-trained transformer lan-
guage models,” CoRR , vol. abs/2205.01068, 2022.
[14] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,
W. Zheng, X. Xia et al. , “Glm-130b: An open bilingual pre-trained
model,” arXiv preprint arXiv:2210.02414 , 2022.
[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,
A. Roberts, P . Barham, H. W. Chung, C. Sutton, S. Gehrmann,
P . Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P . Barnes,
Y. Tay, N. Shazeer, V . Prabhakaran, E. Reif, N. Du, B. Hutchinson,
R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P . Yin,
T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski,
X. Garcia, V . Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito,
D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan,
S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat,
A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou,
X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-
Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel, “Palm: Scaling
language modeling with pathways,” J. Mach. Learn. Res. , vol. 24,
pp. 240:1–240:113, 2023.
[16] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge,
Y. Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu,
G. Liu, C. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan,
J. Tu, P . Wang, S. Wang, W. Wang, S. Wu, B. Xu, J. Xu, A. Yang,
H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang,
X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, and
T. Zhu, “Qwen technical report,” CoRR , vol. abs/2309.16609,
2023.
[17] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, “ERNIE:
enhanced language representation with informative entities,” in
Proceedings of the 57th Conference of the Association for Compu-
tational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,
2019, Volume 1: Long Papers , A. Korhonen, D. R. Traum, and
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 38
L. M `arquez, Eds. Association for Computational Linguistics,
2019, pp. 1441–1451.
[18] H. Touvron, L. Martin, K. R. Stone, P . Albert, A. Almahairi,
Y. Babaei, N. Bashlykov, S. Batra, P . Bhargava, S. Bhosale,
D. M. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull,
D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao,
V . Goswami, N. Goyal, A. S. Hartshorn, S. Hosseini, R. Hou,
H. Inan, M. Kardas, V . Kerkez, M. Khabsa, I. M. Kloumann,
A. V . Korenev, P . S. Koura, M.-A. Lachaux, T. Lavril, J. Lee,
D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov,
P . Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein,
R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith,
R. Subramanian, X. Tan, B. Tang, R. Taylor, A. Williams,
J. X. Kuan, P . Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan,
M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov,
and T. Scialom, “Llama 2: Open foundation and fine-tuned chat
models,” ArXiv , vol. abs/2307.09288, 2023. [Online]. Available:
https://api.semanticscholar.org/CorpusID:259950998
[19] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud,
D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi,
T. Hashimoto, O. Vinyals, P . Liang, J. Dean, and W. Fedus,
“Emergent abilities of large language models,” Trans. Mach. Learn.
Res., vol. 2022, 2022.
[20] G. Yang, D. Lo, R. Mullins, and Y. Zhao, “Dynamic stashing
quantization for efficient transformer training,” arXiv preprint
arXiv:2303.05295 , 2023.
[21] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He, “Zeroquant-v2: Exploring
post-training quantization in llms from comprehensive study to
low rank compensation,” 2023.
[22] Y. Bondarenko, M. Nagel, and T. Blankevoort, “Understanding
and overcoming the challenges of efficient transformer quantiza-
tion,” arXiv preprint arXiv:2109.12948 , 2021.
[23] O. Zafrir, G. Boudoukh, P . Izsak, and M. Wasserblat, “Q8bert:
Quantized 8bit bert,” in 2019 Fifth Workshop on Energy Effi-
cient Machine Learning and Cognitive Computing-NeurIPS Edition
(EMC2-NIPS) . IEEE, 2019, pp. 36–39.
[24] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard,
H. Adam, and D. Kalenichenko, “Quantization and training of
neural networks for efficient integer-arithmetic-only inference,”
inProceedings of the IEEE conference on computer vision and pattern
recognition , 2018, pp. 2704–2713.
[25] S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. W. Ma-
honey, and K. Keutzer, “Q-bert: Hessian based ultra low precision
quantization of bert,” in Proceedings of the AAAI Conference on
Artificial Intelligence , vol. 34, no. 05, 2020, pp. 8815–8821.
[26] T. Piao, I. Cho, and U. Kang, “Sensimix: Sensitivity-aware 8-
bit index & 1-bit value mixed precision quantization for bert
compression,” PloS one , vol. 17, no. 4, p. e0265621, 2022.
[27] W. Zhang, L. Hou, Y. Yin, L. Shang, X. Chen, X. Jiang, and Q. Liu,
“Ternarybert: Distillation-aware ultra-low bit bert,” in Proceedings
of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP) , 2020, pp. 509–521.
[28] H. Qin, Y. Ding, M. Zhang, Y. Qinghua, A. Liu, Q. Dang, Z. Liu,
and X. Liu, “Bibert: Accurate fully binarized bert,” in International
Conference on Learning Representations , 2021.
[29] C. Zhao, T. Hua, Y. Shen, Q. Lou, and H. Jin, “Automatic
mixed-precision quantization search of bert,” arXiv preprint
arXiv:2112.14938 , 2021.
[30] Z. Zhao, Y. Liu, L. Chen, Q. Liu, R. Ma, and K. Yu, “An
investigation on different underlying quantization schemes for
pre-trained language models,” in Natural Language Processing
and Chinese Computing: 9th CCF International Conference, NLPCC
2020, Zhengzhou, China, October 14–18, 2020, Proceedings, Part I 9 .
Springer, 2020, pp. 359–371.
[31] B. Wang, Y. Ren, L. Shang, X. Jiang, and Q. Liu, “Exploring ex-
treme parameter compression for pre-trained language models,”
inInternational Conference on Learning Representations , 2021.
[32] H. Bai, W. Zhang, L. Hou, L. Shang, J. Jin, X. Jiang, Q. Liu, M. Lyu,
and I. King, “Binarybert: Pushing the limit of bert quantization,”
inProceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long Papers) , 2021, pp.
4334–4348.
[33] A. H. Zadeh, I. Edo, O. M. Awad, and A. Moshovos, “Gobo:
Quantizing attention-based nlp models for low latency and en-
ergy efficient inference,” in 2020 53rd Annual IEEE/ACM Interna-tional Symposium on Microarchitecture (MICRO) . IEEE, 2020, pp.
811–824.
[34] S. Kim, A. Gholami, Z. Yao, M. W. Mahoney, and K. Keutzer, “I-
bert: Integer-only bert quantization,” in International conference on
machine learning . PMLR, 2021, pp. 5506–5518.
[35] S. Dai, R. Venkatesan, M. Ren, B. Zimmer, W. Dally, and
B. Khailany, “Vs-quant: Per-vector scaled quantization for ac-
curate low-precision neural network inference,” Proceedings of
Machine Learning and Systems , vol. 3, pp. 873–884, 2021.
[36] T. Li, Y. E. Mesbahi, I. Kobyzev, A. Rashid, A. Mahmud, N. An-
churi, H. Hajimolahoseini, Y. Liu, and M. Rezagholizadeh, “A
short study on compressing decoder-based language models,”
arXiv preprint arXiv:2110.08460 , 2021.
[37] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P . Luo,
and N. Wong, “Compression of generative pre-trained language
models via quantization,” in Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics (Volume 1: Long
Papers) , 2022, pp. 4821–4836.
[38] Z. Li, Z. Wang, M. Tan, R. Nallapati, P . Bhatia, A. Arnold,
B. Xiang, and D. Roth, “Dq-bart: Efficient sequence-to-sequence
model via joint distillation and quantization,” in Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers) , 2022, pp. 203–211.
[39] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and
K. Keutzer, “A survey of quantization methods for efficient neu-
ral network inference,” in Low-Power Computer Vision . Chapman
and Hall/CRC, 2022, pp. 291–326.
[40] C. Xu and J. McAuley, “A survey on model compression and
acceleration for pretrained language models,” in Proceedings of
the AAAI Conference on Artificial Intelligence , vol. 37, no. 9, 2023,
pp. 10 566–10 575.
[41] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, “Awq:
Activation-aware weight quantization for llm compression and
acceleration,” arXiv preprint arXiv:2306.00978 , 2023.
[42] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “Optq:
Accurate quantization for generative pre-trained transformers,”
inThe Eleventh International Conference on Learning Representations ,
2022.
[43] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, “Llm. int8
(): 8-bit matrix multiplication for transformers at scale,” arXiv
preprint arXiv:2208.07339 , 2022.
[44] Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He,
“Zeroquant: Efficient and affordable post-training quantization
for large-scale transformers,” Advances in Neural Information Pro-
cessing Systems , vol. 35, pp. 27 168–27 183, 2022.
[45] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han,
“Smoothquant: Accurate and efficient post-training quantization
for large language models,” in International Conference on Machine
Learning . PMLR, 2023, pp. 38 087–38 099.
[46] Z. Liu, B. Oguz, C. Zhao, E. Chang, P . Stock, Y. Mehdad, Y. Shi,
R. Krishnamoorthi, and V . Chandra, “Llm-qat: Data-free quanti-
zation aware training for large language models,” arXiv preprint
arXiv:2305.17888 , 2023.
[47] Y. Chai, J. Gkountouras, G. G. Ko, D. Brooks, and G.-Y. Wei, “Int2.
1: Towards fine-tunable quantized large language models with
error correction through low-rank adaptation,” arXiv preprint
arXiv:2306.08162 , 2023.
[48] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer,
“Qlora: Efficient finetuning of quantized llms,” arXiv preprint
arXiv:2305.14314 , 2023.
[49] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W.
Mahoney, and K. Keutzer, “Squeezellm: Dense-and-sparse quan-
tization,” arXiv preprint arXiv:2306.07629 , 2023.
[50] Y. J. Kim, R. Henry, R. Fahim, and H. H. Awadalla, “Finequant:
Unlocking efficiency with fine-grained weight-only quantization
for llms,” arXiv preprint arXiv:2308.09723 , 2023.
[51] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee, “nuqmm:
Quantized matmul for efficient inference of large-scale generative
language models,” arXiv preprint arXiv:2206.09557 , 2022.
[52] M. Rastegari, V . Ordonez, J. Redmon, and A. Farhadi, “Xnor-
net: Imagenet classification using binary convolutional neural
networks,” in European conference on computer vision . Springer,
2016, pp. 525–542.
[53] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer, “8-bit op-
timizers via block-wise quantization,” in International Conference
on Learning Representations , 2021.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 39
[54] J. H. Lee, J. Kim, S. J. Kwon, and D. Lee, “Flexround: Learnable
rounding based on element-wise division for post-training quan-
tization,” arXiv preprint arXiv:2306.00317 , 2023.
[55] J. Chee, Y. Cai, V . Kuleshov, and C. De Sa, “Quip: 2-bit quanti-
zation of large language models with guarantees,” arXiv preprint
arXiv:2307.13304 , 2023.
[56] E. Frantar and D. Alistarh, “Optimal brain compression: A
framework for accurate post-training quantization and pruning,”
Advances in Neural Information Processing Systems , vol. 35, pp.
4475–4488, 2022.
[57] C. Lee, J. Jin, T. Kim, H. Kim, and E. Park, “Owq: Lessons learned
from activation outliers for weight quantization in large language
models,” arXiv preprint arXiv:2306.02272 , 2023.
[58] W. Cheng, W. Zhang, H. Shen, Y. Cai, X. He, and K. Lv, “Optimize
weight rounding via signed gradient descent for the quantization
of llms,” arXiv preprint arXiv:2309.05516 , 2023.
[59] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and
T. Blankevoort, “Up or down? adaptive rounding for post-
training quantization,” in International Conference on Machine
Learning . PMLR, 2020, pp. 7197–7206.
[60] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel
mixture models,” in International Conference on Learning Represen-
tations , 2016.
[61] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P . J. Liu, “Exploring the limits of transfer
learning with a unified text-to-text transformer,” The Journal of
Machine Learning Research , vol. 21, no. 1, pp. 5485–5551, 2020.
[62] Y. Li, Y. Yu, C. Liang, P . He, N. Karampatziakis, W. Chen, and
T. Zhao, “Loftq: Lora-fine-tuning-aware quantization for large
language models,” arXiv preprint arXiv:2310.08659 , 2023.
[63] Z. Yuan, L. Niu, J. Liu, W. Liu, X. Wang, Y. Shang, G. Sun, Q. Wu,
J. Wu, and B. Wu, “Rptq: Reorder-based post-training quantiza-
tion for large language models,” arXiv preprint arXiv:2304.01089 ,
2023.
[64] Y. Zhang, L. Zhao, S. Cao, W. Wang, T. Cao, F. Yang, M. Yang,
S. Zhang, and N. Xu, “Integer or floating point? new outlooks for
low-bit quantization on large language models,” arXiv preprint
arXiv:2305.12356 , 2023.
[65] X. Wu, Z. Yao, and Y. He, “Zeroquant-fp: A leap forward in llms
post-training w4a8 quantization using floating-point formats,”
arXiv preprint arXiv:2307.09782 , 2023.
[66] X. Wu, Z. Yao, and Y. H. Zeroquant-fp, “A leap forward in llms
post-training w4a8 quantization using floating-point formats,”
arXiv preprint arXiv:2307.09782 , 2023.
[67] X. Wei, Y. Zhang, X. Zhang, R. Gong, S. Zhang, Q. Zhang, F. Yu,
and X. Liu, “Outlier suppression: Pushing the limit of low-bit
transformer language models,” Advances in Neural Information
Processing Systems , vol. 35, pp. 17 402–17 414, 2022.
[68] X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, and X. Liu,
“Outlier suppression+: Accurate quantization of large language
models by equivalent and optimal shifting and scaling,” arXiv
preprint arXiv:2304.09145 , 2023.
[69] Q. Li, Y. Zhang, L. Li, P . Yao, B. Zhang, X. Chu, Y. Sun, L. Du, and
Y. Xie, “Fptq: Fine-grained post-training quantization for large
language models,” arXiv preprint arXiv:2308.15987 , 2023.
[70] W. Shao, M. Chen, Z. Zhang, P . Xu, L. Zhao, Z. Li, K. Zhang,
P . Gao, Y. Qiao, and P . Luo, “Omniquant: Omnidirectionally
calibrated quantization for large language models,” arXiv preprint
arXiv:2308.13137 , 2023.
[71] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, “Qllm: Ac-
curate and efficient low-bitwidth quantization for large language
models,” arXiv preprint arXiv:2310.08041 , 2023.
[72] E. Yvinec, A. Dapgony, M. Cord, and K. Bailly, “Rex: Data-
free residual quantization error expansion,” arXiv preprint
arXiv:2203.14645 , 2022.
[73] C. Guo, J. Tang, W. Hu, J. Leng, C. Zhang, F. Yang, Y. Liu,
M. Guo, and Y. Zhu, “Olive: Accelerating large language models
via hardware-friendly outlier-victim pair quantization,” in Pro-
ceedings of the 50th Annual International Symposium on Computer
Architecture , 2023, pp. 1–15.
[74] M. Kim, S. Lee, S. Hong, D.-S. Chang, and J. Choi, “Under-
standing and improving knowledge distillation for quantization-
aware training of large transformer encoders,” arXiv preprint
arXiv:2211.11014 , 2022.
[75] J. O. Neill and S. Dutta, “Self-distilled quantization: Achieving
high compression rates in transformer-based language models,”
arXiv preprint arXiv:2307.05972 , 2023.[76] W.-Y. Hua, B. Williams, and D. Shamsi, “Lacos-bloom: Low-rank
adaptation with contrastive objective on 8 bits siamese-bloom,”
arXiv preprint arXiv:2305.06404 , 2023.
[77] A. Kaushal, T. Vaidhya, and I. Rish, “Lord: Low rank decomposi-
tion of monolingual code llms for one-shot compression,” arXiv
preprint arXiv:2309.14021 , 2023.
[78] Y. Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen,
X. Zhang, and Q. Tian, “Qa-lora: Quantization-aware low-
rank adaptation of large language models,” arXiv preprint
arXiv:2309.14717 , 2023.
[79] S. J. Kwon, J. Kim, J. Bae, K. M. Yoo, J.-H. Kim, B. Park, B. Kim,
J.-W. Ha, N. Sung, and D. Lee, “Alphatuning: Quantization-
aware parameter-efficient adaptation of large-scale pre-trained
language models,” arXiv preprint arXiv:2210.03858 , 2022.
[80] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, and
D. Lee, “Memory-efficient fine-tuning of compressed large lan-
guage models via sub-4-bit integer quantization,” arXiv preprint
arXiv:2305.14152 , 2023.
[81] M. Park, J. You, M. Nagel, and S. Chang, “Quadapter: Adapter for
gpt-2 quantization,” in Findings of the Association for Computational
Linguistics: EMNLP 2022 , 2022, pp. 2510–2517.
[82] Z. Xu, Z. Liu, B. Chen, Y. Tang, J. Wang, K. Zhou, X. Hu, and
A. Shrivastava, “Compress, then prompt: Improving accuracy-
efficiency trade-off of llm inference with transferable prompt,”
arXiv preprint arXiv:2305.11186 , 2023.
[83] H. Shen, H. Meng, B. Dong, Z. Wang, O. Zafrir, Y. Ding, Y. Luo,
H. Chang, Q. Gao, Z. Wang et al. , “An efficient sparse inference
software accelerator for transformer-based language models on
cpus,” arXiv preprint arXiv:2306.16601 , 2023.
[84] T. Pegolotti, E. Frantar, D. Alistarh, and M. P ¨uschel, “Generat-
ing efficient kernels for quantized inference on large language
models,” in Workshop on Efficient Systems for Foundation Models@
ICML2023 , 2023.
[85] K. Wang, Z. Liu, Y. Lin, J. Lin, and S. Han, “Haq: Hardware-aware
automated quantization with mixed precision,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition ,
2019, pp. 8612–8620.
[86] C. Yu, T. Chen, and Z. Gan, “Boost transformer-based language
models with gpu-friendly sparsity and quantization,” in Findings
of the Association for Computational Linguistics: ACL 2023 , 2023, pp.
218–235.
[87] Z. Lin, G. Qu, Q. Chen, X. Chen, Z. Chen, and K. Huang, “Push-
ing large language models to the 6g edge: Vision, challenges, and
opportunities,” arXiv preprint arXiv:2309.16739 , 2023.
[88] M. W. U. Rahman, M. M. Abrar, H. G. Copening, S. Hariri,
S. Shao, P . Satam, and S. Salehi, “Quantized transformer lan-
guage model implementations on edge devices,” arXiv preprint
arXiv:2310.03971 , 2023.
[89] E. Kurtic, D. Kuznedelev, E. Frantar, M. Goin, and D. Alistarh,
“Sparse finetuning for inference acceleration of large language
models,” arXiv preprint arXiv:2310.06927 , 2023.
[90] B. Isik, H. Kumbong, W. Ning, X. Yao, S. Koyejo, and C. Zhang,
“Gpt-zip: Deep compression of finetuned large language mod-
els,” in Workshop on Efficient Systems for Foundation Models@
ICML2023 , 2023.
[91] X. Wei, S. Gonugondla, W. Ahmad, S. Wang, B. Ray, H. Qian,
X. Li, V . Kumar, Z. Wang, Y. Tian et al. , “Greener yet powerful:
Taming large code generation models with quantization,” arXiv
preprint arXiv:2303.05378 , 2023.
[92] T. Hu, C. Meinel, and H. Yang, “Empirical evaluation of post-
training quantization methods for language tasks,” arXiv preprint
arXiv:2210.16621 , 2022.
[93] T. Dettmers and L. Zettlemoyer, “The case for 4-bit precision: k-
bit inference scaling laws,” in International Conference on Machine
Learning . PMLR, 2023, pp. 7750–7774.
[94] P . Liu, Z. Liu, Z.-F. Gao, D. Gao, W. X. Zhao, Y. Li, B. Ding, and J.-
R. Wen, “Do emergent abilities exist in quantized large language
models: An empirical study,” arXiv preprint arXiv:2307.08072 ,
2023.
[95] Y. Bondarenko, M. Nagel, and T. Blankevoort, “Quantizable
transformers: Removing outliers by helping attention heads do
nothing,” arXiv preprint arXiv:2306.12929 , 2023.
[96] A. Ahmadian, S. Dash, H. Chen, B. Venkitesh, S. Gou, P . Blunsom,
A.¨Ust¨un, and S. Hooker, “Intriguing properties of quantization
at scale,” arXiv preprint arXiv:2305.19268 , 2023.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 40
[97] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li, “Learning structured
sparsity in deep neural networks,” Advances in neural information
processing systems , vol. 29, 2016.
[98] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights
and connections for efficient neural network,” Advances in neural
information processing systems , vol. 28, 2015.
[99] S. Narang, E. Undersander, and G. Diamos, “Block-sparse recur-
rent neural networks,” arXiv preprint arXiv:1711.02782 , 2017.
[100] M. A. Gordon, K. Duh, and N. Andrews, “Compressing bert:
Studying the effects of weight pruning on transfer learning,”
arXiv preprint arXiv:2002.08307 , 2020.
[101] T. Chen, J. Frankle, S. Chang, S. Liu, Y. Zhang, Z. Wang, and
M. Carbin, “The lottery ticket hypothesis for pre-trained bert net-
works,” Advances in neural information processing systems , vol. 33,
pp. 15 834–15 846, 2020.
[102] S. Prasanna, A. Rogers, and A. Rumshisky, “When bert plays the
lottery, all tickets are winning,” arXiv preprint arXiv:2005.00561 ,
2020.
[103] A. K. Jaiswal, S. Liu, T. Chen, Y. Ding, and Z. Wang, “Instant
soup: Cheap pruning ensembles in a single pass can draw lottery
tickets from large models,” in International Conference on Machine
Learning . PMLR, 2023, pp. 14 691–14 701.
[104] O. Zafrir, A. Larey, G. Boudoukh, H. Shen, and M. Wasserblat,
“Prune once for all: Sparse pre-trained language models,” arXiv
preprint arXiv:2111.05754 , 2021.
[105] M. Zhu and S. Gupta, “To prune, or not to prune: exploring
the efficacy of pruning for model compression,” arXiv preprint
arXiv:1710.01878 , 2017.
[106] E. Kurtic and D. Alistarh, “Gmp*: Well-tuned global magnitude
pruning can outperform most bert-pruning methods,” arXiv
preprint arXiv:2210.06384 , 2022.
[107] L. Yin, S. Liu, A. Jaiswal, S. Kundu, and Z. Wang, “Junk dna hy-
pothesis: A task-centric angle of llm pre-trained weights through
sparsity,” arXiv preprint arXiv:2310.02277 , 2023.
[108] V . Sanh, T. Wolf, and A. Rush, “Movement pruning: Adaptive
sparsity by fine-tuning,” Advances in Neural Information Processing
Systems , vol. 33, pp. 20 378–20 389, 2020.
[109] T. Jiang, D. Wang, F. Zhuang, R. Xie, and F. Xia, “Pruning pre-
trained language models without fine-tuning,” arXiv preprint
arXiv:2210.06210 , 2022.
[110] S. Ren and K. Q. Zhu, “Low-rank prune-and-factorize for lan-
guage model compression,” arXiv preprint arXiv:2306.14152 , 2023.
[111] Q. Zhang, S. Zuo, C. Liang, A. Bukharin, P . He, W. Chen,
and T. Zhao, “Platon: Pruning large transformer models with
upper confidence bound of weight importance,” in International
Conference on Machine Learning . PMLR, 2022, pp. 26 809–26 823.
[112] Y. Li, F. Luo, C. Tan, M. Wang, S. Huang, S. Li, and J. Bai,
“Parameter-efficient sparsity for large language models fine-
tuning,” arXiv preprint arXiv:2205.11005 , 2022.
[113] M. Zhang, C. Shen, Z. Yang, L. Ou, X. Yu, B. Zhuang et al. ,
“Pruning meets low-rank parameter-efficient fine-tuning,” arXiv
preprint arXiv:2305.18403 , 2023.
[114] Y. LeCun, J. Denker, and S. Solla, “Optimal brain damage,”
Advances in neural information processing systems , vol. 2, 1989.
[115] B. Hassibi, D. G. Stork, and G. J. Wolff, “Optimal brain surgeon
and general network pruning,” in IEEE international conference on
neural networks . IEEE, 1993, pp. 293–299.
[116] E. Kurtic, D. Campos, T. Nguyen, E. Frantar, M. Kurtz, B. Fineran,
M. Goin, and D. Alistarh, “The optimal bert surgeon: Scalable and
accurate second-order pruning for large language models,” arXiv
preprint arXiv:2203.07259 , 2022.
[117] C. Louizos, M. Welling, and D. P . Kingma, “Learning sparse
neural networks through l0regularization,” arXiv preprint
arXiv:1712.01312 , 2017.
[118] F.-M. Guo, S. Liu, F. S. Mungall, X. Lin, and Y. Wang, “Reweighted
proximal pruning for large-scale language representation,” arXiv
preprint arXiv:1909.12486 , 2019.
[119] A. Mishra, J. A. Latorre, J. Pool, D. Stosic, D. Stosic, G. Venkatesh,
C. Yu, and P . Micikevicius, “Accelerating sparse deep neural
networks,” arXiv preprint arXiv:2104.08378 , 2021.
[120] A. Zhou, Y. Ma, J. Zhu, J. Liu, Z. Zhang, K. Yuan, W. Sun,
and H. Li, “Learning n: m fine-grained structured sparse neural
networks from scratch,” arXiv preprint arXiv:2102.04010 , 2021.
[121] O. Nordstr ¨om, “Unstructured pruning of pre-trained language
models tuned for sentiment classification.” 2022.[122] B. Cui, Y. Li, and Z. Zhang, “Joint structured pruning and dense
knowledge distillation for efficient transformer model compres-
sion,” Neurocomputing , vol. 458, pp. 56–69, 2021.
[123] B. Li, Z. Kong, T. Zhang, J. Li, Z. Li, H. Liu, and C. Ding, “Effi-
cient transformer-based large scale language representations us-
ing hardware-friendly block structured pruning,” arXiv preprint
arXiv:2009.08065 , 2020.
[124] P . Michel, O. Levy, and G. Neubig, “Are sixteen heads really
better than one?” Advances in neural information processing systems ,
vol. 32, 2019.
[125] J. Li, R. Cotterell, and M. Sachan, “Differentiable subset pruning
of transformer heads,” Transactions of the Association for Computa-
tional Linguistics , vol. 9, pp. 1442–1459, 2021.
[126] Z. Yang, Y. Cui, X. Yao, and S. Wang, “Gradient-based intra-
attention pruning on pre-trained language models,” arXiv
preprint arXiv:2212.07634 , 2022.
[127] G. Wang, Q. Cao, J. Yang, and Y. Sun, “Task-oriented memory-
efficient pruning-adapter,” arXiv preprint arXiv:2303.14704 , 2023.
[128] C. J. Maddison, A. Mnih, and Y. W. Teh, “The concrete distri-
bution: A continuous relaxation of discrete random variables,”
arXiv preprint arXiv:1611.00712 , 2016.
[129] E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov, “Ana-
lyzing multi-head self-attention: Specialized heads do the heavy
lifting, the rest can be pruned,” arXiv preprint arXiv:1905.09418 ,
2019.
[130] F. Lagunas, E. Charlaix, V . Sanh, and A. M. Rush, “Block pruning
for faster transformers,” arXiv preprint arXiv:2109.04838 , 2021.
[131] R. Xu, F. Luo, C. Wang, B. Chang, J. Huang, S. Huang, and
F. Huang, “From dense to sparse: Contrastive pruning for better
pre-trained language model compression,” in Proceedings of the
AAAI Conference on Artificial Intelligence , vol. 36, no. 10, 2022, pp.
11 547–11 555.
[132] Z. Liu, F. Li, G. Li, and J. Cheng, “Ebert: Efficient bert inference
with dynamic structured pruning,” in Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 , 2021, pp. 4814–
4823.
[133] A. Khetan and Z. Karnin, “schubert: Optimizing elements of
bert,” arXiv preprint arXiv:2005.06628 , 2020.
[134] E. Kurtic, E. Frantar, and D. Alistarh, “Ziplm: Hardware-
aware structured pruning of language models,” arXiv preprint
arXiv:2302.04089 , 2023.
[135] A. Klein, J. Golebiowski, X. Ma, V . Perrone, and C. Archambeau,
“Structural pruning of large language models via neural archi-
tecture search,” in AutoML Conference 2023 (Workshop) , 2023.
[136] S. Park, H. Choi, and U. Kang, “Knowledge-preserving prun-
ing for pre-trained language models without retraining,” arXiv
preprint arXiv:2308.03449 , 2023.
[137] Y. Li, Y. Yu, Q. Zhang, C. Liang, P . He, W. Chen, and T. Zhao,
“Losparse: Structured compression of large language models
based on low-rank and sparse approximation,” arXiv preprint
arXiv:2306.11222 , 2023.
[138] M. Santacroce, Z. Wen, Y. Shen, and Y. Li, “What matters in
the structured pruning of generative language models?” arXiv
preprint arXiv:2302.03773 , 2023.
[139] N. Yang, Y. Jang, H. Lee, S. Jeong, and K. Jung, “Task-specific
compression for multi-task language models using attribution-
based pruning,” in Findings of the Association for Computational
Linguistics: EACL 2023 , 2023, pp. 582–592.
[140] J. McCarley, R. Chakravarti, and A. Sil, “Structured prun-
ing of a bert-based question answering model,” arXiv preprint
arXiv:1910.06360 , 2019.
[141] Z. Wang, J. Wohlwend, and T. Lei, “Structured pruning of large
language models,” arXiv preprint arXiv:1910.04732 , 2019.
[142] M. Xia, Z. Zhong, and D. Chen, “Structured pruning learns
compact and accurate models,” arXiv preprint arXiv:2204.00408 ,
2022.
[143] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P . Luo, and
N. Wong, “Structured pruning for efficient generative pre-trained
language models,” in Findings of the Association for Computational
Linguistics: ACL 2023 , 2023, pp. 10 880–10 895.
[144] A. Fan, E. Grave, and A. Joulin, “Reducing transformer
depth on demand with structured dropout,” arXiv preprint
arXiv:1909.11556 , 2019.
[145] M. Zhang and Y. He, “Accelerating training of transformer-based
language models with progressive layer dropping,” Advances in
Neural Information Processing Systems , vol. 33, pp. 14 011–14 023,
2020.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 41
[146] H. Sajjad, F. Dalvi, N. Durrani, and P . Nakov, “On the effect of
dropping layers of pre-trained transformer models,” Computer
Speech & Language , vol. 77, p. 101429, 2023.
[147] S. Goyal, A. R. Choudhury, S. Raje, V . Chakaravarthy, Y. Sabhar-
wal, and A. Verma, “Power-bert: Accelerating bert inference via
progressive word-vector elimination,” in International Conference
on Machine Learning . PMLR, 2020, pp. 3690–3699.
[148] S. Kim, S. Shen, D. Thorsley, A. Gholami, W. Kwon, J. Hassoun,
and K. Keutzer, “Learned token pruning for transformers,” in
Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining , 2022, pp. 784–794.
[149] H. Wang, Z. Zhang, and S. Han, “Spatten: Efficient sparse at-
tention architecture with cascade token and head pruning,” in
2021 IEEE International Symposium on High-Performance Computer
Architecture (HPCA) . IEEE, 2021, pp. 97–110.
[150] Z. Lin, J. Z. Liu, Z. Yang, N. Hua, and D. Roth, “Pruning redun-
dant mappings in transformer models via spectral-normalized
identity prior,” arXiv preprint arXiv:2010.01791 , 2020.
[151] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter, “A simple and effective
pruning approach for large language models,” arXiv preprint
arXiv:2306.11695 , 2023.
[152] Y. Zhang, H. Bai, H. Lin, J. Zhao, L. Hou, and C. V . Cannistraci,
“An efficient plug-and-play post-training pruning strategy in
large language models,” 2023.
[153] Y. Li, L. Niu, X. Zhang, K. Liu, J. Zhu, and Z. Kang, “E-sparse:
Boosting the large language model inference through entropy-
based n: M sparsity,” arXiv preprint arXiv:2310.15929 , 2023.
[154] E. Frantar and D. Alistarh, “Sparsegpt: Massive language models
can be accurately pruned in one-shot,” 2023.
[155] H. Shao, B. Liu, and Y. Qian, “One-shot sensitivity-aware mixed
sparsity pruning for large language models,” arXiv preprint
arXiv:2310.09499 , 2023.
[156] R. J. Das, L. Ma, and Z. Shen, “Beyond size: How gradients
shape pruning decisions in large language models,” arXiv preprint
arXiv:2311.04902 , 2023.
[157] Anonymous, “Pushing gradient towards zero: A novel pruning
method for large language models,” 2024. [Online]. Available:
https://openreview.net/forum?id=IU4L7wiwxw
[158] Y. An, X. Zhao, T. Yu, M. Tang, and J. Wang, “Fluctuation-based
adaptive structured pruning for large language models,” arXiv
preprint arXiv:2312.11983 , 2023.
[159] S. Ashkboos, M. L. Croci, M. G. d. Nascimento, T. Hoefler,
and J. Hensman, “Slicegpt: Compress large language models
by deleting rows and columns,” arXiv preprint arXiv:2401.15024 ,
2024.
[160] X. Ma, G. Fang, and X. Wang, “Llm-pruner: On the struc-
tural pruning of large language models,” arXiv preprint
arXiv:2305.11627 , 2023.
[161] T. Chen, T. Ding, B. Yadav, I. Zharkov, and L. Liang, “Lorashear:
Efficient large language model structured pruning and knowl-
edge recovery,” arXiv preprint arXiv:2310.18356 , 2023.
[162] B. Zhao, H. Hajishirzi, and Q. Cao, “Apt: Adaptive pruning
and tuning pretrained language models for efficient training and
inference,” arXiv preprint arXiv:2401.12200 , 2024.
[163] M. Xia, T. Gao, Z. Zeng, and D. Chen, “Sheared llama: Accelerat-
ing language model pre-training via structured pruning,” arXiv
preprint arXiv:2310.06694 , 2023.
[164] S. Guo, J. Xu, L. L. Zhang, and M. Yang, “Compresso: Structured
pruning with collaborative prompting learns compact large lan-
guage models,” arXiv preprint arXiv:2310.05015 , 2023.
[165] T. F. van der Ouderaa, M. Nagel, M. van Baalen, Y. M.
Asano, and T. Blankevoort, “The llm surgeon,” arXiv preprint
arXiv:2312.17244 , 2023.
[166] M. Williams and N. Aletras, “How does calibration data affect
the post-training pruning and quantization of large language
models?” arXiv preprint arXiv:2311.09755 , 2023.
[167] M. Zimmer, M. Andoni, C. Spiegel, and S. Pokutta, “Perp: Re-
thinking the prune-retrain paradigm in the era of llms,” arXiv
preprint arXiv:2312.15230 , 2023.
[168] S. Gholami and M. Omar, “Can pruning make large language
models more efficient?” arXiv preprint arXiv:2310.04573 , 2023.
[169] T. Valicenti, J. Vidal, and R. Patnaik, “Mini-gpts: Efficient large
language models through contextual pruning,” arXiv preprint
arXiv:2312.12682 , 2023.
[170] Y. Ji, Y. Cao, and J. Liu, “Pruning large language models via
accuracy predictor,” arXiv preprint arXiv:2309.09507 , 2023.[171] Anonymous, “Outlier weighed layerwise sparsity (OWL): A
missing secret sauce for pruning LLMs to high sparsity,” in
Submitted to The Twelfth International Conference on Learning
Representations , 2023, under review. [Online]. Available: https:
//openreview.net/forum?id=pOBvr1PxFd
[172] ——, “BESA: Pruning large language models with blockwise
parameter-efficient sparsity allocation,” in The Twelfth
International Conference on Learning Representations , 2024. [Online].
Available: https://openreview.net/forum?id=gC6JTEU3jl
[173] A. Syed, P . H. Guo, and V . Sundarapandiyan, “Prune and tune:
Improving efficient pruning techniques for massive language
models,” 2023.
[174] Y. Zhang, L. Zhao, M. Lin, Y. Sun, Y. Yao, X. Han, J. Tanner, S. Liu,
and R. Ji, “Dynamic sparse no training: Training-free fine-tuning
for sparse llms,” arXiv preprint arXiv:2310.08915 , 2023.
[175] V . Bo ˇza, “Fast and optimal weight update for pruned large
language models,” 2024.
[176] H. Xia, Z. Zheng, Y. Li, D. Zhuang, Z. Zhou, X. Qiu, Y. Li, W. Lin,
and S. L. Song, “Flash-llm: Enabling cost-effective and highly-
efficient large generative model inference with unstructured spar-
sity,” arXiv preprint arXiv:2309.10285 , 2023.
[177] V . Srinivasan, D. Gandhi, U. Thakker, and R. Prabhakar, “Training
large language models efficiently with sparsity and dataflow,”
arXiv preprint arXiv:2304.05511 , 2023.
[178] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in
a neural network,” arXiv preprint arXiv:1503.02531 , 2015.
[179] Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu, “Deep mutual
learning,” in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2018, pp. 4320–4328.
[180] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and
Y. Bengio, “Fitnets: Hints for thin deep nets,” arXiv preprint
arXiv:1412.6550 , 2014.
[181] J. Yim, D. Joo, J.-H. Bae, and J. Kim, “A gift from knowledge
distillation: Fast optimization, network minimization and
transfer learning,” 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) , pp. 7130–7138, 2017. [Online].
Available: https://api.semanticscholar.org/CorpusID:206596723
[182] R. Tang, Y. Lu, L. Liu, L. Mou, O. Vechtomova, and J. Lin,
“Distilling task-specific knowledge from bert into simple neural
networks,” arXiv preprint arXiv:1903.12136 , 2019.
[183] S. Sun, Y. Cheng, Z. Gan, and J. Liu, “Patient knowledge distilla-
tion for bert model compression,” arXiv preprint arXiv:1908.09355 ,
2019.
[184] L. Hou, Z. Huang, L. Shang, X. Jiang, X. Chen, and Q. Liu, “Dyn-
abert: Dynamic bert with adaptive width and depth,” Advances
in Neural Information Processing Systems , vol. 33, pp. 9782–9793,
2020.
[185] W. Zhou, C. Xu, and J. McAuley, “Bert learns to teach: Knowledge
distillation with meta learning,” arXiv preprint arXiv:2106.04570 ,
2021.
[186] S. Wu, H. Chen, X. Quan, Q. Wang, and R. Wang, “Ad-kd:
Attribution-driven knowledge distillation for language model
compression,” arXiv preprint arXiv:2305.10010 , 2023.
[187] D. Chen, Y. Li, M. Qiu, Z. Wang, B. Li, B. Ding, H. Deng, J. Huang,
W. Lin, and J. Zhou, “Adabert: Task-adaptive bert compression
with differentiable neural architecture search,” arXiv preprint
arXiv:2001.04246 , 2020.
[188] K. J. Liang, W. Hao, D. Shen, Y. Zhou, W. Chen, C. Chen, and
L. Carin, “Mixkd: Towards efficient distillation of large-scale
language models,” arXiv preprint arXiv:2011.00593 , 2020.
[189] H. Pan, C. Wang, M. Qiu, Y. Zhang, Y. Li, and J. Huang, “Meta-kd:
A meta knowledge distillation framework for language model
compression across domains,” arXiv preprint arXiv:2012.01266 ,
2020.
[190] J. Zhang, A. Muhamed, A. Anantharaman, G. Wang, C. Chen,
K. Zhong, Q. Cui, Y. Xu, B. Zeng, T. M. Chilimbi, and
Y. Chen, “Reaugkd: Retrieval-augmented knowledge distillation
for pre-trained language models,” in Annual Meeting of the
Association for Computational Linguistics , 2023. [Online]. Available:
https://api.semanticscholar.org/CorpusID:259370551
[191] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a
distilled version of bert: smaller, faster, cheaper and lighter,”
arXiv preprint arXiv:1910.01108 , 2019.
[192] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou,
“Minilm: Deep self-attention distillation for task-agnostic com-
pression of pre-trained transformers,” Advances in Neural Infor-
mation Processing Systems , vol. 33, pp. 5776–5788, 2020.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 42
[193] Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou, “Mobilebert:
a compact task-agnostic bert for resource-limited devices,” arXiv
preprint arXiv:2004.02984 , 2020.
[194] C. Liang, H. Jiang, Z. Li, X. Tang, B. Yin, and T. Zhao, “Homodis-
til: Homotopic task-agnostic distillation of pre-trained transform-
ers,” arXiv preprint arXiv:2302.09632 , 2023.
[195] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and
Q. Liu, “Tinybert: Distilling bert for natural language under-
standing,” arXiv preprint arXiv:1909.10351 , 2019.
[196] C. Liang, S. Zuo, Q. Zhang, P . He, W. Chen, and T. Zhao, “Less
is more: Task-aware layer-wise distillation for language model
compression,” in International Conference on Machine Learning .
PMLR, 2023, pp. 20 852–20 867.
[197] I. Turc, M.-W. Chang, K. Lee, and K. Toutanova, “Well-read
students learn better: On the importance of pre-training compact
models,” arXiv preprint arXiv:1908.08962 , 2019.
[198] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz,
“mixup: Beyond empirical risk minimization,” arXiv preprint
arXiv:1710.09412 , 2017.
[199] S. Dasgupta, T. Cohn, and T. Baldwin, “Cost-effective
distillation of large language models,” in Annual Meeting of the
Association for Computational Linguistics , 2023. [Online]. Available:
https://api.semanticscholar.org/CorpusID:259858962
[200] S. I. Mirzadeh, M. Farajtabar, A. Li, N. Levine, A. Matsukawa,
and H. Ghasemzadeh, “Improved knowledge distillation via
teacher assistant,” in Proceedings of the AAAI conference on artificial
intelligence , vol. 34, no. 04, 2020, pp. 5191–5198.
[201] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,
and H. Hajishirzi, “Self-instruct: Aligning language model with
self generated instructions,” arXiv preprint arXiv:2212.10560 , 2022.
[202] B. Peng, C. Li, P . He, M. Galley, and J. Gao, “Instruction tuning
with gpt-4,” arXiv preprint arXiv:2304.03277 , 2023.
[203] M. Wu, A. Waheed, C. Zhang, M. Abdul-Mageed, and A. F. Aji,
“Lamini-lm: A diverse herd of distilled models from large-scale
instructions,” arXiv preprint arXiv:2304.14402 , 2023.
[204] Y. Jiang, C. Chan, M. Chen, and W. Wang, “Lion: Adversarial
distillation of closed-source large language model,” arXiv preprint
arXiv:2305.12870 , 2023.
[205] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li,
C. Guestrin, P . Liang, and T. B. Hashimoto, “Stan-
ford alpaca: An instruction-following llama model,”
https://github.com/tatsu-lab/stanford alpaca, 2023.
[206] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang,
L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and
E. P . Xing, “Vicuna: An open-source chatbot impressing gpt-4
with 90%* chatgpt quality,” March 2023. [Online]. Available:
https://lmsys.org/blog/2023-03-30-vicuna/
[207] Y. Anand, Z. Nussbaum, B. Duderstadt, B. Schmidt, and A. Mul-
yar, “Gpt4all: Training an assistant-style chatbot with large scale
data distillation from gpt-3.5-turbo,” GitHub , 2023.
[208] H. Chen, A. Saha, S. Hoi, and S. Joty, “Personalised distillation:
Empowering open-sourced llms with adaptive learning for code
generation,” arXiv preprint arXiv:2310.18628 , 2023.
[209] W. Zhou, S. Zhang, Y. Gu, M. Chen, and H. Poon, “Universalner:
Targeted distillation from large language models for open named
entity recognition,” arXiv preprint arXiv:2308.03279 , 2023.
[210] S. Li, J. Chen, Y. Shen, Z. Chen, X. Zhang, Z. Li, H. Wang, J. Qian,
B. Peng, Y. Mao et al. , “Explanations from large language models
make small reasoners better,” arXiv preprint arXiv:2210.06726 ,
2022.
[211] L. C. Magister, J. Mallinson, J. Adamek, E. Malmi, and A. Sev-
eryn, “Teaching small language models to reason,” arXiv preprint
arXiv:2212.08410 , 2022.
[212] C.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H. Nakhost, Y. Fujii, A. Ratner,
R. Krishna, C.-Y. Lee, and T. Pfister, “Distilling step-by-step!
outperforming larger language models with less training data
and smaller model sizes,” arXiv preprint arXiv:2305.02301 , 2023.
[213] S. Wadhwa, S. Amir, and B. C. Wallace, “Revisiting relation
extraction in the era of large language models,” arXiv preprint
arXiv:2305.05003 , 2023.
[214] N. Ho, L. Schmid, and S.-Y. Yun, “Large language models are
reasoning teachers,” arXiv preprint arXiv:2212.10071 , 2022.
[215] K. Shridhar, A. Stolfo, and M. Sachan, “Distilling reasoning
capabilities into smaller language models,” in Findings of the
Association for Computational Linguistics: ACL 2023 , 2023, pp. 7059–
7073.[216] P . Wang, Z. Wang, Z. Li, Y. Gao, B. Yin, and X. Ren, “Scott:
Self-consistent chain-of-thought distillation,” arXiv preprint
arXiv:2305.01879 , 2023.
[217] M. Kang, S. Lee, J. Baek, K. Kawaguchi, and S. J. Hwang,
“Knowledge-augmented reasoning distillation for small lan-
guage models in knowledge-intensive tasks,” arXiv preprint
arXiv:2305.18395 , 2023.
[218] Z. Jie and W. Lu, “Leveraging training data in few-shot prompt-
ing for numerical reasoning,” arXiv preprint arXiv:2305.18170 ,
2023.
[219] X. Zhu, B. Qi, K. Zhang, X. Long, and B. Zhou, “Pad: Program-
aided distillation specializes large models in reasoning,” arXiv
preprint arXiv:2305.13888 , 2023.
[220] L. H. Li, J. Hessel, Y. Yu, X. Ren, K.-W. Chang, and Y. Choi,
“Symbolic chain-of-thought distillation: Small models can also”
think” step-by-step,” arXiv preprint arXiv:2306.14050 , 2023.
[221] H. Chen, S. Wu, X. Quan, R. Wang, M. Yan, and J. Zhang, “Mcc-
kd: Multi-cot consistent knowledge distillation,” arXiv preprint
arXiv:2310.14747 , 2023.
[222] H. Chae, Y. Song, K. T.-i. Ong, T. Kwon, M. Kim, Y. Yu, D. Lee,
D. Kang, and J. Yeo, “Dialogue chain-of-thought distillation
for commonsense-aware conversational agents,” arXiv preprint
arXiv:2310.09343 , 2023.
[223] Z. Wang, S. Huang, Y. Liu, J. Wang, M. Song, Z. Zhang, H. Huang,
F. Wei, W. Deng, F. Sun et al. , “Democratizing reasoning ability:
Tailored learning from large language model,” arXiv preprint
arXiv:2310.13332 , 2023.
[224] Y. Ma, H. Jiang, and C. Fan, “Sci-cot: Leveraging large language
models for enhanced knowledge distillation in small models for
scientific qa,” arXiv preprint arXiv:2308.04679 , 2023.
[225] Y. Fu, H. Peng, L. Ou, A. Sabharwal, and T. Khot, “Specializing
smaller language models towards multi-step reasoning,” arXiv
preprint arXiv:2301.12726 , 2023.
[226] Y. Huang, Y. Chen, Z. Yu, and K. McKeown, “In-context learning
distillation: Transferring few-shot learning ability of pre-trained
language models,” arXiv preprint arXiv:2212.10670 , 2022.
[227] L. Wang, N. Yang, and F. Wei, “Learning to retrieve in-
context examples for large language models,” arXiv preprint
arXiv:2307.07164 , 2023.
[228] P . West, C. Bhagavatula, J. Hessel, J. D. Hwang, L. Jiang, R. L.
Bras, X. Lu, S. Welleck, and Y. Choi, “Symbolic knowledge distil-
lation: from general language models to commonsense models,”
arXiv preprint arXiv:2110.07178 , 2021.
[229] Z. Chen, Q. Gao, A. Bosselut, A. Sabharwal, and K. Richardson,
“Disco: distilling counterfactuals with large language models,”
inProceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , 2023, pp. 5514–
5528.
[230] Y. Gu, S. Zhang, N. Usuyama, Y. Woldesenbet, C. Wong, P . Sana-
pathi, M. Wei, N. Valluri, E. Strandberg, T. Naumann et al. ,
“Distilling large language models for biomedical knowledge
extraction: A case study on adverse drug events,” arXiv preprint
arXiv:2307.06439 , 2023.
[231] G. Sahu, O. Vechtomova, D. Bahdanau, and I. H. Laradji,
“Promptmix: A class boundary augmentation method for large
language model distillation,” arXiv preprint arXiv:2310.14192 ,
2023.
[232] A. Gudibande, E. Wallace, C. Snell, X. Geng, H. Liu, P . Abbeel,
S. Levine, and D. Song, “The false promise of imitating propri-
etary llms,” arXiv preprint arXiv:2305.15717 , 2023.
[233] Y. Gu, L. Dong, F. Wei, and M. Huang, “Knowledge distillation
of large language models,” arXiv preprint arXiv:2306.08543 , 2023.
[234] R. Agarwal, N. Vieillard, Y. Zhou, P . Stanczyk, S. Ramos, M. Geist,
and O. Bachem, “Generalized knowledge distillation for auto-
regressive language models,” arXiv preprint arXiv:2306.13649 ,
2023.
[235] S. Padmanabhan, Y. Onoe, M. J. Zhang, G. Durrett, and E. Choi,
“Propagating knowledge updates to lms through distillation,”
arXiv preprint arXiv:2306.09306 , 2023.
[236] M. Kim, S. Lee, J. Lee, S. Hong, D.-S. Chang, W. Sung, and J. Choi,
“Token-scaled logit distillation for ternary weight generative
language models,” arXiv preprint arXiv:2308.06744 , 2023.
[237] C. Zhang, D. Song, Z. Ye, and Y. Gao, “Towards the law
of capacity gap in distilling language models,” arXiv preprint
arXiv:2311.07052 , 2023.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 43
[238] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V . Le, and R. Salakhutdi-
nov, “Transformer-xl: Attentive language models beyond a fixed-
length context,” arXiv preprint arXiv:1901.02860 , 2019.
[239] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generat-
ing long sequences with sparse transformers,” arXiv preprint
arXiv:1904.10509 , 2019.
[240] S. Sukhbaatar, E. Grave, P . Bojanowski, and A. Joulin, “Adaptive
attention span in transformers,” arXiv preprint arXiv:1905.07799 ,
2019.
[241] G. M. Correia, V . Niculae, and A. F. Martins, “Adaptively sparse
transformers,” arXiv preprint arXiv:1909.00015 , 2019.
[242] Z. Ye, Q. Guo, Q. Gan, X. Qiu, and Z. Zhang, “Bp-transformer:
Modelling long-range context via binary partitioning,” arXiv
preprint arXiv:1911.04070 , 2019.
[243] J. Qiu, H. Ma, O. Levy, S. W.-t. Yih, S. Wang, and J. Tang,
“Blockwise self-attention for long document understanding,”
arXiv preprint arXiv:1911.02972 , 2019.
[244] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-
document transformer,” arXiv preprint arXiv:2004.05150 , 2020.
[245] N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: The efficient
transformer,” arXiv preprint arXiv:2001.04451 , 2020.
[246] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti,
S. Ontanon, P . Pham, A. Ravula, Q. Wang, L. Yang et al. , “Big
bird: Transformers for longer sequences,” Advances in neural
information processing systems , vol. 33, pp. 17 283–17 297, 2020.
[247] Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan, “Sparse
sinkhorn attention,” in International Conference on Machine Learn-
ing. PMLR, 2020, pp. 9438–9447.
[248] X. Li, Y. Meng, M. Zhou, Q. Han, F. Wu, and J. Li, “Sac:
Accelerating and structuring self-attention via sparse adaptive
connection,” Advances in Neural Information Processing Systems ,
vol. 33, pp. 16 997–17 008, 2020.
[249] H. Ren, H. Dai, Z. Dai, M. Yang, J. Leskovec, D. Schuurmans, and
B. Dai, “Combiner: Full attention transformer with sparse com-
putation cost,” Advances in Neural Information Processing Systems ,
vol. 34, pp. 22 470–22 482, 2021.
[250] A. Roy, M. Saffar, A. Vaswani, and D. Grangier, “Efficient content-
based sparse attention with routing transformers,” Transactions of
the Association for Computational Linguistics , vol. 9, pp. 53–68, 2021.
[251] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, “Trans-
formers are rnns: Fast autoregressive transformers with linear
attention,” in International conference on machine learning . PMLR,
2020, pp. 5156–5165.
[252] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane,
T. Sarlos, P . Hawkins, J. Davis, A. Mohiuddin, L. Kaiser
et al. , “Rethinking attention with performers,” arXiv preprint
arXiv:2009.14794 , 2020.
[253] Y. Xiong, Z. Zeng, R. Chakraborty, M. Tan, G. Fung, Y. Li, and
V . Singh, “Nystr ¨omformer: A nystr ¨om-based algorithm for ap-
proximating self-attention,” in Proceedings of the AAAI Conference
on Artificial Intelligence , vol. 35, no. 16, 2021, pp. 14 138–14 148.
[254] W. Hua, Z. Dai, H. Liu, and Q. Le, “Transformer quality in linear
time,” in International Conference on Machine Learning . PMLR,
2022, pp. 9099–9117.
[255] I. Han, R. Jarayam, A. Karbasi, V . Mirrokni, D. P . Woodruff,
and A. Zandieh, “Hyperattention: Long-context attention in near-
linear time,” arXiv preprint arXiv:2310.05869 , 2023.
[256] Z. Shen, M. Zhang, H. Zhao, S. Yi, and H. Li, “Efficient atten-
tion: Attention with linear complexities,” in Proceedings of the
IEEE/CVF winter conference on applications of computer vision , 2021,
pp. 3531–3539.
[257] A. Rahimi and B. Recht, “Random features for large-scale ker-
nel machines,” Advances in neural information processing systems ,
vol. 20, 2007.
[258] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, “Lin-
former: Self-attention with linear complexity,” arXiv preprint
arXiv:2006.04768 , 2020.
[259] L. D. Lingle, “Transformer-vq: Linear-time transformers via vec-
tor quantization,” arXiv preprint arXiv:2309.16354 , 2023.
[260] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R ´e, “Flashattention:
Fast and memory-efficient exact attention with io-awareness,”
Advances in Neural Information Processing Systems , vol. 35, pp.
16 344–16 359, 2022.
[261] T. Dao, “Flashattention-2: Faster attention with better parallelism
and work partitioning,” arXiv preprint arXiv:2307.08691 , 2023.[262] H. Wang, Z. Wu, Z. Liu, H. Cai, L. Zhu, C. Gan, and S. Han, “Hat:
Hardware-aware transformers for efficient natural language pro-
cessing,” arXiv preprint arXiv:2005.14187 , 2020.
[263] P . Ren, Y. Xiao, X. Chang, P .-Y. Huang, Z. Li, X. Chen, and
X. Wang, “A comprehensive survey of neural architecture search:
Challenges and solutions,” ACM Computing Surveys (CSUR) ,
vol. 54, no. 4, pp. 1–34, 2021.
[264] Y. Liu, Y. Sun, B. Xue, M. Zhang, G. G. Yen, and K. C. Tan,
“A survey on evolutionary neural architecture search,” IEEE
transactions on neural networks and learning systems , 2021.
[265] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search:
A survey,” The Journal of Machine Learning Research , vol. 20, no. 1,
pp. 1997–2017, 2019.
[266] A. Wan, X. Dai, P . Zhang, Z. He, Y. Tian, S. Xie, B. Wu, M. Yu,
T. Xu, K. Chen et al. , “Fbnetv2: Differentiable neural architecture
search for spatial and channel dimensions,” in Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition ,
2020, pp. 12 965–12 974.
[267] B. Wu, X. Dai, P . Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian,
P . Vajda, Y. Jia, and K. Keutzer, “Fbnet: Hardware-aware efficient
convnet design via differentiable neural architecture search,”
inProceedings of the IEEE/CVF conference on computer vision and
pattern recognition , 2019, pp. 10 734–10 742.
[268] D. So, Q. Le, and C. Liang, “The evolved transformer,” in Interna-
tional conference on machine learning . PMLR, 2019, pp. 5877–5886.
[269] Y. Zhao, L. Dong, Y. Shen, Z. Zhang, F. Wei, and
W. Chen, “Memory-efficient differentiable transformer architec-
ture search,” arXiv preprint arXiv:2105.14669 , 2021.
[270] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable architec-
ture search,” arXiv preprint arXiv:1806.09055 , 2018.
[271] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,
R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei,
“Scaling laws for neural language models,” arXiv preprint
arXiv:2001.08361 , 2020.
[272] J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun,
H. Kianinejad, M. M. A. Patwary, Y. Yang, and Y. Zhou,
“Deep learning scaling is predictable, empirically,” arXiv preprint
arXiv:1712.00409 , 2017.
[273] C. Xu and J. McAuley, “A survey on dynamic neural networks
for natural language processing,” arXiv preprint arXiv:2202.07101 ,
2022.
[274] Y.-S. Chuang, Y. Xie, H. Luo, Y. Kim, J. R. Glass, and P . He, “Dola:
Decoding by contrasting layers improves factuality in large
language models,” ArXiv , vol. abs/2309.03883, 2023. [Online].
Available: https://api.semanticscholar.org/CorpusID:261582463
[275] J. Xin, R. Tang, J. Lee, Y. Yu, and J. J. Lin, “Deebert: Dynamic
early exiting for accelerating bert inference,” in Annual Meeting
of the Association for Computational Linguistics , 2020. [Online].
Available: https://api.semanticscholar.org/CorpusID:216552850
[276] W. Liu, P . Zhou, Z. Zhao, Z. Wang, H. Deng, and
Q. Ju, “Fastbert: a self-distilling bert with adaptive inference
time,” ArXiv , vol. abs/2004.02178, 2020. [Online]. Available:
https://api.semanticscholar.org/CorpusID:214802887
[277] S. Geng, P . Gao, Z. Fu, and Y. Zhang, “Romebert: Robust training
of multi-exit bert,” ArXiv , vol. abs/2101.09755, 2021. [Online].
Available: https://api.semanticscholar.org/CorpusID:231698881
[278] J. Wang, K. Chen, G. Chen, L. Shou, and J. McAuley, “Skipbert:
Efficient inference with shallow layer skipping,” in Annual
Meeting of the Association for Computational Linguistics , 2022.
[Online]. Available: https://api.semanticscholar.org/CorpusID:
248780497
[279] W. Zhu, “Leebert: Learned early exit for bert with cross-
level optimization,” in Annual Meeting of the Association
for Computational Linguistics , 2021. [Online]. Available: https:
//api.semanticscholar.org/CorpusID:236459809
[280] J. Kong, J. Wang, L.-C. Yu, and X. Zhang, “Accelerating
inference for pretrained language models by unified multi-
perspective early exiting,” in International Conference on
Computational Linguistics , 2022. [Online]. Available: https:
//api.semanticscholar.org/CorpusID:252818912
[281] D. Ye, Y. Lin, Y. Huang, and M. Sun, “Tr-bert: Dynamic token
reduction for accelerating bert inference,” in North American
Chapter of the Association for Computational Linguistics , 2021.
[Online]. Available: https://api.semanticscholar.org/CorpusID:
235097557
[282] D. Zeng, N. Du, T. Wang, Y. Xu, T. Lei, Z. Chen,
and C. Cui, “Learning to skip for language modeling,”
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 44
ArXiv , vol. abs/2311.15436, 2023. [Online]. Available: https:
//api.semanticscholar.org/CorpusID:265456419
[283] Y. Wang, K. Chen, H. Tan, and K. Guo, “Tabi: An efficient multi-
level inference system for large language models,” Proceedings
of the Eighteenth European Conference on Computer Systems , 2023.
[Online]. Available: https://api.semanticscholar.org/CorpusID:
258508784
[284] L. Chen, M. A. Zaharia, and J. Y. Zou, “Frugalgpt: How to
use large language models while reducing cost and improving
performance,” ArXiv , vol. abs/2305.05176, 2023. [Online].
Available: https://api.semanticscholar.org/CorpusID:258564349
[285] J. Zhang, R. Krishna, A. H. Awadallah, and C. Wang,
“Ecoassistant: Using llm assistant more affordably and
accurately,” ArXiv , vol. abs/2310.03046, 2023. [Online]. Available:
https://api.semanticscholar.org/CorpusID:263671677
[286] B. Zhu, Y. Sheng, L. Zheng, C. W. Barrett, M. I. Jordan, and
J. Jiao, “On optimal caching and model multiplexing for large
model inference,” ArXiv , vol. abs/2306.02003, 2023. [Online].
Available: https://api.semanticscholar.org/CorpusID:259075212
[287] M. Yue, J. Zhao, M. Zhang, L. Du, and Z. Yao, “Large language
model cascades with mixture of thoughts representations
for cost-efficient reasoning,” ArXiv , vol. abs/2310.03094, 2023.
[Online]. Available: https://api.semanticscholar.org/CorpusID:
263671564
[288] D. Patel and G. Wong, “Gpt-4 architecture, infrastructure, train-
ing dataset, costs, vision, moe,” 2023, https://www.semianalysis.
com/p/gpt-4-architecture-infrastructure.
[289] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,
C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand
et al. , “Mixtral of experts,” arXiv preprint arXiv:2401.04088 , 2024.
[290] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi,
A. A. Awan, J. Rasley, and Y. He, “Deepspeed-moe: Advanc-
ing mixture-of-experts inference and training to power next-
generation ai scale,” in International Conference on Machine Learn-
ing. PMLR, 2022, pp. 18 332–18 346.
[291] D. Dai, C. Deng, C. Zhao, R. X. Xu, H. Gao, D. Chen,
J. Li, W. Zeng, X. Yu, Y. Wu, Z. Xie, Y. K. Li, P . Huang,
F. Luo, C. Ruan, Z. Sui, and W. Liang, “Deepseekmoe: Towards
ultimate expert specialization in mixture-of-experts language
models,” ArXiv , vol. abs/2401.06066, 2024. [Online]. Available:
https://api.semanticscholar.org/CorpusID:266933338
[292] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,
and J. Dean, “Outrageously large neural networks: The sparsely-
gated mixture-of-experts layer,” arXiv preprint arXiv:1701.06538 ,
2017.
[293] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang,
M. Krikun, N. Shazeer, and Z. Chen, “Gshard: Scaling giant
models with conditional computation and automatic sharding,”
arXiv preprint arXiv:2006.16668 , 2020.
[294] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling
to trillion parameter models with simple and efficient sparsity,”
The Journal of Machine Learning Research , vol. 23, no. 1, pp. 5232–
5270, 2022.
[295] Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V . Zhao, A. M.
Dai, Q. V . Le, J. Laudon et al. , “Mixture-of-experts with expert
choice routing,” Advances in Neural Information Processing Systems ,
vol. 35, pp. 7103–7114, 2022.
[296] A. Yang, J. Lin, R. Men, C. Zhou, L. Jiang, X. Jia, A. Wang,
J. Zhang, J. Wang, Y. Li et al. , “M6-t: Exploring sparse expert
models and beyond,” arXiv preprint arXiv:2105.15082 , 2021.
[297] Y. Zhou, N. Du, Y. Huang, D. Peng, C. Lan, D. Huang, S. Shakeri,
D. So, A. M. Dai, Y. Lu et al. , “Brainformers: Trading simplicity
for efficiency,” in International Conference on Machine Learning .
PMLR, 2023, pp. 42 531–42 542.
[298] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton,
“Adaptive mixtures of local experts,” Neural computation , vol. 3,
no. 1, pp. 79–87, 1991.
[299] M. I. Jordan and R. A. Jacobs, “Hierarchical mixtures of experts
and the em algorithm,” Neural computation , vol. 6, no. 2, pp. 181–
214, 1994.
[300] A. Graves and A. Graves, “Long short-term memory,” Supervised
sequence labelling with recurrent neural networks , pp. 37–45, 2012.
[301] Z. Chi, L. Dong, S. Huang, D. Dai, S. Ma, B. Patra, S. Singhal,
P . Bajaj, X. Song, X.-L. Mao et al. , “On the representation collapse
of sparse mixture of experts,” Advances in Neural Information
Processing Systems , vol. 35, pp. 34 600–34 613, 2022.[302] Y. Xie, S. Huang, T. Chen, and F. Wei, “Moec: Mixture of expert
clusters,” in Proceedings of the AAAI Conference on Artificial Intelli-
gence , vol. 37, no. 11, 2023, pp. 13 807–13 815.
[303] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer,
“Base layers: Simplifying training of large, sparse models,” in
International Conference on Machine Learning . PMLR, 2021, pp.
6265–6274.
[304] A. Clark, D. De Las Casas, A. Guy, A. Mensch, M. Paganini,
J. Hoffmann, B. Damoc, B. Hechtman, T. Cai, S. Borgeaud et al. ,
“Unified scaling laws for routed language models,” in Interna-
tional Conference on Machine Learning . PMLR, 2022, pp. 4057–
4086.
[305] S. Roller, S. Sukhbaatar, J. Weston et al. , “Hash layers for large
sparse models,” Advances in Neural Information Processing Systems ,
vol. 34, pp. 17 555–17 566, 2021.
[306] C. N. dos Santos, J. Lee-Thorp, I. Noble, C.-C. Chang, and
D. Uthus, “Memory augmented language models through
mixture of word experts,” ArXiv , vol. abs/2311.10768, 2023.
[Online]. Available: https://api.semanticscholar.org/CorpusID:
265295488
[307] X. Ren, P . Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P . Li,
X. Zhang, A. V . Podolskiy, G. Arshinov, A. Bout, I. Piontkovskaya,
J. Wei, X. Jiang, T. Su, Q. Liu, and J. Yao, “Pangu-
σ: Towards trillion parameter language model with sparse
heterogeneous computing,” ArXiv , vol. abs/2303.10845, 2023.
[Online]. Available: https://api.semanticscholar.org/CorpusID:
257666647
[308] J. Li, Z. Sun, X. He, L. Zeng, Y. Lin, E. Li, B. Zheng,
R. Zhao, and X. Chen, “Locmoe: A low-overhead moe for
large language model training,” 2024. [Online]. Available:
https://api.semanticscholar.org/CorpusID:267212059
[309] S. Zuo, X. Liu, J. Jiao, Y. J. Kim, H. Hassan, R. Zhang, T. Zhao, and
J. Gao, “Taming sparsely activated transformer with stochastic
experts,” arXiv preprint arXiv:2110.04260 , 2021.
[310] Y. J. Kim, A. A. Awan, A. Muzio, A. F. C. Salinas, L. Lu, A. Hendy,
S. Rajbhandari, Y. He, and H. H. Awadalla, “Scalable and efficient
moe training for multitask multilingual models,” arXiv preprint
arXiv:2109.10465 , 2021.
[311] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton,
A. Susano Pinto, D. Keysers, and N. Houlsby, “Scaling vision
with sparse mixture of experts,” Advances in Neural Information
Processing Systems , vol. 34, pp. 8583–8595, 2021.
[312] B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer,
and W. Fedus, “St-moe: Designing stable and transferable sparse
expert models,” arXiv preprint arXiv:2202.08906 , 2022.
[313] H. Hazimeh, Z. Zhao, A. Chowdhery, M. Sathiamoorthy, Y. Chen,
R. Mazumder, L. Hong, and E. Chi, “Dselect-k: Differentiable
selection in the mixture of experts with applications to multi-
task learning,” Advances in Neural Information Processing Systems ,
vol. 34, pp. 29 335–29 347, 2021.
[314] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu,
M. Krikun, Y. Zhou, A. W. Yu, O. Firat et al. , “Glam: Efficient scal-
ing of language models with mixture-of-experts,” in International
Conference on Machine Learning . PMLR, 2022, pp. 5547–5569.
[315] M. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer,
X. V . Lin, J. Du, S. Iyer, R. Pasunuru et al. , “Efficient large
scale language modeling with mixtures of experts,” arXiv preprint
arXiv:2112.10684 , 2021.
[316] D. Dai, L. Dong, S. Ma, B. Zheng, Z. Sui, B. Chang, and F. Wei,
“Stablemoe: Stable routing strategy for mixture of experts,” arXiv
preprint arXiv:2204.08396 , 2022.
[317] M. R. Costa-juss `a, J. Cross, O. C ¸ elebi, M. Elbayad, K. Heafield,
K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard et al. , “No
language left behind: Scaling human-centered machine transla-
tion,” arXiv preprint arXiv:2207.04672 , 2022.
[318] M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for
convolutional neural networks,” in International conference on
machine learning . PMLR, 2019, pp. 6105–6114.
[319] O. Press, N. A. Smith, and O. Levy, “Improving trans-
former models by reordering their sublayers,” arXiv preprint
arXiv:1911.03864 , 2019.
[320] D. So, W. Ma ´nke, H. Liu, Z. Dai, N. Shazeer, and Q. V . Le,
“Searching for efficient transformers for language modeling,”
Advances in Neural Information Processing Systems , vol. 34, pp.
6010–6022, 2021.
[321] B. Zoph and Q. V . Le, “Neural architecture search with reinforce-
ment learning,” arXiv preprint arXiv:1611.01578 , 2016.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 45
[322] X. Nie, X. Miao, S. Cao, L. Ma, Q. Liu, J. Xue, Y. Miao, Y. Liu,
Z. Yang, and B. Cui, “Evomoe: An evolutional mixture-of-experts
training framework via dense-to-sparse gate,” arXiv preprint
arXiv:2112.14397 , 2021.
[323] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov, “Dropout: a simple way to prevent neural net-
works from overfitting,” The journal of machine learning research ,
vol. 15, no. 1, pp. 1929–1958, 2014.
[324] R. Liu, Y. J. Kim, A. Muzio, and H. Hassan, “Gating dropout:
Communication-efficient regularization for sparsely activated
transformers,” in International Conference on Machine Learning .
PMLR, 2022, pp. 13 782–13 792.
[325] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi,
J. Guan, P . Ke et al. , “Cpm-2: Large-scale cost-effective pre-trained
language models,” AI Open , vol. 2, pp. 216–224, 2021.
[326] F. Xue, Z. Zheng, Y. Fu, J. Ni, Z. Zheng, W. Zhou, and Y. You,
“Openmoe: An early effort on open mixture-of-experts language
models,” 2024.
[327] T. Chen, S. Huang, Y. Xie, B. Jiao, D. Jiang, H. Zhou, J. Li,
and F. Wei, “Task-specific expert pruning for sparse mixture-of-
experts,” arXiv preprint arXiv:2206.00277 , 2022.
[328] Z.-F. Gao, P . Liu, W. X. Zhao, Z.-Y. Lu, and J.-R. Wen, “Parameter-
efficient mixture-of-experts architecture for pre-trained language
models,” arXiv preprint arXiv:2203.01104 , 2022.
[329] S. Zuo, Q. Zhang, C. Liang, P . He, T. Zhao, and W. Chen,
“Moebert: from bert to mixture-of-experts via importance-guided
adaptation,” arXiv preprint arXiv:2204.07675 , 2022.
[330] P . Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, “Impor-
tance estimation for neural network pruning,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition ,
2019, pp. 11 264–11 272.
[331] Z. Zhang, Y. Lin, Z. Liu, P . Li, M. Sun, and J. Zhou, “Moefication:
Transformer feed-forward layers are mixtures of experts,” arXiv
preprint arXiv:2110.01786 , 2021.
[332] R. Csord’as, K. Irie, and J. Schmidhuber, “Approximating
two-layer feedforward networks for efficient transformers,”
ArXiv , vol. abs/2310.10837, 2023. [Online]. Available: https:
//api.semanticscholar.org/CorpusID:264172384
[333] R. Csord’as, P . Piekos, K. Irie, and J. Schmidhuber, “Switchhead:
Accelerating transformers with mixture-of-experts attention,”
ArXiv , vol. abs/2312.07987, 2023. [Online]. Available: https:
//api.semanticscholar.org/CorpusID:266191825
[334] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai,
T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit et al. ,
“Mlp-mixer: An all-mlp architecture for vision,” Advances in
neural information processing systems , vol. 34, pp. 24 261–24 272,
2021.
[335] J. Lee-Thorp and J. Ainslie, “Sparse mixers: Combining moe
and mixing to build a more efficient bert,” arXiv preprint
arXiv:2205.12399 , 2022.
[336] P . Yu, M. Artetxe, M. Ott, S. Shleifer, H. Gong, V . Stoyanov, and
X. Li, “Efficient language modeling with sparse all-mlp,” arXiv
preprint arXiv:2203.06850 , 2022.
[337] Y. Wang, S. Agarwal, S. Mukherjee, X. Liu, J. Gao, A. H. Awadal-
lah, and J. Gao, “Adamix: Mixture-of-adaptations for parameter-
efficient model tuning,” arXiv preprint arXiv:2210.17451 , 2022.
[338] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Larous-
silhe, A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-
efficient transfer learning for nlp,” in International Conference on
Machine Learning . PMLR, 2019, pp. 2790–2799.
[339] E. J. Hu, Y. Shen, P . Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
and W. Chen, “Lora: Low-rank adaptation of large language
models,” arXiv preprint arXiv:2106.09685 , 2021.
[340] S. Diao, T. Xu, R. Xu, J. Wang, and T. Zhang, “Mixture-of-
domain-adapters: Decoupling and injecting domain knowledge
to pre-trained language models’ memories,” in Annual Meeting
of the Association for Computational Linguistics , 2023. [Online].
Available: https://api.semanticscholar.org/CorpusID:259108831
[341] R. Li, G. Murray, and G. Carenini, “Mixture-of-linguistic-
experts adapters for improving and interpreting pre-trained
language models,” in Conference on Empirical Methods in
Natural Language Processing , 2023. [Online]. Available: https:
//api.semanticscholar.org/CorpusID:264487239
[342] Y. Zhu, N. Wichers, C.-C. Lin, X. Wang, T. Chen, L. Shu, H. Lu,
C. Liu, L. Luo, J. Chen, and L. Meng, “Sira: Sparse mixture of
low rank adaptation,” ArXiv , vol. abs/2311.09179, 2023. [Online].
Available: https://api.semanticscholar.org/CorpusID:265213347[343] S. Dou, E. Zhou, Y. Liu, S. Gao, J. Zhao, W. Shen, Y. Zhou,
Z. Xi, X. Wang, X. Fan, S. Pu, J. Zhu, R. Zheng, T. Gui,
Q. Zhang, and X. Huang, “Loramoe: Revolutionizing mixture
of experts for maintaining world knowledge in language
model alignment,” ArXiv , vol. abs/2312.09979, 2023. [Online].
Available: https://api.semanticscholar.org/CorpusID:266335873
[344] Y. Gui, X. Yan, P . Yin, H. Yang, and J. Cheng, “Spt:
Fine-tuning transformer-based language models efficiently with
sparsification,” ArXiv , vol. abs/2312.10365, 2023. [Online].
Available: https://api.semanticscholar.org/CorpusID:266348310
[345] W. Niu, J. Guan, Y. Wang, G. Agrawal, and B. Ren, “Dnnfusion:
accelerating deep neural networks execution with advanced op-
erator fusion,” in Proceedings of the 42nd ACM SIGPLAN Interna-
tional Conference on Programming Language Design and Implementa-
tion, 2021, pp. 883–898.
[346] R. Y. Aminabadi, S. Rajbhandari, A. A. Awan, C. Li, D. Li,
E. Zheng, O. Ruwase, S. Smith, M. Zhang, J. Rasley et al. ,
“Deepspeed-inference: enabling efficient inference of transformer
models at unprecedented scale,” in SC22: International Conference
for High Performance Computing, Networking, Storage and Analysis .
IEEE, 2022, pp. 1–15.
[347] J. Fang, Y. Yu, C. Zhao, and J. Zhou, “Turbotransformers: an effi-
cient gpu serving system for transformer models,” in Proceedings
of the 26th ACM SIGPLAN Symposium on Principles and Practice of
Parallel Programming , 2021, pp. 389–402.
[348] Y. Zhai, C. Jiang, L. Wang, X. Jia, S. Zhang, Z. Chen, X. Liu,
and Y. Zhu, “Bytetransformer: A high-performance transformer
boosted for variable-length inputs,” in 2023 IEEE International
Parallel and Distributed Processing Symposium (IPDPS) . IEEE,
2023, pp. 344–355.
[349] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P . Liang,
C. R ´e, I. Stoica, and C. Zhang, “Flexgen: High-throughput gen-
erative inference of large language models with a single gpu,”
inInternational Conference on Machine Learning . PMLR, 2023, pp.
31 094–31 116.
[350] Y. Song, Z. Mi, H. Xie, and H. Chen, “Powerinfer: Fast large
language model serving with a consumer-grade gpu,” arXiv
preprint arXiv:2312.12456 , 2023.
[351] L. Zheng, Z. Li, H. Zhang, Y. Zhuang, Z. Chen, Y. Huang,
Y. Wang, Y. Xu, D. Zhuo, E. P . Xing et al. , “Alpa: Automating inter-
and{Intra-Operator }parallelism for distributed deep learning,”
in16th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 22) , 2022, pp. 559–578.
[352] M. Shoeybi, M. Patwary, R. Puri, P . LeGresley, J. Casper, and
B. Catanzaro, “Megatron-lm: Training multi-billion parame-
ter language models using model parallelism,” arXiv preprint
arXiv:1909.08053 , 2019.
[353] S. Li, H. Liu, Z. Bian, J. Fang, H. Huang, Y. Liu, B. Wang, and
Y. You, “Colossal-ai: A unified deep learning system for large-
scale parallel training,” in Proceedings of the 52nd International
Conference on Parallel Processing , 2023, pp. 766–775.
[354] M. Baines, S. Bhosale, V . Caggiano, N. Goyal, S. Goyal,
M. Ott, B. Lefaudeux, V . Liptchinsky, M. Rabbat, S. Sheif-
feret al. , “Fairscale: A general purpose modular pytorch li-
brary for high performance and large scale training,” 2021,
https://github.com/facebookresearch/fairscale.
[355] G. Lai, “Pax: A jax-based machine learning framework for large
scale models.” https://github.com/google/paxml.
[356] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, “Deepspeed:
System optimizations enable training deep learning models with
over 100 billion parameters,” in Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data
Mining , 2020, pp. 3505–3506.
[357] T. M. M. Team, “composer,” https://github.com/mosaicml/
composer/, 2021.
[358] A. Pham, C. Yang, S. Sheng, S. Zhao, S. Lee, B. Jiang, F. Dong,
X. Guan, and F. Ming, “Openllm: Operating llms in production,”
2023, https://github.com/bentoml/OpenLLM.
[359] T. A. B. Team, “Rayllm,” https://github.com/ray-project/ray-
llm.
[360] M. team, “MLC-LLM,” 2023. [Online]. Available: https:
//github.com/mlc-ai/mlc-llm
[361] T. W. J. Team, “Saxml,” https://github.com/google/saxml.
[362] K. Yang, Z. Liu, and P . Cheng, “MOSEC: Model Serving
made Efficient in the Cloud,” 2021. [Online]. Available:
https://github.com/mosecorg/mosec
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 46
[363] T. D. K. Team, “Llm foundry,”
https://github.com/mosaicml/llm-foundry.
[364] TensorFlow, “Tensorflow xla,” https://www.tensorflow.org/xla.
[365] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen,
M. Cowan, L. Wang, Y. Hu, L. Ceze et al. , “{TVM}: An automated
{End-to-End }optimizing compiler for deep learning,” in 13th
USENIX Symposium on Operating Systems Design and Implementa-
tion (OSDI 18) , 2018, pp. 578–594.
[366] X. Jiang, H. Wang, Y. Chen, Z. Wu, L. Wang, B. Zou, Y. Yang,
Z. Cui, Y. Cai, T. Yu et al. , “Mnn: A universal and efficient
inference engine,” Proceedings of Machine Learning and Systems ,
vol. 2, pp. 1–13, 2020.
[367] Pytorch, “Pytorch jit,” https://github.com/pytorch/torchdynamo.
[368] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Larous-
silhe, A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-
efficient transfer learning for nlp,” in International Conference on
Machine Learning . PMLR, 2019, pp. 2790–2799.
[369] A. R ¨uckl ´e, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers,
and I. Gurevych, “Adapterdrop: On the efficiency of adapters in
transformers,” arXiv preprint arXiv:2010.11918 , 2020.
[370] J. Pfeiffer, A. Kamath, A. R ¨uckl ´e, K. Cho, and I. Gurevych,
“Adapterfusion: Non-destructive task composition for transfer
learning,” arXiv preprint arXiv:2005.00247 , 2020.
[371] S. He, L. Ding, D. Dong, M. Zhang, and D. Tao, “Sparseadapter:
An easy approach for improving the parameter-efficiency of
adapters,” arXiv preprint arXiv:2210.04284 , 2022.
[372] X. L. Li and P . Liang, “Prefix-tuning: Optimizing continuous
prompts for generation,” arXiv preprint arXiv:2101.00190 , 2021.
[373] E. B. Zaken, S. Ravfogel, and Y. Goldberg, “Bitfit: Simple
parameter-efficient fine-tuning for transformer-based masked
language-models,” arXiv preprint arXiv:2106.10199 , 2021.
[374] E. J. Hu, Y. Shen, P . Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
and W. Chen, “Lora: Low-rank adaptation of large language
models,” arXiv preprint arXiv:2106.09685 , 2021.
[375] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer,
“Qlora: Efficient finetuning of quantized llms,” arXiv preprint
arXiv:2305.14314 , 2023.
[376] Y. Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen,
X. Zhang, and Q. Tian, “Qa-lora: Quantization-aware low-
rank adaptation of large language models,” arXiv preprint
arXiv:2309.14717 , 2023.
[377] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi,
“Dylora: Parameter efficient tuning of pre-trained models us-
ing dynamic search-free low-rank adaptation,” arXiv preprint
arXiv:2210.07558 , 2022.
[378] Q. Zhang, M. Chen, A. Bukharin, P . He, Y. Cheng, W. Chen, and
T. Zhao, “Adaptive budget allocation for parameter-efficient fine-
tuning,” arXiv preprint arXiv:2303.10512 , 2023.
[379] R. Karimi Mahabadi, J. Henderson, and S. Ruder, “Compacter:
Efficient low-rank hypercomplex adapter layers,” Advances in
Neural Information Processing Systems , vol. 34, pp. 1022–1035, 2021.
[380] V . Lialin, V . Deshpande, and A. Rumshisky, “Scaling down
to scale up: A guide to parameter-efficient fine-tuning,” arXiv
preprint arXiv:2303.15647 , 2023.
APPENDIX A
PARAMETER -EFFICIENT FINETUNING (PEFT)
When training a large language model for specific tasks,
it is often faster and more efficient to fine-tune a pre-
trained model. There are two types of fine-tuning methods:
full fine-tuning and parameter-efficient fine-tuning (PEFT).
However, full fine-tuning can be expensive and may cause
catastrophic forgetfulness problems. To address these issues,
PEFT was developed. PEFT methods can be categorized into
three categories: additive, selective, and reparameterization.
A.1 Additive methods
The main idea behind additive methods is to add additional
parameters to the model while keeping the original param-
eters fixed and train the model by fine-tuning the additional
parameters [368], [369], [370], [371], [372].
Multi-head 
attentionAdapter
Feed-forward 
layerLayer Norm
+2x Feed-forward 
layerAdapter+Layer Norm
Transformer 
Layer
Feedforward
down-project+ Adapter
LayerNonlinearityFeedforward
up-projectFig. 10: Architecture of the adapter module and its integra-
tion with the Transformer.
The Adapter [368] is the pioneering work of the additive
methods. As shown in Fig. 10, the Adapter fine-tunes the
model by inserting an adapter module between the feed-
forward layer and skip-connection within the Transformer
layer. The Adapter module is a small fully-connected net-
work consisting of an upper projection layer, a nonlinear
layer, a lower projection layer, and an additional skip-
connection. Whenever a new downstream task arises, we
can mitigate the problem of full fine-tuning and catastrophic
forgetting by adding an adapter module to the model to
produce an easily scalable downstream model. The adapter
idea has been widely used and many adapter variants have
been proposed.
There are two main strategies for integrating multi-task
knowledge: sequential fine-tuning and multi-task learning.
However, both strategies have specific problems. Sequential
fine-tuning requires prior knowledge to determine the order
of tasks which can result in the model forgetting knowledge
learned from previous tasks. On the other hand, multi-task
learning makes balancing data from various tasks challeng-
ing as different tasks can interfere with each other. As a re-
sult, both methods face challenges in effectively transferring
knowledge. [370]proposes a variant called AdapterFusion,
which effectively mitigates the above problems in multi-
task training using a two-stage learning algorithm, achieves
effective knowledge sharing across multiple tasks, and out-
performs fully fine-tuned models on a single target task.
Typically, fine-tuning a Transformer with the adapter
is 60 %faster than full fine-tuning in training but 4-6 %
slower in inference. The AdapterDrop method proposed by
[369] can efficiently and dynamically remove adapters with
minimal impact on task performance. This dramatically
improves the model’s efficiency during backpropagation
(training) and forward propagation (inference).
Existing methods increase the bottleneck dimension to
match the performance of full fine-tuning as much as possi-
ble because the number of trainable parameters determines
the adapter’s capacity. However, these methods increase the
overall parameter count and FLOPs, violating the adapter’s
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 47
original intention. [371] combines the adapter with pruning
to propose SparseAdapter, using a frustratedly easy setting
called Large-Sparse, which effectively improves the capacity
of the adapter under a given parameter budget and can also
be effectively applied to other types of adapter methods,
such as LoRA.
Another similar class of additive methods is Soft
Prompts, of which the most representative work is Prefix-
Tuning [372], which effectively avoids data cross-pollination
and dramatically reduces the number of parameters by
adding a continuous, independent, learnable, task-specific
prefix to the input. At the same time, it can easily
switch tasks and realize processing examples from multiple
users/tasks in one batch.
Overall, while additive methods add additional param-
eters to the model and inevitably bring about additional
inference delays, they significantly improve training speed
and memory efficiency by reducing the gradient size and
optimizer state.
A.2 Selective methods
Selective methods select a portion of the parameters on the
original model for fine-tuning and keep the rest frozen, such
as BitFit [373], which selectively fine-tunes only the model’s
bias terms. It is more suitable for cases with less training
data because of the limited expression of the parameters it
tunes.
These methods are relatively more straightforward to
implement. Still, the problem is that we can only deter-
mine which part to tune empirically and experimentally,
which can be inefficient and inaccurate.In addition, since
these methods modify the original model, we also need to
consider the problem of model forgetting.
A.3 Reparameterization methods
The reparameterization methods take advantage of low rank
in the weight matrix of the pre-trained model. Instead of
fine-tuning the entire weight matrix, these methods con-
struct smaller fine-tuned modules by reparameterization
[374], [375], [376], [377], [378], [379].
The most representative work in this area is LoRA [374],
which approximates the entire weight matrix by fine-tuning
the two-rank decomposition matrices instead of fine-tuning
them. In this case, the rank rof the decomposition matrices
is a hyperparameter; the larger ris, the more parameters
need to be fine-tuned and the larger the capacity of the
decomposition matrices, and vice versa. Due to the low-
rank nature of the pre-trained model’s weight matrix, it is
generally sufficient for us to set r relatively small, better
mitigating the forgetting problem. In addition, when rea-
soning, we can merge the decomposition matrix with the
weight matrix to avoid introducing inference delays. When
switching tasks, we must subtract the original parts and add
new ones.
There are two problems with LoRA. The first is that
its size is fixed, which means that once we have set the
rank and trained on it, it cannot be modified anymore. The
second is that it is difficult to find the optimal rank unless we
perform a search, which is expensive. [377] has developed
a new algorithm called DyLoRA based on LoRA. DyLorahas developed a unique algorithm called DyLoRA based
on LoRA. DyLora has developed a new algorithm called
DyLoRA based on LoRA. DyLora has developed a new
algorithm called DyLoRA based on LoRA. It dynamically
searches for the optimal rank through LoRA selects it, and
performs dynamic inference without incurring additional
costs.
Another related work is AdaLoRA [378], which adap-
tively assigns parameter budgets based on importance
scores. In AdaLoRA, the incremental update of the weight
matrix is parameterized in the form of a singular value
decomposition. The parameter budget is dynamically al-
located among the total matrices by manipulating the dis-
tinct values according to the new importance metrics. This
method effectively improves model performance and pa-
rameter efficiency.
LoRA has another advantage: it can be naturally orthog-
onalized to other model compression acceleration meth-
ods. As mentioned earlier, QLoRA [375], which combines
LoRA with quantization, dramatically reduces the mem-
ory footprint during training, realizing the advancement
of fine-tuning 65B parametric models on a single 48GB
memory GPU, making it possible for more researchers to
participate in the study of large models. However, QLoRA
only considers resources at training, not inference. Although
QLoRA quantizes the model during the training process,
since the LoRA parameters for training are of FP16 type,
when reasoning, the quantized model is fused with the
LoRA parameters, the quantization will be destroyed and
go back to the unquantized form, and then if you want to
reason efficiently, you have to perform another step of PTQ
(Post-training quantization). PTQ will bring extra errors and
affect the performance of the model. Recently, QALoRA
[376] solved the problem by introducing group-wise oper-
ators, which realize one-click low-precision deployment of
models.
A.4 Hybrid methods
Combining the previous methods is a natural idea to get bet-
ter results. Compacter [379] combines the ideas of Adapter
and reparameterization to balance the number of trainable
parameters, task performance, and memory footprint. The
above is a brief overview of representative work on PEFT.
For more detailed information, see [380].
PEFT methods offer an effective way to train models, but
it’s still difficult to deploy trained models on edge devices
like cell phones. We can tweak the models before compres-
sion, which may affect their performance. Therefore, it’s
important to explore new PEFT methods and find ways
to combine them with other compression acceleration tech-
niques to compress models while minimizing performance
loss in downstream tasks. This remains a valuable area of
research.
