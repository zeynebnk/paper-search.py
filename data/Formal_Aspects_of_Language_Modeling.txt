arXiv:2311.04329v2  [cs.CL]  17 Apr 2024

Formal Aspects
of
Language Modeling
Ryan Cotterell, Anej Svete, Clara Meister,
Tianyu Liu, and Li Du
Thursday 18thApril, 2024

Contents
1 Introduction 5
1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2 Probabilistic Foundations 7
2.1 An Invitation to Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2 A Measure-theoretic Foundation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.3 Language Models: Distributions over Strings . . . . . . . . . . . . . . . . . . . . . . 14
2.3.1 Sets of Strings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.3.2 Defining a Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.4 Global and Local Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.4.1 Globally Normalized Language Models . . . . . . . . . . . . . . . . . . . . . . 18
2.4.2 Locally Normalized Language Models . . . . . . . . . . . . . . . . . . . . . . 20
2.5 Tight Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.1 Tightness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.5.2 Defining the probability measure of an LNM . . . . . . . . . . . . . . . . . . 28
2.5.3 Interpreting the Constructed Probability Space . . . . . . . . . . . . . . . . . 35
2.5.4 Characterizing Tightness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3 Modeling Foundations 45
3.1 Representation-based Language Models . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.1.1 Vector Space Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.1.2 Compatibility of Symbol and Context . . . . . . . . . . . . . . . . . . . . . . 52
3.1.3 Projecting onto the Simplex . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.1.4 Representation-based Locally Normalized Models . . . . . . . . . . . . . . . . 58
3.1.5 Tightness of Softmax Representation-based Models . . . . . . . . . . . . . . . 58
3.2 Estimating a Language Model from Data . . . . . . . . . . . . . . . . . . . . . . . . 61
3.2.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3.2.2 Language Modeling Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3.2.3 Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
3.2.4 Regularization Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4 Classical Language Models 75
4.1 Finite-state Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.1.1 Weighted Finite-state Automata . . . . . . . . . . . . . . . . . . . . . . . . . 76
4.1.2 Finite-state Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
3
4 CONTENTS
4.1.3 Normalizing Finite-state Language Models . . . . . . . . . . . . . . . . . . . . 87
4.1.4 Tightness of Finite-state Models . . . . . . . . . . . . . . . . . . . . . . . . . 93
4.1.5 The n-gram Assumption and Subregularity . . . . . . . . . . . . . . . . . . . 97
4.1.6 Representation-based n-gram Models . . . . . . . . . . . . . . . . . . . . . . . 101
4.2 Pushdown Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
4.2.1 Human Language Is not Finite-state . . . . . . . . . . . . . . . . . . . . . . . 107
4.2.2 Context-free Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
4.2.3 Weighted Context-free Grammars . . . . . . . . . . . . . . . . . . . . . . . . . 115
4.2.4 Context-free Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . 118
4.2.5 Tightness of Context-free Language Models . . . . . . . . . . . . . . . . . . . 120
4.2.6 Normalizing Weighted Context-free Grammars . . . . . . . . . . . . . . . . . 124
4.2.7 Pushdown Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
4.2.8 Pushdown Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
4.2.9 Multi-stack Pushdown Automata . . . . . . . . . . . . . . . . . . . . . . . . . 133
4.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
5 Neural Network Language Models 137
5.1 Recurrent Neural Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.1.1 Human Language is Not Context-free . . . . . . . . . . . . . . . . . . . . . . 138
5.1.2 Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
5.1.3 General Results on Tightness . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
5.1.4 Elman and Jordan Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
5.1.5 Variations on Recurrent Networks . . . . . . . . . . . . . . . . . . . . . . . . 152
5.2 Representational Capacity of Recurrent Neural Networks . . . . . . . . . . . . . . . 157
5.2.1 RNNs and Weighted Regular Languages . . . . . . . . . . . . . . . . . . . . 158
5.2.2 Addendum to Minsky’s Construction: Lower Bounds on the Space Complexity
of Simulating PFSAs with RNNs . . . . . . . . . . . . . . . . . . . . . . . . . 172
5.2.3 Lower Bound in the Probabilistic Setting . . . . . . . . . . . . . . . . . . . . 187
5.2.4 Turing Completeness of Recurrent Neural Networks . . . . . . . . . . . . . . 192
5.2.5 The Computational Power of RNN Variants . . . . . . . . . . . . . . . . . . . 204
5.2.6 Consequences of the Turing completeness of recurrent neural networks . . . . 205
5.3 Transformer-based Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
5.3.1 Informal Motivation of the Transformer Architecture . . . . . . . . . . . . . . 208
5.3.2 A Formal Definition of Transformers . . . . . . . . . . . . . . . . . . . . . . . 210
5.3.3 Tightness of Transformer-based Language Models . . . . . . . . . . . . . . . . 222
5.4 Representational Capacity of Transformer Language Models . . . . . . . . . . . . . . 225
Chapter 1
Introduction
1.1 Introduction
Welcome to the class notes for the first third of Large Language Models (263-5354-00L). The course
comprises an omnibus introduction to language modeling. The first third of the lectures focuses on
a formal treatment of the subject. The second part focuses on the practical aspects of implementing
a language model and its applications. Many universities are offering similar courses at the moment,
e.g., CS324 at Stanford University ( https://stanford-cs324.github.io/winter2022/ ) and CS
600.471 ( https://self-supervised.cs.jhu.edu/sp2023/ ) at Johns Hopkins University. Their
syllabi may serve as useful references.
Disclaimer. This is the third time the course is being taught and we are improving the notes
as we go. We will try to be as careful as possible to make them typo- and error-free. However,
there will undoubtedly be mistakes scattered throughout. We will be very grateful if you report
any mistakes you spot, or anything you find unclear and confusing in general—this will benefit the
students as well as the teaching staff by helping us organize a better course!
5
6 CHAPTER 1. INTRODUCTION
Chapter 2
Probabilistic Foundations
2.1 An Invitation to Language Modeling
The first module of the course focuses on defining a language model mathematically. To see why
such a definition is nuanced, we are going to give an informal definition of a language model and
demonstrate two ways in which that definition breaks and fails to meet our desired criteria.
Definition 2.1.1: Language Model (Informal)
Given an alphabetaΣ and a distinguished end-of-sequence symbol eosRΣ, a language model
is a collection of conditional probability distributions ppy|yqforyPΣYteosuandyPΣ˚,
where Σ˚is the set of all strings over the alphabet Σ. The term ppy|yqrepresents the
probability of the symbol yoccurring as the next symbol after the string y.
aAn alphabet is a finite, non-empty set. It is also often referred to as a vocabulary.
Definition 2.1.1 is the definition of a language model that is implicitly assumed in most papers
on language modeling. We say implicitly since most technical papers on language modeling simply
write down the following autoregressive factorization
ppyq“ppy1¨¨¨yTq“ppeos|yqTź
t“1ppyt|yătq (2.1)
as the probability of a string according to the distribution p.1The part that is left implicit in
Eq. (2.1) is whether or not pis indeed a probability distribution and, if it is, over what space.
The natural assumption in Definition 2.1.1 is that pis a distribution over Σ˚, i.e., the set of all
finite strings2over an alphabet Σ. However, in general, it is not true that all such collections of
conditionals will yield a valid probability distribution over Σ˚; some may “leak” probability mass
to infinite sequences.3More subtly, we additionally have to be very careful when dealing with
1Many authors (erroneously) avoid writing eosfor concision.
2Some authors assert that strings are by definition finite.
3However, the converse istrue: All valid distributions over Σ˚may be factorized as the above.
7
8 CHAPTER 2. PROBABILISTIC FOUNDATIONS
uncountably infinite spaces lest we run into a classic paradox. We highlight these two issues with
two very simple examples. The first example is a well-known paradox in probability theory.
Example 2.1.1: Infinite Coin Toss
Consider the infinite independent fair coin toss model, where we aim to place a distribution
overtH,Tu8, the (uncountable) set of infinite sequences of tH,Tu(Hrepresents the event of
throwing heads and Tthe event of throwing tails). Intuitively, such a distribution corresponds
to a “language model” as defined above in which for all yăt,ppH|yătq“ppT|yătq“1
2and
ppeos|yătq“0. However, each individual infinite sequence over tH,Tushould also be assigned
probabilityp1
2q8“0. Without a formal foundation, one arrives at the following paradox:
1“pptH,Tu8q
“p¨
˝ď
ωPtH,Tu8tωu˛
‚
“ÿ
ωPtH,Tu8pptωuq
“ÿ
ωPtH,Tu80?“0.
The second example is more specific to language modeling. As we stated above, an implicit
assumption made by most language modeling papers is that a language model constitutes a
distribution over Σ˚. However, in our next example, we show that a collection of conditions that
satisfy Definition 2.1.1 may not sum to 1 if the sum is restricted to elements of Σ˚. This means
that it is not a priori clear what space our probability distribution is defined over.4
0{11
2{1
2H{1
2
T{1
2H{1
T{1
2
Figure 2.1: Graphical depiction of the possibly finite coin toss model. The final weight1
2of the
state 2 corresponds to the probability ppeos|yt´1“Tq“1
2.
4This also holds for the first example.
2.1. AN INVITATION TO LANGUAGE MODELING 9
Example 2.1.2: Possibly Finite Coin Toss
Consider now the possibly finite “coin toss” model with a rather peculiar coin: when tossing
the coin for the first time, both HandTare equally likely. After the first toss, however, the
coin gets stuck: If y1“H, we can only ever toss another Hagain, whereas if y1“T, the next
toss can result in another Tor “end” the sequence of throws ( eos) with equal probability. We,
therefore, model a probability distribution over tH,Tu˚YtH,Tu8, the set of finite and infinite
sequences of tosses. Formally:a
ppH|yă1q“ppT|yă1q“1
2
ppH|yătq“#
1 iftą1 andyt´1“H
0 iftą1 andyt´1“T
ppT|yătq“#
1
2iftą1 andyt´1“T
0 iftą1 andyt´1“H
ppeos|yătq“#
1
2iftą1 andyt´1“T
0 otherwise.
If you are familiar with (probabilistic) finite-state automata,byou can imagine the model
as depicted in Fig. 2.1. It is easy to see that this model only places the probability of1
2on
finite sequences of tosses. If we were only interested in those (analogously to how we are only
interested in finite strings when modeling language), yet still allowed the model to specify the
probabilities as in this example, the resulting probability distribution would not model what
we require.
aNote thatppH|yă1q“ppH|εqandppT|yă1q“ppT|εq.
bThey will be formally introduced in §4.1.5
It takes some mathematical heft to define a language model in a manner that avoids such
paradoxes. The tool of choice for mathematicians is measure theory, as it allows us to define
probability over uncountable sets5in a principled way. Thus, we begin our formal treatment of
language modeling with a primer of measure theory in §2.2. Then, we will use concepts discussed in
the primer to work up to a formal definition of a language model.
5As stated earlier, tH,Tu8is uncountable. It’s easy to see there exists a surjection from tH,Tu8to the binary
expansion of the real interval p0,1s. Readers who are interested in more details and mathematical implications can
refer to §1 in Billingsley (1995).
10 CHAPTER 2. PROBABILISTIC FOUNDATIONS
2.2 A Measure-theoretic Foundation
At their core, (large) language models are an attempt to place a probabilistic distribution over
natural language utterances. However, our toy examples in Examples 2.1.1 and 2.1.2 in the previous
section reveal that it can be relatively tricky to get a satisfying definition of a language model. Thus,
our first step forward is to review the basics of rigorous probability theory,6the tools we need to
come to a satisfying definition. Our course will assume that you have had some exposure to rigorous
probability theory before, and just review the basics. However, it is also possible to learn the basics
of rigorous probability on the fly during the course if it is new to you. Specifically, we will cover
measure-theoretic foundations of probability theory. This might come as a bit of a surprise since
we are mostly going to be talking about language , which is made up of discrete objects—strings.
However, as we will see in §2.5 soon, formal treatment of language modeling indeed requires some
mathematical rigor from measure theory.
The goal of measure-theoretic probability is to assign probabilities to subsets of an outcome
space Ω. However, in the course of the study of measure theory, it has become clear that for
many common Ω, it is impossible to assign probabilities in a way that satisfies a set of reasonable
desiderata.7Consequently, the standard approach to probability theory resorts to only assigning
probability to certain “nice” (but not necessarily all) subsets of Ω, which are referred to as events or
measurable subsets , as in the theory of integration or functional analysis. The set of measurable
subsets is commonly denoted as F(Definition 2.2.1) and a probability measure P:FÑr0,1sis the
function that assigns a probability to each measurable subset. The triple pΩ,F,Pqis collectively
known as a probability space (Definition 2.2.2). As it turns out, the following simple and reasonable
requirements imposed on FandPare enough to rigorously discuss probability.
Definition 2.2.1: σ-algebra
LetPpΩqbe the power set of Ω. Then FĎPpΩqis called aσ-algebra (orσ-field) over Ω if
the following conditions hold:
1) ΩPF,
2) ifEPF, then EcPF,
3) ifE1,E2,... is a finite or infinite sequence of sets in F, thenŤ
nEnPF.
IfFis aσ-algebra over Ω, we call the tuple pΩ,Fqameasurable space .
Example 2.2.1: σ-algebras
Let Ω be any set. Importantly, there is more than one way to construct a σ-algebra over Ω:
1.The family consisting of only the empty set Hand the set Ω, i.e., Fdef“tH,Ωu, is called
theminimal ortrivialσ-algebra.
2. The full power set Fdef“PpΩqis called the discreteσ-algebra .
6By rigorous probability theory we mean a measure-theoretic treatment of probability theory.
7Measure theory texts commonly discuss such desiderata and the dilemma that comes with it. See, e.g., Chapter 7
in Tao (2016), Chapter 3 in Royden (1988) or Chapter 3 in Billingsley (1995). We also give an example later.
2.2. A MEASURE-THEORETIC FOUNDATION 11
3. Given AĎΩ, the family Fdef“tH,A,ΩzA,Ωuis aσ-algebra induced by A.
4.Suppose we are rolling a six-sided die. There are six events that can happen: We can
roll any of the numbers 1 –6. In this case, we will then define the set of outcomes Ω as
Ωdef“ tThe number observed is n|n“1,..., 6u. There are of course multiple ways to
define an event space Fand with it a σ-algebra over this outcome space. By definition,
HPFand ΩPF. One way to intuitively construct a σ-algebra is to consider that
all individual events (observing any number) are possible, meaning that we would like
to later assign probabilities to them (see Definition 2.2.2). This means that we should
include individual singleton events in the event space: tThe number observed is nuPF
forn“1,..., 6. It is easy to see that in this case, to satisfy the axioms in Definition 2.2.1,
the resulting event space should be F“PpΩq.
You might want to confirm these are indeed σ-algebra s by checking them against the axioms
in Definition 2.2.1.
A measurable space guarantees that operations on countably many sets are always valid, and
hence permits the following definition.
Definition 2.2.2: Probability measure
Aprobability measure Pover a measurable space pΩ,Fqis a function P:FÑr0,1ssuch
that
1)PpΩq“1,
2) ifE1,E2,... is a countable sequence of disjoint sets in F, then PpŤ
nEnq“ř
nPpEnq.
In this case we call pΩ,F,Pqaprobability space .
As mentioned, measure-theoretic probability only assigns probabilities to “nice” subsets of Ω. In
fact, it is often impossible to assign a probability measure to every single subset of Ω and we must
restrict our probability space to a strict subset of PpΩq. More precisely, the sets BĎΩ for which a
probability (or more generally, a volume ) can not be defined are called non-measurable sets . An
example of such sets is the Vitali set.8See also Appendix A.2 in Durrett (2019).
Later, we will be interested in modeling probability spaces over sets of (infinite) sequences. By
virtue of a theorem due to Carath´ eodory, there is a natural way to construct such a probability
space for sequences (and many other spaces) that behaves in accordance with our intuition, as we
will clarify later. Here, we shall lay out a few other necessary definitions.
Definition 2.2.3: Algebra
AĎPpΩqis called an algebra (or field) over Ω if
1) ΩPA,
2) ifEPA, then EcPA,
8Seehttps://en.wikipedia.org/wiki/Non-measurable_set and https://en.wikipedia.org/wiki/Vitali_set .
12 CHAPTER 2. PROBABILISTIC FOUNDATIONS
3) ifE1,E2PA, then E1YE2PA.
Definition 2.2.4: Probability pre-measure
LetAbe an algebra over some set Ω. A probability pre-measure overpΩ,Aqis a function
P0:AÑr0,1ssuch that
1)P0pΩq“1,
2)ifE1,E2,... is a (countable) sequence of disjoint sets in Awhose (countable) union is
also in A, then P0pY8
n“1Enq“ř8
n“1P0pEnq.
Note that the only difference between a σ-algebra (Definition 2.2.1) and an algebra is that
condition 3 is weakened from countable to finite, and the only difference between a probability
measure (Definition 2.2.2) and a pre-measure is that the latter is defined with respect to an algebra
instead of a σ-algebra.
The idea behind Carath´ eodory’s extension theorem is that there is often a simple construction
of an algebra Aover Ω such that there is a natural way to define a probability pre-measure. One
can then extend this probability pre-measure to a probability measure that is both minimal and
unique in a precise sense. For example, the standard Lebesgue measure over the real line can be
constructed this way.
Finally, we define random variables.
Definition 2.2.5: Random
A mapping x: ΩÑSbetween two measurable spaces pΩ,FqandpS,Tqis anpS,Tq-valued
random variable , or a measurable mapping, if, for all BPT,
x´1pBqdef“tωPΩ : xpωqPBuPF. (2.2)
Any measurable function (random variable) induces a new probability measure on the output
σ-algebra based on the one defined on the original σ-algebra. This is called the pushforward
measure (cf.§2.4 in Tao, 2011), which we will denote by P˚, given by
P˚pxPEqdef“P`
x´1pEq˘
, (2.3)
that is, the probability of the result of xbeing in some event Eis determined by the probability of
the event of all the elements which x maps into E, i.e., the pre-image of Egiven by x.
Example 2.2.2: Random Variables
We give some simple examples of random variables.
1.Let Ω be the set of possible outcomes of throwing a fair coin, i.e., Ωdef“tT,Hu. Define
2.2. A MEASURE-THEORETIC FOUNDATION 13
Fdef“PpΩq,Sdef“t0,1u, and Tdef“PpSq. Then, the random variable
x:#
TÞÑ0
HÞÑ1
assigns tails ( T) the value 0 and heads ( H) the value 1.
2.Consider the probability space of throwing two dice (similar to Example 2.2.1) where
Ω“tpi,jq:i,j“1,..., 6uwhere the element pi,jqrefers to rolling ion the first and
jon the second die and F“PpΩq. Define Sdef“ZandTdef“PpSq. Then, the random
variable
x:pi,jqÞÑi`j
is anpS,Tq-valued random variable which represents the sum of two dice.
14 CHAPTER 2. PROBABILISTIC FOUNDATIONS
2.3 Language Models: Distributions over Strings
Language models are defined as probability distributions over sequences of words, referred to as
utterances. This chapter delves into the formalization of the term “utterance” and introduces
fundamental concepts such as the alphabet, string, and language. Utilizing these concepts, a formal
definition of a language model is presented, along with a discussion on the intricacies of defining
distributions over infinite sets.
2.3.1 Sets of Strings
We begin by defining the very basic notions of alphabets and strings, where we take inspiration from
formal language theory . First and foremost, formal language theory concerns itself with sets of
structures . The simplest structure it considers is a string . So what is a string? We start with the
notion of an alphabet.
Definition 2.3.1: Alphabet
Analphabet is a finite, non-empty set. In this course, we will denote an alphabet using
Greek capital letters, e.g., Σ and ∆. We refer to the elements of an alphabet as symbols or
letters and will denote them with lowercase letters: a,b,c.
Definition 2.3.2: String
Astringaover an alphabet is any finite sequence of letters. Strings made up of symbols from
Σ will denoted by bolded Latin letters, e.g., y“y1¨¨¨yTwhere each ynPΣ.
aA string is also referred to as a word , which continues with the linguistic terminology.
The length of a string, written as |y|, is the number of letters it contains. Usually, we will use T
to denote|y|more concisely whenever the usage is clear from the context. There is only one string
of length zero, which we denote with the distinguished symbol εand refer to as the empty string .
By convention, εisnotan element of the original alphabet.
New strings are formed from other strings and symbols with concatenation . Concatenation,
denoted with x˝yor just xy, is an associative operation on strings. Formally, the concatenation
of two words yandxis the word y˝x“yx, which is obtained by writing the second argument
after the first one. The result of concatenating with εfrom either side results in the original string,
which means that εis the unit of concatenation and the set of all words over an alphabet with the
operation of concatenation forms a monoid .
We have so far only defined strings as individual sequences of symbols. To give our strings made
up of symbols in Σ a set to live in, we now define Kleene closure of an alphabet Σ.
Definition 2.3.3: Kleene Star
Let Σ be an alphabet. The Kleene star Σ˚is defined as
Σ˚“8ď
n“0Σn(2.4)
2.3. LANGUAGE MODELS: DISTRIBUTIONS OVER STRINGS 15
where
Σndef“Σˆ¨¨¨ˆ Σ looooomooooon
ntimes(2.5)
Note that we define Σ0def“tεu. We call the Σ˚theKleene closure of the alphabet Σ. We
also define
Σ`def“8ď
n“1Σn“ΣΣ˚. (2.6)
Finally, we also define the set of all infinite sequences of symbols from some alphabet Σ as Σ8.
Definition 2.3.4: Infinite sequences
Let Σ be an alphabet. The set of all infinite sequences over Σ is defined as:
Σ8def“Σˆ¨¨¨ˆ Σ looooomooooon
8-times, (2.7)
Since strings are canonically finite in computer science, we will explicitly use the terms infinite
sequence or infinite string to refer to elements of Σ8.
More informally, we can think of Σ˚as the set which contains εand all (finite-length) strings
which can be constructed by concatenating arbitrary symbols from Σ. Σ`, on the other hand, does
notcontainε, but contains all other strings of symbols from Σ. The Kleene closure of an alphabet
is acountably infinite set (this will come into play later!). In contrast, the set Σ8isuncountably
infinite for any Σ such that |Σ|ě2.
The notion of the Kleene closure leads us very naturally to our next definition.
Definition 2.3.5: Formal language
Let Σ be an alphabet. A languageLis a subset of Σ˚.
That is, a language is just a specified subset of all possible strings made up of the symbols in the
alphabet. This subset can be specified by simply enumerating a finite set of strings, or by a formal
model . We will see examples of those later. Importantly, these strings are finite . If not specified
explicitly, we will often assume that L“Σ˚.
A note on terminology. As we mentioned, these definitions are inspired by formal language
theory. We defined strings as our main structures of interest and symbols as their building blocks.
When we talk about natural language, the terminology is often slightly different: we may refer
to the basic building blocks (symbols) as tokens orwords (which might be composed of one or
more characters and form some form of “words”) and their compositions (strings) as sequences or
sentences . Furthermore, what we refer to here as an alphabet may be called a vocabulary (of
words or tokens) in the context of natural language. Sentences are therefore concatenations of words
from a vocabulary in the same way that strings are concatenations of symbols from an alphabet.
16 CHAPTER 2. PROBABILISTIC FOUNDATIONS
Example 2.3.1: Kleene Closure
Let Σ“ta,b,cu. Then
Σ˚“tε,a,b,c,aa,ab,ac,ba,bb,bc,ca,cb,cc,aaa,aab,aac,... u.
Examples of a languages over this alphabet include L1def“ta,b,ab,bau,L2def“tyPΣ˚|y1“au,
andL3def“tyPΣ˚||y|is evenu.
Next, we introduce two notions of subelements of strings.
Definition 2.3.6: String Subelements
Asubsequence of a string yis defined as a sequence that can be formed from yby deleting
some or no symbols, leaving the order untouched. A substring is a contiguous subsequence.
For instance, abandbcare substrings and subsequences of y“abc, whileacis a subsequence
but not a substring. Prefixes andsuffixes are special cases of substrings. A prefix is a
substring of ythat shares the same first letter as yand a suffix is a substring of ythat shares
the same last letter as y. We will also denote a prefix y1...yn´1of the string y“y1...yTas
yăn. We will also use the notation yŸy1to denote that yis a suffix of y1.
2.3.2 Defining a Language Model
We are now ready to introduce the main interest of the entire lecture series: language models.
Definition 2.3.7: Language model
Let Σ be an alphabet. A language model is a (discrete) distribution pLMover Σ˚.
Example 2.3.2: A very simple language model
Let Σdef“tau. FornPNě0, define
pLMpanqdef“2´pn`1q,
wherea0def“εandandef“a...aloomoon
ntimes.
We claim that pLMis a language model. To see that, we verify that it is a valid probability
distribution over Σ˚. It is easy to see that pLMpanqě0 for anyn. Additionally, we see that
the probabilities of finite sequences indeed sum to 1:
ÿ
yPΣ˚pLMpyq“8ÿ
n“0pLMpanq“8ÿ
n“02´pn`1q“1
28ÿ
n“02´n“1
21
1´1
2“1.
In our formal analysis of language models, we will also often refer to the language defined by a
language model.
2.3. LANGUAGE MODELS: DISTRIBUTIONS OVER STRINGS 17
Definition 2.3.8: Weighted language
LetpLMbe a language model. The weighted language ofpLMis defined as
LppLMqdef“tpy,pLMpyqq|yPΣ˚u (2.8)
Example 2.3.3: Languge of a langauge model
The language of the language model from Example 2.3.2 is
LppLMqdef“!´
an,2´pn`1q¯
|nPNě0)
(2.9)
A language model is itself a very simple concept—it is simply a distribution that weights strings
(natural utterances) by their probabilities to occur in a particular language. Note that we have
not said anything about how we can represent or model this distribution yet. Besides, for any
(natural) language, the ground-truth language model pLMis of course unknown and complex. The
next chapter, therefore, discusses in depth the computational models which we can use to try to
tractably represent distributions over strings and ways of approximating (learning) the ground-truth
distribution based on finite datasets using such models.
18 CHAPTER 2. PROBABILISTIC FOUNDATIONS
2.4 Global and Local Normalization
The previous chapter introduced a formal definition of a language as a set of strings and the definition
of a language model as a distribution over strings. We now delve into a potpourri of technical
questions to complete the theoretical minimum for discussing language models. While doing so, we
will introduce (and begin to answer) three fundamental questions in the first part of the course. We
will introduce them later in the section.
A note on terminology. Unfortunately, we will encounter some ambiguous terminology. In §2.5,
we explicitly define a language model as a valid probability distribution over Σ˚, the Kleene closure
of some alphabet Σ, which means thatř
yPΣ˚pLMpyq“1. As we will see later, this means that
the model is tight, whereas it is non-tight ifř
yPΣ˚pLMpyqă1. Definitionally, then, all language
models are tight. However, it is standard in the literature to refer to many non-tight language
models as language models as well. We pardon in advance the ambiguity that this introduces. Over
the course of the notes, we attempt to stick to the convention that the term “language model”
without qualification only refers to a tight language model whereas a “non-tight language model” is
used to refer to a language model in the more colloquial sense. Linguistically, tight is acting as a
non-intersective adjective. Just as in English, where a fake gun is not a gun, so too in our course
notes a non-tight language model is not a language model. This distinction does in fact matter. On
one hand, we can prove that many language models whose parameters are estimated from data
(e.g., a finite-state language model estimated by means of maximum-likelihood estimation) are, in
fact, tight. On the other hand, we can show that this is nottrue in general, i.e., notall language
models estimated from data will be tight. For instance, a recurrent neural network language model
estimated through gradient descent may not be tight (Chen et al., 2018).
When specifying pLM, we have two fundamental options. Depending on whether we model
pLMpyqfor each string ydirectly or we model individual conditional probabilities pLMpyn|yătqwe
distinguish globally and locally normalized models. The names naturally come from the way the
distributions in the two families are normalized: whereas globally normalized models are normalized
by summing over the entire (infinite) space of strings, locally normalized models define a sequence of
conditional distributions and make use of the chain rule of probability to define the joint probability
of a whole string.
The beginning of sequence string symbol. Conventionally, we will include a special symbol
over which globally or locally normalized models operate: the beginning ofsequence (bos)
symbol, which, as the name suggests, denotes the beginning of a string or a sequence. For a string
y“y1¨¨¨yT, we will suggestively denote y0def“bos.
2.4.1 Globally Normalized Language Models
We start with globally normalized models. Such models are also called energy-based language
models in the literature (Bakhtin et al., 2021). To define a globally normalized language model, we
start with the definition of an energy function.
Definition 2.4.1: Energy function
Anenergy function is a function pp: Σ˚ÑR.
2.4. GLOBAL AND LOCAL NORMALIZATION 19
Inspired by concepts from statistical mechanics, an energy function can be used to define a very
general class of probability distributions by normalizing its exponentiated negative values.
Now, we can define a globally normalized language model in terms of an energy function over
Σ˚.
Definition 2.4.2: Globally normalized models
LetppGNpyq:Σ˚ÑRbe an energy function. A globally normalized model (GNM) is
defined as
pLMpyqdef“expr´ppGNpyqsř
y1PΣ˚expr´ppGNpy1qsdef“1
ZGexpr´ppGNpyqs, (2.10)
whereZGdef“ř
y1PΣ˚expr´ppGNpy1qs.aWe callZGthenormalization constant .
aWe will later return to this sort of normalization when we define the softmax function in §3.1.
Globally normalized models are attractive because one only needs to define an (unnormalized)
energy function ppGN, which scores entire sequences at once. This is often easier than specifying a
probability distribution. Furthermore, they define a probability distribution over strings yPΣ˚
directly . As we will see in §2.4.2, this stands in contrast to locally normalized language models
which require care with the space over which they operate. However, the downside is that it may be
difficult to compute the normalizer ZG.
Normalizability
In defining the normalizer ZGdef“ř
y1PΣ˚expr´ppGNpy1qs, we notationally cover up a certain subtlety.
The set Σ˚is countably infinite, so ZGmay diverge to 8. In this case, Eq. (2.10) is not well-defined.
This motivates the following definition.
Definition 2.4.3: Normalizable energy function
We say that an energy function is normalizable if the quantity ZGin Eq. (2.10) is finite, i.e.,
ifZGă8.
With this definition, we can state a relatively trivial result that characterizes when an energy
function can be turned into a globally normalized language model.
Theorem 2.4.1: Normalizable energy functions induce language models
Any normalizable energy function pGNinduces a language model, i.e., a distribution over Σ˚.
20 CHAPTER 2. PROBABILISTIC FOUNDATIONS
Proof. Given an energy function ppGN, we have exp r´ppGNpyqsě0 and
ÿ
yPΣ˚pGNpyq“ÿ
yPΣ˚expr´ppGNpyqsř
y1PΣ˚expr´ppGNpy1qs(2.11)
“1ř
y1PΣ˚expr´ppGNpy1qsÿ
yPΣ˚expr´ppGNpyqs (2.12)
“1, (2.13)
which means that pGNis a valid probability distribution over Σ˚. ■
While the fact that normalizable energy functions always form a language model is a big
advantage, we will see later that ensuring that they are normalizable can be difficult and restrictive.
This brings us to the first fundamental question of the section:
Question 2.1: Normalizing an energy function
When is an energy function normalizable? More precisely, for which energy functions ppGNis
ZGă8?
We will not discuss any specific results here, as there are no general necessary or sufficient
conditions—the answer to this of course depends on the precise definition of ppGN. Later in the course
notes, we will present two formalisms where we can exactly characterize when an energy function
is normalizable. First, when it is weighted finite-state automaton (cf. §4.1), and, second, when it
is defined through weighted context-free grammars ( §4.2) and discuss the specific sufficient and
necessary conditions there. However, under certain assumptions, determining whether an energy
function is normalizable in the general case is undecidable.
Moreover, even if it is known that an energy function is normalizable, we still need an efficient
algorithm to compute it. But, efficiently computing ZGcan be challenging: the fact that Σ˚
isinfinite means that we cannot always compute ZGin a tractable way. In fact, there are no
general-purpose algorithms for this. Moreover, sampling from the model is similarly intractable, as
entire sequences have to be drawn at a time from the large space Σ˚.
2.4.2 Locally Normalized Language Models
The inherent difficulty in computing the normalizer, an infinite summation over Σ˚, motivates
the definition of locally normalized language models, which we will denote with pLN. Rather than
defining a probability distribution over Σ˚directly, they decompose the problem into the problem
of modeling a series of conditional distributions over the next possible symbol in the string given
the context so far, i.e., pLNpy|yq, which could be na¨ ıvely combined into the full probability of the
string by multiplying the conditional probabilities.9Intuitively, this reduces the problem of having
to normalize the distribution over an infinite set Σ˚to the problem of modeling the distribution of
thenext possible symbol yngiven the symbols seen so far yăn. This means that normalization would
only ever require summation over |Σ|symbols at a time, solving the tractability issues encountered
by globally normalized models.
9We will soon see why this would not work and why we have to be a bit more careful.
2.4. GLOBAL AND LOCAL NORMALIZATION 21
However, we immediately encounter another problem: In order to be a language model, pLNpy|yq
must constitute a probability distribution over Σ˚. However, as we will discuss in the next section,
this may not be the case because locally normalized models can place positive probability mass on
infinitely long sequences (cf. Example 2.5.1 in §2.5.1). Additionally, we also have to introduce a new
symbol that tells us to “stop” generating a string, which we call the endofsequence symbol, eos.
Throughout the notes, we will assume eosRΣ and we define
Σdef“ΣYteosu. (2.14)
Moreover, we will explicitly denote elements of Σ˚asyand symbols in Σasy. Given a sequence
of symbols and the eossymbol, we take the string to be the sequence of symbols encountered
before thefirsteossymbol. Informally, you can think of the bossymbol as marking the beginning
of the string, and the eossymbol as denoting the end of the string or even as a language model
terminating its generation, as we will see later.
Due to the issues with defining valid probability distributions over Σ˚, we will use the term
sequence model to refer to any model that may place positive probability on infinitely long sequences.
Thus, sequence models are strictly more general than language models, which, by definition, only
place positive probability mass on strings, i.e., finite sequences.
Definition 2.4.4: Sequence model
Let Σ be an alphabet. A sequence model (SM) over Σ is defined as a set of conditional
probability distributions
pSMpy|yq (2.15)
foryPΣ and yPΣ˚. We will refer to the string yinpSMpy|yqas the history or the
context .
Note that we will mostly consider SMs over the set Σ. To reiterate, we have just formally defined
locally normalized sequence models rather than locally normalized language models. That has to do
with the fact that, in contrast to a globally normalized model with a normalizable energy function,
a SM might not correspond to a language model, as alluded to at the beginning of this section and
as we discuss in more detail shortly.
We will now work up to a locally normalized language model.
Definition 2.4.5: Locally normalized language model
Let Σ be an alphabet. Next, let pSMbe a sequence model over Σ. Alocally normalized
language model (LNM) over Σ is defined as
pLNpyqdef“pSMpeos|yqTź
t“1pSMpyt|yătq (2.16)
foryPΣ˚with|y|“T. We say a locally normalized language model is tight if
ÿ
yPΣ˚pLNpyq“1. (2.17)
Tightness is a nuanced concept that will be discussed in great detail in §2.5.
22 CHAPTER 2. PROBABILISTIC FOUNDATIONS
We now contrast globally and locally normalized models pictorially in the following example.
Example 2.4.1: Locally and globally normalized language models
Fig. 2.2a shows a simple instance of what a locally normalized language model would look
like. We can compute the probabilities of various strings by starting at the root node bos
and choosing one of the paths to a leaf node, which will always be eos. The values on the
edges represent the conditional probabilities of observing the new word given at the target of
the edge given the context seen on the path so far, i.e., pLNpyt|yătqat the level tof the tree.
For example, the probability of the string bos“The best” eosunder this language model is
0.04¨0.13¨0.22“0.001144. On the other hand, a globally normalized model would simply
score all possible sentences using the score function ppGNpyq, as is hinted at in Fig. 2.2b.
Locally Normalizing a Language Model
The second fundamental question of this section concerns the relationship between language models
and local normalization.
Question 2.2: Locally normalizing a language model
When can a language model be locally normalized?
The answer to that is simple: every language model can be locally normalized! While the
intuition behind this is very simple, the precise formulation is not. Before we discuss the details, we
have to introduce the concept of prefix probabilities, which denote the sum of the probabilities of all
strings beginning with a certain prefix.
Definition 2.4.6: Prefix probability
LetpLMbe a language model. We define a pLM’sprefix probability πas
πpyqdef“ÿ
y1PΣ˚pLM`
yy1˘
, (2.18)
that is, the probability that yis a prefix of any string yy1in the language, or, equivalently,
the cumulative probability of all strings beginning with y.
Note that, naturally, πpεq“1.
Theorem 2.4.2: Any language model can be locally normalized
LetpLMbe a language model. Then, there exists a locally normalized language model pLN
such that, for all yPΣ˚with|y|“T,
pLMpyq“pLNpyq“pSMpeos|yqTź
t“1pSMpyt|yătq. (2.19)
2.4. GLOBAL AND LOCAL NORMALIZATION 23
bos
Hello
there
¨¨¨world
¨¨¨0.210.06¨¨¨ Please
consider
¨¨¨don’t
¨¨¨0.090.02The
best
!
eos1eos0.220.07quick
and
¨¨¨brown
¨¨¨0.120.010.080.130.04
0.010.03
(a) An example of a locally normalized language model. The values of the edges represent the conditional
probability of observing the new word given the observed words (higher up on the path from the root node
bos). Note that the probabilities stemming from any inner node should sum to 1—however, to avoid clutter,
only a subset of the possible arcs is drawn.
y„ppGNpThe bestq
ppGNpThe best!q
ppGNpThe quick fox. q
ppGNpHello World! q
(b) An example of a globally normalized model which can for example generate sentences based on the
probabilities determined by normalizing the assigned scores ppGN.
Figure 2.2: “Examples” of a locally and a globally normalized language model.
24 CHAPTER 2. PROBABILISTIC FOUNDATIONS
Proof. We define the individual conditional probability distributions over the next symbol of the
SMpSMusing the chain rule of probability. If πpyqą0, then define
pSMpy|yqdef“πpyyq
πpyq(2.20)
foryPΣ and yPΣ˚such thatppyqą0. We still have to define the probabilities of ending the
sequence using pSMby defining the eosprobabilities. We define, for any yPΣ˚such thatπpyqą0,
pSMpeos|yqdef“pLMpyq
πpyq(2.21)
that is, the probability that the globally normalized model will generate exactly the string yand
not any continuation of it yy1, given that yhas already been generated. Each of the conditional
distributions of this model (Eqs. (2.20) and (2.21)) is clearly defined over Σ. This, therefore, defines
a valid SM. To see that pLNconstitutes the same distribution as pLM, consider two cases.
Case 1: Assumeπpyqą0. Then, we have
pLNpyq“«Tź
t“1pSMpyt|yătqff
pSMpeos|yq (2.22)
“πpy1q
πpεqπpy1y2q
πpy1q¨¨¨πpyăTq
πpyăT´1qπpyq
πpyăTqpSMpeos|yq
“πpyq
πpεqpLMpyq
πpyq(2.23)
“pLMpyq (2.24)
whereπpεq“1.
Case 2: Assumeπpyq“0. Let y“y1¨¨¨yT. Then, there must exist a 1 ďt1ďTsuch that
πpyăt1q“0. Note that
pLNpyq“t1ź
t“1pSMpyt|yătq“0 (2.25)
whereas the conditional probabilities after t1can be arbitrarily defined since they do not affect the
string having 0 probability. ■
When Is a Locally Normalized Language Model a Language Model?
LNMs which specify distributions over strings pLNpy1...yTqin terms of their conditional probabilities
pSMpyt|yătqfort“1,...,T andpSMpeos|yqhave become the standard in NLP literature. However,
LNMs come with their own set of problems. An advantage of normalizable globally normalized
models is that they, by definition, always define a valid probability space over Σ. Although this might
be counterintuitive at first, the same cannot be said for LNMs—in this sense, locally normalized
“language models” might not even be language models! One might expect that in a LNM pLN, it
would hold thatř
yPΣ˚pLNpyq“1. However, this might not be the case! This is the issue with the
terminology we brought up earlier and it brings us to the last fundamental question of this section.
2.4. GLOBAL AND LOCAL NORMALIZATION 25
Question 2.3: Locally normalized language models
When does an LNM encode a language model?
As the conditions are a bit more nuanced, it requires a longer treatment. We explore this issue
in much more detail in the next section.
26 CHAPTER 2. PROBABILISTIC FOUNDATIONS
2.5 Tight Language Models
We saw in the last section that any language model pLMcan be converted into a locally normalized
sequence model (cf. §2.4.2). The converse, however, is nottrue. As alluded to in the previous
section and as we detail in this section, there exist sets of conditional distributions pLNpy|yqover
Σ˚such thatpLNpyqas defined in Eq. (2.15) does not represent a valid probability measure over Σ˚
(after taking into account the semantics of eos), i.e., over the set of finite strings. Indeed, we will
later show that some popular classes of locally normalized sequence models used in practice have
parameter settings in which the generative process terminates with probability ă1. This means
thatpLN“leaks” some of its probability mass to infinite sequences. This section investigates this
behavior in a lot of detail. It is based on the recent work from Du et al. (2022).
2.5.1 Tightness
Models whose generative process may fail to terminate are called non-tight (Chi, 1999).10
Definition 2.5.1: Tightness
A locally normalized language model pLNderived from a sequence model pSMis called tight if
it defines a valid probability distribution over Σ˚:
ÿ
yPΣ˚pLNpyq“ÿ
yPΣ˚«
pSMpeos|yqTź
t“1pSMpyt|yătqff
“1. (2.26)
Note that the individual conditional distributions pSMpy|yqin a non-tight LNM still are valid
conditional distributions (i.e., they sum to one). However, the distribution over all possible strings
that they induce may not sum to 1. To be able to investigate this phenomenon more closely, let us
first examine what the conditional probabilities of an LNM actually define and how they can result
in non-tightness. We now ask ourselves: given a sequence model pSM, what ispLN? IspLNa language
model, i.e., a distribution over Σ˚(after taking into account the semantics of eos)? Certainly, the
answer is yes if the LNM’s conditional probabilities match the conditional probabilities of some
known language model pLMas defined in §2.4.2,11in which case pLNis specifically the language
modelpLMitself. In this case clearly pLNpΣ˚qdef“ř
yPΣ˚pLNpyq“ř
yPΣ˚pLMpyq“1. If instead
pLNpΣ˚qă1, the LNM’s conditional probabilities do notmatch the conditional probabilities of any
language model pLM.
To see how this can happen, we now exhibit such an LNM in the following example.
Example 2.5.1: A non-tight 2-gram model
Consider the bigram model defined in Fig. 2.3a over the alphabet Σ “ta,bu.aAlthough the
conditional probability distributions pLNp¨|yănqeach sum to 1 over Σ, they fail to combine
into a model pLNthat sums to 1 over Σ˚(i.e., a language model): under this model, any finite
10Tight models are also called consistent (Booth and Thompson, 1973; Chen et al., 2018) and proper (Chi, 1999)
in the literature.
11That is,pLMpyt|yătq“pLNpyt|yătqwhenever the former conditional probability is well-defined under the
language model pLM, i.e., whenever ytPΣ and yătPΣ˚withpLMpyătqą0.
2.5. TIGHT LANGUAGE MODELS 27
string that contains the symbol b will have probability 0, since pLNpeos|bq“pLNpa|bq“0.
This implies pLNpΣ˚q“ř8
n“0pLNpanq“ř8
n“0p0.7qn¨0.1“0.1
1´0.7“1
3ă1.
aThe graphical representation of the LNM depicts a so-called weighted finite-state automaton, a framework
of language models we will introduce shortly. For now, it is not crucial that you understand the graphical
representation and you can simply focus on the conditional probabilities specified in the figure.
Example 2.5.2: A tight 2-gram model
On the other hand, in the bigram model in Fig. 2.3b, obtained from Example 2.5.1 by changing
the arcs from the b state, pLNpΣ˚q“1. We can see that by calculating:
PpΣ˚q“8ÿ
n“18ÿ
m“0Ppanbmq
“8ÿ
n“1˜
Ppanq`8ÿ
m“1Ppanbmq¸
“8ÿ
n“1˜
0.1¨p0.7qn´1`8ÿ
m“1p0.7qn´1¨0.2¨p0.9qm´1¨0.1¸
“8ÿ
n“1ˆ
0.1¨p0.7qn´1`p0.7qn´1¨0.2¨1
1´0.9¨0.1˙
“8ÿ
n“1`
0.1¨p0.7qn´1`0.2¨p0.7qn´1˘
“8ÿ
n“10.3¨p0.7qn´1“0.3
1´0.7“1.
Example 2.5.1 confirms that the local normalization does not necessarily yield pLNthat is a valid
distribution over Σ˚. But ifpLNis not a language model, what is it? It is intuitive to suspect that,
in a model with pLNpΣ˚qă1, the remainder of the probability mass “leaks” to infinite sequences,
i.e., the generative process may continue forever with probability ą0. This means that, to be able
to characterize pLN, we will have to be able to somehow take into account infinite sequences. We
will make this intuition formal below.
Delving a bit deeper, the non-tightness of Example 2.5.1 is related to the fact that the conditional
probability of eosis 0 at some states, in contrast to Example 2.5.2. However, requiring pLNpyn“
eos|yănqą0 for all prefixes yănis neither necessary norsufficient to ensure tightness. It is not
necessary because one can, for example, construct an LNM in which pLNpyn“eos|yănq“0.1
whennis even but“0 otherwise. Such a model generates only odd-length strings but is tight.
We will postpone non-sufficienty for later, where we will present specific LNMs under which the
conditional probability of eosis alwaysą0, yet are non-tight.
28 CHAPTER 2. PROBABILISTIC FOUNDATIONS
pLNpa|bosq 1
pLNpa|aq 0.7
pLNpb|aq 0.2
pLNpeos|aq 0.1
pLNpb|bq 1
pLNpeos|eosq1ab
eosa{0.7b{1
eos{1b{0.2
eos
{0.1
(a) A non-tight 2-gram model.
pLNpa|bosq 1
pLNpa|aq 0.7
pLNpb|aq 0.2
pLNpeos|aq 0.1
pLNpb|bq 0.9
pLNpeos|bq 0.1
pLNpeos|eosq1ab
eosa{0.7b{0.9
eos{0.1
eos{1b{0.2
eos
{0.1
(b) A tight 2-gram model.
Figure 2.3: Tight and non-tight bigram models, expressed as Mealy machines. Symbols with
conditional probability of 0 are omitted.
2.5.2 Defining the probability measure of an LNM
We now rigorously characterize the kind of distribution induced by an LNM, i.e., we investigate what
pLNis. As mentioned earlier, an LNM can lose probability mass to the set of infinite sequences, Σ8.
However, Σ8, unlike Σ˚, isuncountable , and it is due to this fact that we need to work explicitly with
themeasure-theoretic formulation of probability which we introduced in §2.2. We already saw the
peril of not treating distributions over uncountable sets carefully is necessary in Example 2.1.1—the
set of all infinite sequences of coin tosses is indeed uncountable.
Including infinite strings and the end of string symbol. As we saw in Example 2.1.1,
sampling successive symbols from a non-tight LNM has probability ą0 of continuing forever, i.e.,
generating infinite strings. Motivated by that, we hope to regard the LNM as defining a valid
probability space over Ω “Σ˚YΣ8, i.e., both finite as well as infinite strings, and then “relate” it
to our definition of true language models. Notice, however, that we also have to account for the
difference in the alphabets: while we would like to characterize language models in terms of strings
over the alphabet Σ, LNMs work over symbols in Σ.
With this in mind, we now embark on our journey of discovering what pLNrepresents. Given an
LNM, we will first need to turn its pLNinto a measurable space by defining an appropriate σ-algebra .
This type of distribution is more general than a language model as it works over both finite as
well as infinite sequences. To distinguish the two, we will expand our vocabulary and explicitly
differentiate between true language models and non-tight LNMs. We will refer to a distribution over
Σ˚YΣ8as a sequence model. As noted in our definition of a sequence model (cf. Definition 2.4.4),
2.5. TIGHT LANGUAGE MODELS 29
Σ8Algebra´
Σ8,C¯Pre-measure´
Σ8,C,P0¯
Measure´
Σ8,σ`
C˘
,P¯ Measure (sequence model)
pΣ8YΣ˚,σpCq,P1qCylinder
setsConditional prob-
abilities from pLN
Carath´ eodory’s Extension
Random vari-
able construction
Figure 2.4: The outline of our measure-theoretic treatment of LNMs in this section to arrive at
a precise characterization of pLN. The final box corresponds to the sequence model (probability
measure over Σ˚YΣ8) constructed for pLN.
an LNM defines a probabilty measure over Σ˚YΣ8. Thus, an equivalent distribution, which will
be useful for this section, would be the following.
Definition 2.5.2: Sequence model
Asequence model is a probability space over the set Σ˚YΣ8.
Intuitively, and we will make this precise later, the set Σ8ĂΣ˚YΣ8in Definition 2.5.2
represents the event where the sequence model is non-terminating , i.e., it attempts to generate an
infinitely long sequence. We can then understand language models in a new sense.
Definition 2.5.3: Re-definition of a Language model
Alanguage model is a probability space over Σ˚. Equivalently, a language model is a
sequence model such that PpΣ8q“0.
Now buckle up! Our goal through the rest of this section is to rigorously construct a probability
space of a sequence model as in Definition 2.2.2 and Definition 2.5.2 which encodes the probabilities
assigned by an LNM. Then, we will use this characterization to formally investigate tightness. An
outline of what this is going to look like is shown in Fig. 2.4.
Defining an Algebra over Σ8(Step 1)
Since an LNM produces conditional distributions over the augmented alphabet Σ(first box in
Fig. 2.4) and results in possibly infinite strings, we will first construct a probability space over Σ8,
which will naturally induce a sequence model. We will do that by first constructing an algebra (cf.
30 CHAPTER 2. PROBABILISTIC FOUNDATIONS
Definition 2.2.3) over Ω “Σ8for some alphabet Σ (second box in Fig. 2.4). Then, assuming we
are given an LNM pLNover Σ, we will associate the constructed algebra with a pre-measure (cf.
Definition 2.2.4) that is “consistent” with pLN(third box in Fig. 2.4).
We will make use of the following definition to construct the algebra:
Definition 2.5.4: Cylinder set
Given any set HĎΣk, i.e., a set of sequences of symbols from Σof lengthk, define its
cylinder set (of rankk) to be
CpHqdef“!
yω:yPH,ωPΣ8)
(2.27)
In essence, a cylinder set of rank kis the set of infinite strings that share their k-prefix with some
string yPHĎΣk. In particular, for a length- kstring y“y1¨¨¨yk, the cylinder set Cpyqdef“Cptyuq
is the set of all infinite strings prefixed by y.12
We denote the collection of all rank- kcylinder sets by
Ckdef“!
CpHq:HPP´
Σk¯)
(2.28)
and define
Cdef“8ď
k“1Ck (2.29)
to be the collection of all cylinder sets over Ω.13
The following lemma asserts CĎPpΩqis what we want in the second block of Fig. 2.4.
Lemma 2.5.1
CĎPpΩqis an algebra over Ω “Σ8.
Proof. First, Σ8“CpΣkqfor anyk, and in particular is a cylinder set of any rank. Secondly,
given a cylinder set CpHqof rankk, i.e., HĎΣk,`
CpHq˘c“C´
ΣkzH¯
. Hence, Cis closed under
complements. Finally, notice that the intersection of two cylinder sets of ranks k1ďk2is another
cylinder set of rank k2. Hence, Cis an algebra over Ω. ■
With this, the first step of Fig. 2.4 is done!
Defining a Pre-measure over C(Step 2)
We are now ready to define the pre-measure P0for the cylinder algebra C. Given an LNM pLNand
any setCpHqPC, let
P0pCpHqqdef“ÿ
yPHpLNpyq (2.30)
12This type of cylinder set, i.e., one that is generated by a singleton, is also called a thin cylinder .
13We invite the reader to verify that C1ĂC2ĂC3Ă¨¨¨ .
2.5. TIGHT LANGUAGE MODELS 31
where we have defined
pLNpyqdef“Tź
t“1pLNpyt|yătq. (2.31)
Note that there is a caveat here since the same cylinder set may admit different H.14Before showing
thatP0defines a valid pre-measure, we address this and show that P0is indeed well defined.
Proposition 2.5.1
P0as defined in Eq. (2.30) is a well-defined function.
Proof. Suppose a cylinder set can be described by two different prefix sets: H1ĎΣk1andH2ĎΣk2.
In other words, CpH1q“CpH2q. Without loss of generality, assume that k1ďk2. Then,
CpH2q“CpH1q (2.32a)
“ď
yPH1Cpyq (2.32b)
“ď
yPH1ď
yPΣk2´k1Cpyyq. (2.32c)
All the unions above are disjoint, and hence H2“Ť
yPΣk2´k1tyy:yPH1u. Then, by the
locally-normalizing property of pLN, we have that
P0pCpH1qq“P0pCpH2qq. (2.33)
■
With this, we are able to state and prove the lemma which shows that P0is a pre-measure, which
is what we need in the third block of Fig. 2.4.
Lemma 2.5.2
P0is a pre-measure over C.
For the proof of Lemma 2.5.2, we will mostly follow the proof of Theorem 2.3 in Billingsley
(1995), with the exception of invoking the Tychonoff theorem directly. This proof depends on the
following lemma, which is Example 2.10 in Billingsley (1995). We repeat the statement and proof
here for the reader’s convenience.
Lemma 2.5.3
LetP0be a finitely additive probability pre-measure over Csuch that, given a decreasing
sequence of sets A1ĄA2Ą¨¨¨ inCwhereŞ8
n“1An“H,limnÑ8P0pAnq“0. Then, P0is
also countably additive over C.
14For example, in the infinite coin toss model, CpHq“CptHH,HTuq.
32 CHAPTER 2. PROBABILISTIC FOUNDATIONS
Proof. LettAnube a sequence of disjoint sets in Csuch thatA“Ť
nAnPC. Then, defining
Bn“Ť
mąnAm, we see that B1ĄB2Ą¨¨¨ andŞ
nBn“H. Notice that
A“A1YB1“A1YA2YB2“¨¨¨“A1Y¨¨¨YAnYBn (2.34)
for anynand hence by finite additivity of P0
P0pAq“P0pA1q`¨¨¨` P0pAnq`P0pBnq (2.35)
or equivalently
P0pA1q`¨¨¨` P0pAnq“P0pAq´P0pBnq. (2.36)
Since,BnÓHimplies that P0pBnqÓ0 by assumption, taking the limits on both sides of Eq. (2.36)
yields ÿ
nP0pAnq“lim
nÑ8ÿ
iďnP0pAiq“P0pAq´lim
nÑ8P0pBnq“P0pAq (2.37)
which shows countable additivity. ■
We also recall the Tychonoff theorem.15
Theorem 2.5.1: Tychonoff
LettXαuαPJbe an indexed family of compact topologies. Then, their product topologyś
αPJXαis also compact.
We can now give the proof for Lemma 2.5.2.
Proof of Lemma 2.5.2. We first show that P0is finitely additive over C. Let CpH1qandCpH2qbe
two disjoint cylinder sets. By Proposition 2.5.1, we can assume they are of the same rank without
loss of generality. Then,
CpH1qYCpH2q“ď
yPH1tyω:ωPΣ8uYď
yPH2tyω:ωPΣ8u (2.38a)
“ď
yPH1YH2tyω:ωPΣ8u (H1andH2equal rank and disjoint) (2.38b)
“CpH1YH2q (2.38c)
which leads to
P0pCpH1qYCpH2qq“P0pCpH1YH2qq (2.39a)
“ÿ
yPH1YH2pLNpyq (2.39b)
“P0pCpH1qq`P0pCpH2qq. (2.39c)
Hence, P0is finitely additive.
Now, equip Σwith the discrete topology. Since Σis finite, it is compact under the discrete
topology and so is Σ8by Theorem 2.5.1. Then, by properties of the product topology over discrete
15See§37 in Munkres (2000) for a detailed and well-written treatise.
2.5. TIGHT LANGUAGE MODELS 33
finite spaces, all cylinder sets in Σ8are compact. To apply Lemma 2.5.3, let C1ĄC2Ą ¨¨¨
be a decreasing sequence of cylinder sets with empty intersection. Suppose to the contrary that
P0pŞ
nCnqą0. This would imply that all Cnare nonempty (any of these being empty would result
in a measure 0). However, by Cantor’s intersection theorem16,Ş
nCnis nonempty, contradicting
the assumption. Hence, P0pŞ
nCnq“0, and by Lemma 2.5.3, P0is countably additive.
With this, we have proved that P0is countably additive. To show that P0defines a pre-measure,
we still have to show that P0pΩq“1. Recall from the proof of Lemma 2.5.1 that Σ8“CpΣkqfor
anyką0. In particular, Σ8“CpΣ1q“CpΣq. This means that
P0pΩq“P0`
C`
Σ˘˘
(2.40)
“ÿ
yPΣpLNpyq (2.41)
“ÿ
yPΣpLNpy|bosq“1. (2.42)
The last equality follows from local normalization of the sequence model. ■
With this, we have successfully completed the first two steps of Fig. 2.4! However, we have
only defined a pre-measure over the set of infinite eos-containing sequences Σ8. This does not yet
satisfy all the properties we would like from a probability space. Because of that, we next extend
the constructed probability pre-measure P0into a valid probability measure Pto arrive to a valid
probability space.
Extending the Pre-measure P0into a Measure P(Step 3)
To extend P0into a measure, we will use Carath´ eodory’s theorem:
Theorem 2.5.2: Carath ´eodory’s Extension Theorem
Given an algebra Aover some set Ω and a probability pre-measure P0:AÑr0,1s, there exists
a probability space pΩ,F,Pqsuch that AĂFandP|A“P0. Furthermore, the σ-algebra F
depends only on Aand is minimal and unique, which we will also denote by σpAq, and the
probability measure Pis unique.
Proof Sketch. First, construct an outer measure by approximation with countable coverings. Then,
show that the collection of sets that is measurable with respect to this outer measure is a σ-algebra
Fthat contains A. Finally, restricting the outer measure to this σ-algebra , one is then left with
a probability space. To show minimality, one can show that Fis contained in any σ-algebra that
contains A. Uniqueness is given by applying Dynkin’s π-λtheorem (Theorem 3.2 in Billingsley,
1995).
Great care must be taken in each step involved in the outline above. To address these is well
beyond the scope of this treatment and we refer the reader to the many excellent texts with a proof
of this theorem, such as Chapter 12 in Royden (1988) and Chapter 11 in Billingsley (1995). ■
16Cantor’s intersection theorem states that a decreasing sequence of nonempty compact sets have a nonempty
intersection. A version of this result in introductory real analysis is the Nested Interval Theorem.
34 CHAPTER 2. PROBABILISTIC FOUNDATIONS
Applying Carath´ eodory’s extension theorem to our cylinder algebra Cand pre-measure P0, we
see that there exists a probability space pΣ8,σpCq,Pqover Σ8that agrees with the LNM pLN’s
probabilities.
Phew! This now gets us to the fourth box in Fig. 2.4 and we only have one step remaining.
Defining a Sequence Model (Step 4)
We now have to make sure that the outcome space of the defined probability space fits the definition
of a sequence model. That is, we have to find a way to convert (map) the infinite eos-containing
sequences from Σ8intoeos-free finite or possibly infinite strings processed by a sequence model as
required by Definition 2.5.2. We will achieve this through the use of a random variable .
Recall from Definition 2.2.5 that a random variable is a mapping between twoσ-algebra s. Since
we want our final measure space to work with the outcome space Σ8YΣ˚, we, therefore, want
to construct a σ-algebra over Σ˚YΣ8and then map elements from Σ8toΣ˚YΣ8to have the
appropriate objects. We will do so in a similar fashion as we constructed pΣ8,Cq. Given HĎΣk,
define a rank- kcylinder set in Σ˚YΣ8to be
CpHqdef“tyω:yPH,ωPΣ˚YΣ8u. (2.43)
Notice the major change from Eq. (2.27): the suffixes ωof the elements in CpHqnow come from
Σ˚YΣ8rather than Σ8. This means (i)that they do not contain eosand (ii)that they (and
thus, elements of CpHq) can also be finite. Let Ckbe the set of all rank- kcylinder sets. Define
Cdef“Ť8
k“1Ck. Then,σpCqis aσ-algebra by the same reasoning as in Lemma 2.5.1 and Theorem 2.5.2.
We can now define the following random variable
xpωq“#
ωăkifkis the first eosinω,
ω otherwise (ifeosRω)(2.44)
given any ωPΣ8. The proposition below shows that x is well-defined.
Proposition 2.5.2
The function x:pΣ8,σpCqqÑp Σ˚YΣ8,σpCqqdefined in Eq. (2.44) is a measurable mapping.
Proof. To show that xis measurable, it suffices to show the measurability of preimage of a generating
set of theσ-algebra . Note that the set of thin cylinder sets is a generating set. Let Cpyqbe a thin
cylinder set,
x´1pCpyqq“x´1ptyω:ωPΣ˚YΣ8uq (2.45a)
“x´1ptyω:ωPΣ˚uqYx´1ptyω:ωPΣ8uq (2.45b)
“˜ď
ωPΣ˚Cpyωeosq¸
Y˜
CpyqX8č
k“1Ac
k¸
(2.45c)
Note that the sets Akabove are defined in Eq. (2.58) which are cylinder sets representing the event
of terminating at step k. Then, from the derivation above, we can see that x´1pCpyqqis formed by
countable operations over measurable sets (cylinder sets) in Σ8, and is hence measurable. So xis
a measurable function. ■
2.5. TIGHT LANGUAGE MODELS 35
xintuitively “cuts out” the first stretch of ωbefore the first eossymbol (where an LNM would
stop generating) or leaves the sequence intact if there is no termination symbol eos. One can
check that P˚, defined using P, is indeed a probability measure on pΣ˚YΣ8,σpCqqand hence
pΣ˚YΣ8,σpCq,P˚qis a probability space. We have therefore arrived at the final box of Fig. 2.4
and shown that, given any LNM, we can construct an associated sequence model as defined in
Definition 2.5.2! In other words, given an LNM pLN, we have constructed a sequence model pSM(a
probability space over Σ8YΣ˚where the probabilities assigned to (infinite) strings by pSMagree
withpLN.
2.5.3 Interpreting the Constructed Probability Space
Under the formulation of a probability space together with a random variable, useful probability
quantities arise naturally and intuitively.
Consider, for example, the probability of a single finite string yPΣ˚,P˚pyq. By definition of x,
this equals
P˚pyq“P˚px“yq (2.46)
“P`
x´1pyq˘
(2.47)
“P´
All the sequences ωPΣ8which map to y.¯
(2.48)
All the sequences ωPΣ8which map to yare sequences of the form ω“yeosω1forω1PΣ8—this
is exactly the cylinder Cpyeosq! By the definition of the probability space`
Σ,σ`
C˘
,P˘
, this is
P`
Cpyeosq˘
“ÿ
y1PtyeosupLN`
y1˘
“pLNpyeosq (2.49)
and as before pLNpyeosq“śT
t“1pLNpyt|yătqpLNpeos|yq.
Altogether, this means that, given a finite string yPΣ˚, we intuitively have
P˚px“yq“pLNpeos|yqpLNpyq. (2.50)
Additionally, as we will show in the next section, the probability of the set of infinite strings
P˚pxPΣ8qis the probability of generating an infinite string.
An important technical detail left out in this discussion so far is that both the singleton set tyu
and Σ8need to be measurable in pΣ˚YΣ8,σpCqqfor the above to make sense. This is addressed
by Proposition 2.5.3 and Proposition 2.5.4.
Proposition 2.5.3
In measure space pΣ˚YΣ8,σpCqq,tyuis measurable for all yPΣ˚.
Proof. By definition in Eq. (2.43), for any yPΣ˚,
Cpyq“tyω:ωPΣ˚YΣ8u (2.51a)
“tyω:ωPΣ˚uYtyω:ωPΣ8u (2.51b)
36 CHAPTER 2. PROBABILISTIC FOUNDATIONS
where
tyω:ωPΣ˚u“tyuYď
aPΣtyaω:ωPΣ˚u (2.52a)
and
tyω:ωPΣ8u“ď
aPΣtyaω:ωPΣ8u. (2.53)
So,
Cpyq“tyuYď
aPΣˆ
tyaω:ωPΣ˚uYtyaω:ωPΣ8u˙
(2.54a)
“tyuYď
aPΣCpyaq (2.54b)
which implies that tyu“CpyqzŤ
aPΣCpyaqand hence measurable. ■
Proposition 2.5.4
In the measure space pΣ˚YΣ8,σpCqq, Σ8is measurable.
Proof. First, the outcome space Σ˚YΣ8is measurable by definition of σ-algebra. Notice that
Σ8“pΣ˚YΣ8qzď
yPΣ˚tyu. (2.55)
Since eachtyuin the above is measurable by Proposition 2.5.3 and Σ˚is a countable set, Σ8is
then measurable. ■
Since bothtyuand Σ8are measurable in pΣ˚YΣ8,σpCqqby Propositions 2.5.3 and 2.5.4, we
have the following.
Proposition 2.5.5
A sequence model pΣ˚YΣ8,σpCq,Pqis tight if and only ifř
yPΣ˚Pptyuq“ 1.
Proof. By definition, a sequence model is tight if and only if PpΣ8q“0. By Propositions 2.5.3
and 2.5.4, we can write
PpΣ˚YΣ8q“PpΣ8q`PpΣ˚q (countable additivity) (2.56a)
“PpΣ8q`ÿ
yPΣ˚Pptyuq. (countable additivity) (2.56b)
Hence, a sequence model is tight if and only ifř
yPΣ˚Pptyuq“ 1. ■
2.5. TIGHT LANGUAGE MODELS 37
Deriving eos
As an aside, the preceding section allows us to motivate the eostoken in LNM as a construct that
emerges naturally. Specifically, for any yPΣ˚, rearranging Eq. (2.50):
pLNpeos|yq“P˚px“yq
pLNpyq(2.57a)
“P˚px“yq
P˚pxPCpyqq(2.57b)
“P˚px“y|xPCpyqq (2.57c)
where we have used pLNpyq“PpCpyqq“Ppx´1pCpyqqq“P˚pxPCpyqq. This means that the eos
probability in an LNM emerges as the conditional probability that, given that we must generate a
string with a prefix yPΣ˚, the string is exactly y, i.e., that generation ends there.
2.5.4 Characterizing Tightness
Now that we have derived a measure-theoretic formalization of the probability space induced by
locally-normalized models, we can use it to provide an exact characterization of tightness in LNMs.
First, we consider the event
Akdef“tωPΣ8:ωk“eosu (2.58)
in the probability space pΣ8,σpCq,Pq. Intuitively, Akis the event that an eossymbol appears at
positionkin the string. Note that under this definition the Akare not disjoint. For example, the
string ω“abeosceosdddd¨¨¨lives in the intersection of A3andA5since eosappears at both
position 3 and position 5. Using Eq. (2.58), we can express the event consisting of all finite strings as
8ď
k“1Ak. (2.59)
It follows that we can express the event of an infinite string as
˜8ď
k“1Ak¸c
“8č
k“1Ac
k. (2.60)
Thus, using the random variable x, we can express the probability of generating an infinite string as
P˚pxPΣ8q“Ppx´1pΣ8qq (2.61a)
“P˜8č
k“1Ac
k¸
. (2.61b)
Hence, we can now restate and formalize the notion of tightness.
38 CHAPTER 2. PROBABILISTIC FOUNDATIONS
Definition 2.5.5: Tight sequence model
A sequence model is said to be tight ifP˚pxPΣ8q“0, in which case it is also a language
model. Otherwise, we say that it is non-tight .
Note that the definition of Akonly uses a string’s k-prefix, and hence is a cylinder set of rank k.
Recalling that the cylinder sets are measurable and so are the sets countably generated by them, we
see that both the event consisting of all finite strings and the event consisting of all infinite strings
are measurable. Thus, P`Ť8
k“1Ak˘
andP`Ş8
k“1Ac
k˘
are well defined.
A Lower Bound Result
We have characterized tightness in terms of the probability of a specific event P`Ş8
k“1Ac
k˘
, a
quantity we now seek to determine.
Lemma 2.5.4
Ifř8
n“2P´
An|Şn´1
m“1Ac
m¯
“8, then P`Ş8
m“1Ac
m˘
“0.
Proof. First, recall an elementary inequality that for xą0,
x´1ělogxô 1´xďlog1
x. (2.62)
Note that PpŞn
m“1Ac
mqą0 for anyn, for otherwise the conditional probabilities would be undefined.
2.5. TIGHT LANGUAGE MODELS 39
Letpndef“PpŞn
m“1Ac
mq. Then we have that pną0 for alln, and
8“8ÿ
n“2PpAn|n´1č
m“1Ac
mq (2.63a)
“8ÿ
n“21´PpAc
n|n´1č
m“1Ac
mq (2.63b)
“lim
NÑ8Nÿ
n“21´PpAc
n|n´1č
m“1Ac
mq (2.63c)
ďlim
NÑ8Nÿ
n“2log 1{PpAc
n|n´1č
m“1Ac
mq (by Eq. (2.62)) (2.63d)
“lim
NÑ8Nÿ
n“2logPpŞn´1
m“1Ac
mq
PpŞn
m“1Acmq(2.63e)
“lim
NÑ8Nÿ
n“2logpn´1
pn(2.63f)
“lim
NÑ8Nÿ
n“2plogpn´1´logpnq (2.63g)
“lim
NÑ8plogp1´logpNq (2.63h)
“logp1´lim
NÑ8logpN (2.63i)
which implies that
lim
NÑ8logpN“´8 (2.64a)
ô lim
NÑ8pN“0 (2.64b)
ô lim
NÑ8PpNč
m“1Ac
mq“0 (2.64c)
ôPp8č
m“1Ac
mq“0. (by continuity of measure) (2.64d)
■
Using Lemma 2.5.4, we can derive the following useful condition of tightness of a language model.
Specifically, it applies when the probability of eosis lower bounded by a function that depends only
on the length and not the content of the prefix.
Proposition 2.5.6
IfpLNpeos|yqěfptqfor all yPΣtand for all tandř8
t“1fptq“8 , then PpŞ8
k“1Ac
kq“0.
In other words, pLNis tight.
40 CHAPTER 2. PROBABILISTIC FOUNDATIONS
Proof. SupposepLNpeos|yqěfptqfor all yPΣt. To apply Lemma 2.5.4, we observe that
AnXpAc
1X¨¨¨X Ac
n´1q“tωPΣ8:ωn“eosu X˜n´1č
i“1tωPΣ8:ωi­“eosu¸
(2.65a)
“tωPΣ8:ω“eos,@iăn,ω­“eosu (2.65b)
“tωPΣ8:ω’s first eosis at position nu (2.65c)
and similarly
Ac
1X¨¨¨X Ac
n´1“tωPΣ8: There is no eosinω’s firstn´1 positionsu (2.66)
Setting Gdef“tωeos:ωPΣn´1uĂΣn, we get
PpAn|Ac
1X¨¨¨X Ac
n´1q“PpAnXpAc
1X¨¨¨X Ac
n´1qq
PpAc
1X¨¨¨X Ac
n´1q(2.67a)
“PpCpGqq
PpCpΣn´1qq(definition of G) (2.67b)
“ř
ωPΣn´1ppeos|ωqppωqř
ωPΣn´1ppωq(by Eq. (2.30)) (2.67c)
ěř
ωPΣn´1fpn´1qppωqř
ωPΣn´1ppωq(definition of fptq) (2.67d)
“fpn´1qř
ωPΣn´1ppωqř
ωPΣn´1ppωq(2.67e)
“fpn´1q. (2.67f)
Sinceř8
t“0fptq“8 , Lemma 2.5.4 shows that the event of a string never terminating, i.e.,Ş8
k“1Ac
k
has probability measure PpŞ8
k“1Ac
kq“0. In other words, if the eosprobability of a language model
is lower bounded by a divergent sequence at every step, then the event that this language model
terminates has probability 1. ■
The Borel–Cantelli Lemmata
It turns out that Proposition 2.5.6 admits a converse statement in which we can prove a similar
property of pLNby assuming that the model is tight. To show this result, we will use a fundamental
inequality from probability theory—the Borel–Cantelli lemmata. The Borel–Cantelli lemmata are
useful for our purposes because they relate the probability measure of sets of the formŞ8
n“0AnorŤ8
n“0Anto a seriesř8
n“0pn. We will only state the lemmata here without supplying their proofs;17
however, we point out that Lemma 2.5.4 can be viewed as a parallel statement to the Borel–Cantelli
lemmata and one can prove the lemmata using a very similar proof (cf. proof of Theorem 2.3.7 in
Durrett, 2019).
Concretely, given a sequence of events tAnu8
n“1in some probability space, the Borel–Cantelli
lemmata are statements about the event
tAni.o.udef“8č
m“18ď
n“mAn (2.68)
17See§2.3 in Durrett (2019) or §4 in Billingsley (1995) instead.
2.5. TIGHT LANGUAGE MODELS 41
where i.o. stands for “infinitely often.” Intuitively, tAni.o.uis the set of outcomes that appear
in infinitely many sets in the collection tAnu8
n“1—they are the events that always remain in the
union of an infinite family of sets no matter how many of the leading ones we remove (hence the
name). We will not use Borel–Cantelli directly, but they offer a probabilistic proof of a key result
(Corollary 2.5.1) which will in turn lead to the desired statement about tightness. We formally state
the first and second Borel–Cantelli lemmata below.
Lemma 2.5.5: Borel–Cantelli I
Ifř8
n“1PpAnqă8 , then PpAni.o.q“0.
Lemma 2.5.6: Borel–Cantelli II
AssumetAnuis a sequence of independent events, thenř8
n“1PpAnq“8ñ PpAni.o.q“1.
Using the Borel–Cantelli lemmata, we can prove the following useful fact.
Corollary 2.5.1
Given a sequence tpnuwherepnPr0,1q. Then,
8ź
n“1p1´pnq“0ðñ8ÿ
n“1pn“8. (2.69)
To show Corollary 2.5.1, we first show the following simple consequence of Borel–Cantelli.
Corollary 2.5.2
IfPpAni.o.q“1, thenř8
n“1PpAnq“8 .
Proof. Suppose to the contrary thatř8
n“1PpAnqă8 , then, by Borel–Cantelli I (Lemma 2.5.5),
PpAni.o.q“0, which contradicts the assumption. Hence,ř8
n“1PpAnq“8 . ■
Proof. We can use a product measure to construct a sequence of independent events tAnu8
n“1such
thatPpAnq“pn. (The product measure ensures independence.) Then, by definition in Eq. (2.68),
tAni.o.uc“8ď
m“1č
němAc
n (2.70)
42 CHAPTER 2. PROBABILISTIC FOUNDATIONS
So,
1´PpAni.o.q“P˜ď
mč
němAc
n¸
(2.71a)
“lim
mÑ8P˜č
němAc
n¸
(2.71b)
“lim
mÑ8ź
němPpAc
nq (Anare independent by construction) (2.71c)
“lim
mÑ8ź
němp1´pnq (2.71d)
pñq:Assumeś8
n“1p1´pnq“0. Then, for any m,
0“ź
ně1p1´pnq“˜ź
1ďnămp1´pnq¸
loooooooooomoooooooooon
ą0˜ź
němp1´pnq¸
(2.72)
So it must the case that, for any m,ś
němp1´pnq“0. Therefore,
1´PpAni.o.q“ lim
mÑ8ź
němp1´pnq“0 (2.73)
which implies PpAni.o.q“1. Corollary 2.5.2 implies thatř8
n“1pn“8.
pðq:Assumeř8
n“1pn“ 8. Then by Borel–Cantelli II (Lemma 2.5.6), PpAni.o.q “1 which
implies
0“1´PpAni.o.q“ lim
mÑ8ź
němp1´pnq (2.74)
Observe that!ś
němp1´pnq)
mis a non-decreasing sequence in m; to see this, note that as m
grows larger we multiply strictly fewer values p1´pnqPp0,1s. However, since we know the sequence
is non-negative and tends to 0, it follows that for anym, we have
ź
němp1´pnq“0. (2.75)
It follows that, for any m, we have
8ź
n“1p1´pnq“ź
nămp1´pnqź
němp1´pnq
loooooomoooooon
“0“ź
nămp1´pnq¨0“0. (2.76)
■
2.5. TIGHT LANGUAGE MODELS 43
We now turn to proving a more general version of Proposition 2.5.6, which would imply its
converse. First, we define the following quantity
˜peosptqdef“PpAt|Ac
1X¨¨¨X Ac
t´1q (2.77)
which can be viewed as the eosprobability at step t, given that eoswas not generated at any
earlier step. One can also show that, when ˜ peosptqis defined, it has the same value as
˜peosptq“ř
ωPΣt´1pLNpωqpLNpeos|ωqř
ωPΣt´1pLNpωq, (2.78)
which one can see as the weighted average probability of terminating at a string of length t.
We can now completely characterize the tightness of an LNM with the following theorem.
Theorem 2.5.3: A sufficient condition for tightness
An LNM is tight if and only if ˜ peosptq“1 for sometorř8
t“1˜peosptq“8 .
Proof. Recall the definition of ˜ peos, as previously defined in Eq. (2.77), is
˜peosptqdef“PpAt|Ac
1X¨¨¨X Ac
t´1q. (2.79)
Case 1. Suppose that ˜ peosptqă1 for allt. Consider the termination probability again:
P˜8č
t“1Ac
t¸
“lim
TÑ8P˜Tč
t“1Ac
t¸
(2.80a)
“lim
TÑ8Tź
t“1PpAc
t|Ac
1X¨¨¨X Ac
t´1q (2.80b)
“lim
TÑ8Tź
t“1p1´rpeosptqq (2.80c)
“8ź
t“1p1´rpeosptqq. (2.80d)
In the above, we have assumed that PpAc
1X¨¨¨X Ac
tqą0 for allt, which is true by assumption that
˜peosptqă1. Hence, by Corollary 2.5.1, Eq. (2.80d) is 0 if and only ifř
trpeosptq“8 .
Case 2. Ifrpeosptq“1 is true for some t“t0, then PpAc
1X¨¨¨X Ac
t0q“0 and hence P`Ş8
t“1Ac
t˘
“0
and such a language model is guaranteed to terminate at t0. ■
The first condition intuitively says that there exists a step tat which the LNM will stop with
probability 1. If the first case of the condition does not hold, the second case can be checked since its
summands will be well-defined (the conditional probabilities in Eq. (2.78) will not divide by 0). We
remark that Theorem 2.5.3 is a generalization of Proposition 2.5.6 since if ˜peosptqis lower-bounded
byfptqwhose series diverges, its own series would also diverge. However, since ˜peosptqinvolves the
computation of a partition function in its denominator, it is most likely intractable to calculate
44 CHAPTER 2. PROBABILISTIC FOUNDATIONS
(Lin et al., 2021a; Lin and McCarthy, 2022). Hence, Proposition 2.5.6 will be the main tool for
determining tightness when we explore concrete language modeling frameworks later.
We have now very thoroughly defined the notion of language model tightness and provided
sufficient and necessary conditions for an LNM or a sequence model to be tight. In the next
sections, we start our exploration of concrete computational models of language, from the very
simple and historically important finite-state language models, their neural variants, to the modern
Transformer architectures. For each of them, we will also individually discuss their tightness results
and conditions.
Chapter 3
Modeling Foundations
The previous chapter introduced the fundamental measure-theoretic characteristics of language
modeling. We will revisit those over and over as they will serve as the foundations on which
subsequent concepts are built.
In this chapter, we turn our attention to modeling foundations, that is, the decisions we face
when we want to build a distribution over strings and learn the appropriate parameters for that
distribution. We first discuss howto parameterize a distribution over strings ( §3.1), what it means
tolearn good parameters, and how this can be done with modern optimization techniques and
objectives ( §3.2).
Continuing our framing of the notes in terms of questions, we will try to address the following:
Question 3.1: Parametrizing a sequence model
How can a sequence model be parameterized?
We introduce a more formal definition of a “parametrized model” later. For now, you can simply
think of it as a function pθ:Σ˚ÑRdescribed by some free parameters θPΘfrom a parameter
spaceΘ. This means that the values that pθmaps its inputs to might depend on the choice of the
parameters θ—the presence of parameters in a model, therefore, allows us to fitthem, which in our
context specifically, means choosing them to maximize some objective with respect to data. This
raises the following question:
Question 3.2: Training a model
Given a parameterized model and a dataset, how can model parameters be chosen to reflect
the dataset as well as possible?
We begin with Question 3.1.
45
46 CHAPTER 3. MODELING FOUNDATIONS
3.1 Representation-based Language Models
Most modern language models are defined as locally normalized models. However, in order to define
locally normalized language model, we first define a sequence model pSMpy|yq. Then, we prove
that the specific parameterization used in pSMpy|yqencodes a tight locally normalized language
model. However, as we demonstrated in Example 2.5.1, not all sequence models encode tight locally
normalized language models in the sense of Definition 2.5.1. So far, however, we have only talked
about this process abstractly. For example, we have proven that every language model can be locally
normalized and we have also given necessary and sufficient conditions for when a sequence model
encodes a tight locally normalized language model. In this section, we start making the abstraction
more concrete by considering a very general framework for parameterizing a locally normalized
language model through sequence models pSMpy|yq. We will call this the representation-based
language modeling framework.
In the representation-based language modeling framework, each conditional distribution in a
sequence model pSMpy|yqdirectly models the probability of the next symbol yPΣgiven the
context y—in other words, it tells us how likely yis to appear in the context of y.1For example,
given the string y““Papa eats caviar with a” , we would like pLNpy|yqto capture that “spoon” is
more likely than “fork” At the same time, since eating caviar with a fork is technically possible, we
would also like pLNpy|yqto capture that “fork” is likelier than, for example, “pencil” .
However, it is not a-priori clear how we should model pSMpy|yqconcretely. We want to define
a function that can map contexts yto a distribution over possible continuations ywith the caveat
that this distribution can be easily adjusted, i.e., we can optimize its parameters with some objective
in mind (cf. §3.2). We will do this by adopting a very general idea of defining pSMpy|yqin terms
of similarity between representations that represent the symbol yand the context y. The more
compatible the symbol yis with the context y, the more probable it should be. Intuitively, going from
the example above, this means that “spoon” should be more similar to “Papa eats caviar with a”
than “fork” should be, and that should still be more similar than “pencil” . On the other hand,
notice that this also means that “spoon” and“fork” should be closer together than any of them to
“pencil” .
One possibility for doing this is by embedding individual symbols yand all possible contexts y
as vectors in a Hilbert space, i.e., a complete vector space endowed with an inner product. Once we
embed the symbols and contexts in such a space, we can talk about how similar they are. We will first
describe how this can be done abstractly §3.1.1 and then discuss how exactly vector representations
can be used when defining discrete probability distributions over the symbols in §3.1.3 by taking into
account the notion of similarities between vectors. We discuss methods for learning representations
later in this chapter ( §3.2) and in Chapter 5.
3.1.1 Vector Space Representations
It is not immediately obvious how to measure the similarity or compatibility between two symbols,
two contexts or a symbol and a context. However, such a notion is required as part of our intuitive
desiderata for pSMpy|yq. We begin by stating an important guiding principle, which we describe in
detail next and use heavily throughout the rest of the notes.
1Unless explicitly stated otherwise, we use the phrase “in the context of” to imply given prior context—i.e.,
when discussing probability distributions, this refers to the distribution pSMpyt|yătqwithy“yt. We will also see
examples of models which specify the conditional probabilities in terms of symbols that do not necessarily appear
before the current one.
3.1. REPRESENTATION-BASED LANGUAGE MODELS 47
Principle 3.1.1: Representation Learning
Thegood representation principle states that the success of a machine learning model
depends—in great part—on the representation that is chosen (or learned) for the objects that
are being modeled. In the case of language modeling, the two most salient choice points are
the representations chosen for the symbols, elements of Σ, and the representations chosen for
the contexts, elements of Σ˚.
Learning vector representations from data where individual entities are represented in some
representation space (i.e., a Hilbert space) has a rich history in NLP and machine learning in
general (Bengio et al., 2013).
To discuss the representations of symbols and strings more formally, we first introduce the notion
of aHilbert space , which leads us to a useful geometric manner to discuss the similarity and
compatibility of symbols and contexts. We first start with some more basic definitions. A vector
space over a field Fis a set Vtogether with two binary operations that satisfy certain axioms. The
elements of Fare often referred to as scalars and the elements of Vasvectors . The two operations
in the definition of a vector space are the addition of vectors and scalar multiplication of vectors .
Definition 3.1.1: Vector space
Avector space over a field Fis a set Vtogether with two binary operations that satisfy the
following axioms:
1.Associativity of vector addition: for all v,u,qPV
pv`uq`q“v`pu`qq (3.1)
2.Commutativity of vector addition: for all v,uPV
v`u“u`v (3.2)
3.Identity element of vector addition: there exists 0PVsuch that for all vPV
v`0“v (3.3)
4.Inverse elements of vector addition: for every vPVthere exists a ´vPVsuch that
v`p´vq“0 (3.4)
5.Compatibility of scalar multiplication with field multiplication: for all vPVand
x,yPF
xpyvq“pxyqv (3.5)
6.Identity element of scalar multiplication: for all vPV
1v“v (3.6)
where 1 is the multiplicative identity in F.
48 CHAPTER 3. MODELING FOUNDATIONS
7.Distributivity of scalar multiplication with respect to vector addition: for all xPF
and all u,vPV
xpv`uq“xv`xu (3.7)
8.Distributivity of scalar multiplication with respect to field addition: for all x,yPF
and all vPV
px`yqv“xv`yv (3.8)
In almost all practical cases, Fwill be RandVwill be RDfor someDPN.
An important characteristic of a vector space is its dimensionality , which, informally, corre-
sponds to the number of independent directions— basis vectors —in the space. Any vPVcan be
expressed as a linear combination of theDbasis vectors. The coefficients of this linear combination
can then be combined into a D-dimensional coordinate vector inFD. Vector spaces, therefore,
allow us to talk about their elements in terms of their expressions with respect to the basis vectors.
Inner product spaces additionally define an inner product , mapping pairs of elements of the vector
space to scalars. More formally, it is a vector space together with a map x¨,¨y(the inner product)
defined as follows.
Definition 3.1.2: Inner product space
Aninner product space is a vector space Vover a field Fcoupled with a map
x¨,¨y:VˆVÑF (3.9)
such that the following axioms hold
1.Conjugate symmetry : for all v,uPV
xv,uy“xu,vy (3.10)
wherexdenotes the conjugate of the element xPF.
2.Linearity in the first argument: for all v,u,zPVandx,yPF
xxv`yu,zy“xxv,zy`yxu,zy (3.11)
3.Positive-definiteness : for all v‰0
xv,vyą0 (3.12)
Inner products are often defined such that they capture some notion of similarity of the vectors
inV. We will use this when formally defining pSMpy|yqin§3.1.2.
Every inner product on a real or complex vector space induces a vector norm defined as follows.
3.1. REPRESENTATION-BASED LANGUAGE MODELS 49
Definition 3.1.3: Norm
Given a vector space VoverRorCand an inner product x¨,¨yover it, the norm induced by
the inner product is defined as the function ∥¨∥:VÑRě0where
∥v∥def“a
xv,vy. (3.13)
A Hilbert space is then an inner product space in which all sequences of elements satisfy a useful
property with respect to the norm defined by the inner product: every convergent series with respect
to the norm converges to a vector in V.
Definition 3.1.4: Hilbert space
AHilbert space is an inner product space that is complete with respect to the norm defined
by the inner product. An inner product space is complete with respect to the norm if every
Cauchy sequence (an absolutely convergent sequence, i.e., a sequence whose elements become
arbitrarily close to each other) converges to an element in V. More precisely, an inner product
space is complete if, for every series
8ÿ
n“1vn (3.14)
such that8ÿ
n“1∥vn∥ă8, (3.15)
it holds that8ÿ
n“1vnPV. (3.16)
Note that even if an inner product space Vis not necessarily a Hilbert space, Vcan always be
completed to a Hilbert space.
Theorem 3.1.1: Completion theorem for inner product spaces
Any inner product space can be completed into a Hilbert space.
We omit the proof for this theorem. More precisely, the inner product space can be completed
into a Hilbert space by completing it with respect to the norm induced by the inner product on the
space. For this reason, inner product spaces are also called pre-Hilbert spaces.
To motivate our slightly more elaborate treatment of representation spaces, we consider an
example of a model which falls under our definition of a representation-based language model but
would be ill-defined if it worked under any space with fewer axioms than a Hilbert space.
50 CHAPTER 3. MODELING FOUNDATIONS
Space Utility
Vector space A space in which representations of symbols and string live. It
also allows the expression of the vector representations in terms
of the basis vectors.
Inner product space Defines an inner product, which defines a norm and can measure
similarity.
Hilbert space There are no “holes” in the representation space with respect to
the defined norm, since all convergent sequences converge into V.
Table 3.1: The utility of different spaces introduced in this section.
Example 3.1.1: A series of representations
Recurrent neural networks are a type of neural network that sequentially process their input
and compute the output (context representation) at time step tbased on the output at time
stept´1:ht“fpht´1,ytq. A formal definition of a recurrent neural network, which we
provide in §5.1.2, is not required at the moment. However, note that a recurrent neural
network with one-dimensional representations hcould, for example, take the specific form
ht“1
2ht´1`1
ht´1(3.17)
withh0“2.
Suppose we chose the inner product space Qover the field Qfor our representation space.
All elements of the sequence htare indeed rational numbers. However, the limit of the
sequence, which can be shown to be?
2, isnotin the inner product space! This shows
thatQis not a Hilbert space and that we must, in full generality, work with Hilbert spaces
whenever we are dealing with possibly infinite sequences of data. The reason this is especially
relevant for language modeling is the need to consider arbitrarily long strings (contexts), whose
representations we would like to construct in a way similar to Eq. (3.17). Such representations
can, therefore, approach a limiting representation outside the space whenever the representation
space does not satisfy the axioms of a Hilbert space.
A summary of the utilities of the three algebraic spaces introduced in this subsection is summarized
in Tab. 3.1.
Representation Functions
We can now introduce the notion of a general representation function.
Definition 3.1.5: Representation function
LetSbe a set and Va Hilbert space over some field F. Arepresentation function f for
the elements of Sis a function of the form
f:SÞÑV. (3.18)
3.1. REPRESENTATION-BASED LANGUAGE MODELS 51
The dimensionality of the Hilbert space of the representations, D, is determined by the modeler.
In NLP,Dusually ranges between 10 to 10000.
Importantly, in the case that Sis finite, we can represent a representation function as a matrix
EPR|S|ˆD(assuming V“RDwhere thenthrow corresponds to the representation of the nth
element of S. This method for representing fis both more concise and will be useful for integrating
the symbol representation function into a model, where matrix multiplications are often the most
efficient way to implement such functions on modern hardware.
This is the case for the representations of the individual symbols yfrom Σ, where the representa-
tion function, which we will denote as ep¨q, is implemented as a lookup into the embedding matrix
EPR|Σ|ˆD, i.e., epyq“Ey.2In this case, we will also refer to ep¨qas the embedding function.
Definition 3.1.6: Symbol embedding function
Let Σ be an alphabet. An embedding function e p¨q:ΣÑRDis a representation function
of individual symbols yPΣ.
The representations epyqare commonly referred to as embeddings , but, for consistency, we
will almost exclusively use the term representations in this text. Let us first consider possibly the
simplest way to represent discrete symbols with real-valued vectors: one-hot encodings .
Example 3.1.2: One-hot encodings
Letn:ΣÑ␣
1,...,|Σ|(
be a bijection (i.e., an ordering of the alphabet, assigning an index
to each symbol in Σ). A one-hot encoding J¨Kis a representation function which assigns the
symbolyPΣ thenpyqthbasis vector:
JyKdef“dnpyq, (3.19)
where here dnis thenthcanonical basis vector, i.e., a vector of zeros with a 1 at position n.
While one-hot encodings are an easy way to create vector representations of symbols, they have
a number of drawbacks. First, these representations are relatively large—we have D“|Σ|—and
sparse , since only one of the dimensions is non-zero. Second, such representations are not ideal
for capturing the variation in the similarity between different words. For example, the cosine
similarity—a metric we will motivate in the next section for measuring the similarity between symbol
representations—between symbols’ one-hot encodings is zero for all non-identical symbols. Ideally,
we would like symbol representations to encode semantic information, in which case, a metric such
as cosine similarity could be used to quantify semantic similarity. This motivates the use of more
complex representation functions, which we subsequently discuss.
While most systems use this standard way of defining individual symbol representations using the
embedding matrix, the way that the context is encoded (and what even is considered as context) is
really the major difference between the different architectures which we will consider later. Naturally,
since the set of all contexts is infinite, we cannot simply represent the representation function with
a matrix. Rather, we define the representation of a context ythrough an encoding function.
2Here, we use the notation Eyto refer to the lookup of the row in Ecorresponding to y.
52 CHAPTER 3. MODELING FOUNDATIONS
Definition 3.1.7: Context encoding function
Let Σ be an alphabet. A context encoding function encp¨q:Σ˚ÑRDis a representation
function of strings yPΣ˚.a
aNote that, to be completely consistent, the encoding function should be defined over the set´
ΣYtbosu¯˚
to allow for the case when y0“bos. However, unlike eos, we do not necessarily require bosin any formal
setting, which is why we leave it out. We apologize for this inconsistency.
We will refer to encpyqas the encoding ofyPΣ˚. In the general framework, we can simply
consider the encoding function encto be a black box—however, a major part of Chapter 5 will
concern defining specific functions enc and analyzing their properties.
With this, we now know how we can represent the discrete symbols and histories as real-valued
vectors. We next consider how to use such representations for defining probability distributions over
the next symbol.
3.1.2 Compatibility of Symbol and Context
Inner products naturally give rise to the geometric notion of angle, by giving us the means to
measure the similarity between two representations. Concretely, the smaller the angle between the
two representations is, the more similar the two representations are. In a Hilbert space, we define
the cosine of the angle θbetween the two representations
cospθqdef“xu,vy
∥u∥∥v∥. (3.20)
The Cauchy–Schwartz inequality immediately gives us that cospθq P r´ 1,1ssince´∥u∥∥v∥ď
xu,vyď∥u∥∥v∥. Traditionally, however, we take the unnormalized cosine similarity as our measure
of similarity, which simply corresponds to the inner product of the Hilbert space.
Given a context representation encpyq, we can compute its inner products with all symbol
representations epyq:
xepyq,encpyqy. (3.21)
which can be achieved simply with a matrix-vector product:
Eencpyq. (3.22)
EencpyqPR|Σ|, therefore, has the nice property that each of the individual entries corresponds to
the similarities of a particular symbol to the context y. For reasons that will become clear soon, the
entries of the vector Eencpyqare often called scores orlogits . This brings us almost to the final
formulation of the probability distribution pSMpy|yq.
IfEencpyqencodes similarity or compatibility, then a natural way to model the probability
distribution pSMpy|yqwould be as proportional to the inner product between epyqandencpyq.
However, the inner product xepyq,encpyqymay be negative; further, the sum over the similarity
between a context and all tokens is not necessarily 1. To resolve this, we have to introduce the last
piece of the puzzle: transforming Eencpyqinto a valid discrete probability distribution by using a
projection function.
3.1. REPRESENTATION-BASED LANGUAGE MODELS 53
3.1.3 Projecting onto the Simplex
In the previous subsections we discussed how to encode symbols and contexts in a Hilbert space and
how an inner product gives us a natural notation of similarity between a potentially infinite number
of items. We can now finally discuss how to create the conditional distribution pSMpy|yq, i.e., how
we can map the real-valued Eencpyqthat encodes symbol–context similarities to a valid probability
distribution—a vector on the probability simplex .
Projection Functions: Mapping Vectors onto the Probability Simplex
pSMpy|yqis acategorical distribution with|Σ|categories, i.e., a vector of probabilities whose
components correspond to the probabilities of individual categories. Perhaps the simplest way to
represent a categorical distribution is as a vector on a probability simplex.
Definition 3.1.8: Probability Simplex
Aprobability simplex ∆D´1is the set of non-negative vectors RDwhose components sum
to 1:
∆D´1def“#
xPRD|xdě0,d“1,...,D andDÿ
d“1xd“1+
(3.23)
So far, we have framed pSMas a function assigning the conditional distribution over yto each
string y. The definition of a simplex means that we can more formally express pSMas aprojection
from the Hilbert space of the context representations to ∆|Σ|´1, i.e.,pSM:VÑ∆|Σ|´1. Yet all
we have discussed so far is creating a vector Eencpyqthat encodes symbol–context similarities—
Eencpyqis not necessarily on the probability simplex ∆|Σ|´1. To address this issue, we turn to
projection functions:
Definition 3.1.9: Projection Function
Aprojection function f∆D´1is a mapping from a real-valued Hilbert space RDto the
probability simplex ∆D´1
f∆D´1:RDÑ∆D´1. (3.24)
which allows us to define a probability distribution according to Eencpyq:
pSMpy|yq“f∆|Σ|´1pEencpyqqy (3.25)
Clearly, we still want the projection of Eencpyqonto∆|Σ|´1to maintain several attributes of
the original vector—otherwise, we will lose the notion of compatibility that Eencpyqinherently
encodes. However, f∆|Σ|´1must satisfy several additional criteria in order to map onto a valid
point in ∆|Σ|´1. For example, the inner product of two vectors (and consequently Eencpyq) is not
necessarily positive—yet all points in ∆|Σ|´1are positive (see Definition 3.1.8). These characteristics
motivate the use of a projection function that is both monotonic and positive everywhere. Thus,
one clear choice is to base our chosen projection function on the exponential function, i.e.,
f∆|Σ|´1pEencpyqq9exppEencpyqq. (3.26)
54 CHAPTER 3. MODELING FOUNDATIONS
To make a function of the form in Eq. (3.26) a valid projection function, we now simply have to
ensure that the output of f∆D´1sums to 1, which can easily be accomplished by re-normalizing the
vector of exponentiated values by their sum. This brings us to the main star of this subsection: the
softmax.
While we simply motivated its introduction by chasing our goal of ending up on the probability
simplex, the origin of the softmax function goes back to the Boltzmann distribution from statistical
mechanics introduced in the mid-1800s by Boltzmann (1868). It was then studied intensely and
popularized by Gibbs (1902). It was originally introduced as a way to convert the energy function
of the Boltzmann distribution into a probability distribution.3Yet now, for reasons we will see in
this subsection, the softmax is the predominant choice of projection function in machine learning
applications.
Formally, the softmax is often defined in terms of a temperature parameter τas follows.
Definition 3.1.10: Softmax
LetτPR`be the temperature . The softmax at temperature τis the projection function
defined as:
softmaxpxqddef“exp“1
τxd‰
řD
j“1exp“1
τxj‰,ford“1,...,D (3.27)
where the temperature parameter τgives us a mechanism for controlling the entropy of the
softmax function by scaling the individual scores in the input vector before their exponentiation. In
the context of the Boltzmann distribution, it was used to control the “randomness” of the system:
When the temperature is high, the softmax function outputs a more uniform probability distribution
whose probabilities are relatively evenly spread out among the different categories. When the
temperature is low, the softmax function outputs a peaked probability distribution, where the
probability mass is concentrated on the most likely category. In the limit, as we take τto the edge
of the possible values it can assume, the following properties hold:
Theorem 3.1.2: Limiting behavior of the softmax function
lim
τÑ8softmaxpxq“1
D1 (3.28)
lim
τÑ0`softmaxpxq“eargmaxpxq, (3.29)
where eddenotes the dthbasis vector in RD,1PRDthe vector of all ones, and
argmaxpxqdef“min"
d|xd“max
d“1,...,Dpxdq*
, (3.30)
i.e., the index of the maximum element of the vector x(with the ties broken by choosing
the lowest such index). In words, this means that the output of the softmax approaches the
uniform distribution as τÑ8 and towards a single mode as τÑ0`.a
aτÑ0`denotes the limit from above.
3This is precisely the connection we mentioned in Definition 2.4.1.
3.1. REPRESENTATION-BASED LANGUAGE MODELS 55
Proof. Let us first consider the case of τÑ0`. Without loss of generality, let us consider a
2-dimensional vector x“rx1,x2sJ
lim
τÑ0`softmaxpxq1“lim
τÑ0`exppx1
τq
exppx1
τq`exppx2
τq(3.31)
“lim
τÑ0`exppx1
τqexpp´x1
τq`
exppx1
τq`exppx2
τq˘
expp´x1
τq(3.32)
“lim
τÑ0`1
1`exppx2´x1
τq(3.33)
which leads us to the following definition for element-wise values:
lim
τÑ0`expˆx2´x1
τ˙
“$
&
%0,ifx1ąx2
1,ifx1“x2
8,o.w.(3.34)
Then the limit of softmax as τÑ0`is given as
lim
τÑ0`softmaxpxq“$
’&
’%r1,0sJ,ifx1ąx2“1
2,1
2‰J,ifx1“x2
r0,1sJ,o.w.(3.35)
which is equivalent to the argmax operator over x. The proof extends to arbitrary D-dimensional
vectors.
The case of τÑ8 follows similar logic, albeit limτÑ8exp`x2´x1
τ˘
“1 in all cases. Hence, we
get limτÑ8softmaxpxq“1
D1. ■
The second property, specifically, shows that the softmax function resembles the argmax function
as the temperature approaches 0—in that sense, a more sensible name for the function would have
been “soft argmax”. We will most often simply take τto be 1. However, different values of the
parameter are especially useful when sampling or generating text from the model, as we discuss
subsequently.
The output of the softmax is equivalent to the solution to a particular optimization problem,
giving it a variational interpretation.
Theorem 3.1.3: Variational characterization of the softmax
Given a set of real-valued scores x, the following equality holds
softmaxpxq“argmax
pP∆D´1˜
pJx´τDÿ
d“1pdlog pd¸
(3.36)
“argmax
pP∆D´1`
pJx`τHppq˘
(3.37)
This tells us that softmax can be given a variational characterization, i.e., it can be viewed as
the solution to an optimization problem.
56 CHAPTER 3. MODELING FOUNDATIONS
Proof. Eq. (3.36) can equivalently be written as
softmaxpxq“argmax˜
pJx´τDÿ
d“1pdlog pd¸
(3.38)
s.t.ÿ
dpd“1 (3.39)
from which we can clearly see that the Lagrangian of this optimization problem is Λ “pJx´
τřD
d“1pdlog pd`λř
dpd. Taking the derivative of Λ with respect to pd, we see that the optimum
of is reached when
BΛ
Bpd“vd´τplogpd`1q`λ“0 (3.40)
Solving for pdgives uspd“Zexppxi
τq, whereZis the normalizing constant that ensuresř
dpd“1.
This solution is equivalent to performing the softmax operation over x, as desired. ■
Theorem 3.1.3 reveals an interpretation of the softmax as the projection pP∆D´1that has the
maximal similarity with xwhile being regularized to produce a solution with high entropy. Further,
from both Definition 3.1.10 and Eq. (3.36), we can see that softmax leads to non-sparse solutions as
an entry softmax pxqican only be 0 if xd“´8 .
In summary, the softmax has a number of desirable properties for use in machine learning
settings.
Theorem 3.1.4: Desirable properties of the softmax function
The softmax function with temperature parameter τexhibits the following properties.
1.In the limit as τÑ0`andτÑ 8 , the softmax recovers the argmax operator and
projection to the center of the probability simplex (at which lies the uniform distribution),
respectively.
2. softmaxpx`c1q“softmaxpxqforcPR, i.e., the softmax is invariant to adding the
same constant to all coordinates in x.
3.The derivative of the softmax is continuous and differentiable everywhere; the value of
its derivative can be explicitly computed.
4.For all temperatures τPR`, ifxiďxj, then softmaxpxqiďsoftmaxpxqj. In words, the
softmax maintains the rank of x.
Proof. Property 1. is simply a restatement of Theorem 3.1.2. The proof for property 2. can be
shown using simple algebraic manipulation:
softmaxpx`c1qd“expr1
τxd`csřD
j“1expr1
τxj`cs“expr1
τxds¨expc
řD
j“1expr1
τxjs¨expc“softmaxpxqd (3.41)
The derivative of the softmax at position iwith respect to the variable at position jis given by
Bsoftmaxpxqi
xj“δipjq¨exppxiqř
kexppxkq´exppxiq¨exppxjq
př
kexppxkqq2(3.42)
3.1. REPRESENTATION-BASED LANGUAGE MODELS 57
whereδipjqis the Dirac Delta function, defined as δipjq“#
1ifi“j
0else. Clearly, Eq. (3.42) is
continuous. Further, it takes on values for all xPRd. Lastly, property 4. follows from the
monotonicity of the exp function.
■
There are many other valid projection functions that one could choose from. For example,
Martins and Astudillo (2016) introduce the sparsemax , which canoutput sparse distributions:
sparsemaxpxqdef“argmin
pP∆D´1||p´x||2
2 (3.43)
In words, sparsemax directly maps xonto the probability simplex, which often leads to solutions on
the boundary, i.e., where at least one entry of pis 0. Martins and Astudillo (2016) provide a method
for computing the closed form solution of this optimization problem in Alg. 1 of their work. Blondel
et al. (2019) later introduced a framework that encompasses many different projection functions,
which they term regularized prediction functions. Essentially, this framework considers the subset of
projection functions that can be written as:
f∆|Σ|´1pxqdef“argmax
pP∆D´1`
pJx´Ωppq˘
(3.44)
where Ω :RDÑRis regularization term. For certain choices of Ω, there are straightforward
closed-form solutions to Eq. (3.44). For example, as we can see from Eq. (3.36), Eq. (3.44) is
equivalent to the softmax when Ωppq“´ Hppq, meaning we can compute its closed form using
Eq. (3.27). Further, we recover the sparsemax when Ωppq“´|| p||2
2, which likewise has a closed-form
solution. The notion of regularizing pmay be unintuitive at first, but we can view it as trying to
balance out the “suitability” term pJxwith a “confidence” term Ω ppq, which should be smaller
when pis “uncertain.” We point the interested reader to the comprehensive work of Blondel et al.
(2019) for further elaboration.
So why aren’t these other projection functions more widely employed in machine learning
frameworks? First, not all choices of Ω lead to closed-form solutions; further, not all meet the desirable
criterion listed in Theorem 3.1.4. For example, the sparsemax is not everywhere differentiable,
meaning that one could not simply use out-of-the-box automatic differentiation frameworks when
training a model using the sparsemax as its projection function. Rather one would have to specify
its gradient explicitly.
Theorem 3.1.5: Deterivative of the sparsemax function
The derivative of the the sparsemax with respect to its input xis as follows:
Bsparsemaxpxqi
Bxj“#
δij´1
Spxqifi,jPSpxq
0 else(3.45)
Proof. See Martins and Astudillo (2016). ■
To conclude, projection functions, together with symbol representations and the representation
function enc, give us the tools to define a probability distribution over next symbols that encodes
58 CHAPTER 3. MODELING FOUNDATIONS
complex linguistic interactions. We now bring all the components together into the locally normalized
modeling framework in the next section.
3.1.4 Representation-based Locally Normalized Models
With these tools at hand, we now define representation-based locally normalized language models.
Definition 3.1.11: Representation-Based Locally Normalized Model
Letencbe an encoding function. A representation-based locally normalized model is a
model of the following form:
pSMpyt|yătqdef“f∆|Σ|´1pEencpyătqqyt(3.46)
where unless otherwise stated, we assume f∆|Σ|´1“softmax . It defines the probability of an
entire string yPΣ˚as
pLNpyqdef“pSMpeos|yqTź
t“1pSMpyt|yătq (3.47)
wherey0def“bos.
Alternatively, we could also include an additive bias term bas part of the projection func-
tionf∆|Σ|´1in the definition of the conditional distribution pSMpyt|yătq, i.e.,pSMpyt|yătq “
f∆|Σ|´1pEencpyătq`bqyt. However, note that the bias term can be absorbed into the encoding
function enc, meaning that we can assume the form Eq. (3.46) without loss of generality. In
representation-based language models, epyqandencpyqcarry all the necessary information to deter-
mine how probable individual symbols yare given the context y. Therefore, the design choices of
epyqandencpyqare crucial when building language models this way. Indeed, a large portion of the
discussion in the remainder of the notes will center around how to build good representations of the
context and individual symbols.
3.1.5 Tightness of Softmax Representation-based Models
Having introduced representation-based language models, we can now state a very general result
about the tightness of such models. It connects the notion of tightness to the intuition about
the “compatibility” of symbols to the context—namely, the compatibility of the eossymbol to the
context (compared to the compatibility of all other symbols). The compatibility is here captured by
the distance of the representation of the eossymbol to the representation of the other symbols—if
this distance grows slowly enough with respect to t(modulo the norm of the context representation),
the model is tight.
3.1. REPRESENTATION-BASED LANGUAGE MODELS 59
Theorem 3.1.6: Proposition 5.9 in Du et al., 2022
LetpSMbe a representation-based sequence model over the alphabet Σ, as defined in Defini-
tion 3.1.11. Let
sdef“sup
yPΣ∥epyq´epeosq∥2, (3.48)
i.e, the largest distance to the representation of the eossymbol, and
zmaxdef“max
yPΣt∥encpyq∥2, (3.49)
i.e., the maximum attainable context representation norm for contexts of length t. Then the
locally normalized model pLNinduced by pSMis tight if
szmaxďlogt. (3.50)
Proof. Letxtpωqbe the random variable that is equal to the tthtoken in an outcome ωPΩ. Then
for an arbitrary tPNand any yPΣt, we have:
Ppxt“eos|xăt“yq“exp”
epeosqJencpyqı
ř
yPΣexp”
epyqJencpyqı (3.51a)
“1
ř
yPΣexprepyqJencpyqs
exprepeosqJencpyqs(3.51b)
“1
1`ř
yPΣexprpepyq´epeosqqJencpyqs(3.51c)
ě1
1`ř
yPΣexpr}epyq´epeosq}2}encpyq}2s(Cauchy–Schwarz) (3.51d)
ě1
1`ř
yPΣexprk}encpyq}2s(3.51e)
“1
1`|Σ|expr}encpyq}2s(3.51f)
Now define zmaxdef“supyPΣt}encpyq}2. We then have that @tPNand@yPΣt:
Ppxt“eos|xăt“yqě1
1`|Σ|exppkzmaxq(3.52)
Now, by Proposition 2.5.6, we have that ifř8
t“11
1`|Σ|exppkzmaxqdiverges, then the language
model is tight. We will show that if we have that DNPNsuch that@těN,kzmaxďlogt, then the
sequence model must be tight.
First, note that limtÑ81
t1`|Σ|t
1“limtÑ81
t`|Σ|“|Σ|Pp0,8q. Hence, by the limit comparison
test, sinceř8
t“11
tdiverges, this meansř8
t“11
1`|Σ|tmust also diverge.
Now, suppose that kzmaxďlogtfor alltěN. This implies that for těNwe have
1
1`|Σ|exppkzmaxqě1
1`|Σ|t, which combined with the above and the comparison test, implies that
60 CHAPTER 3. MODELING FOUNDATIONS
ř8
t“N1
1`|Σ|exppkzmaxqdiverges. This in turn means thatř8
t“11
1`|Σ|exppkzmaxqdiverges. Hence, if
kzmaxďlogtfor alltěNfor someNPN, then the language model is tight.
■
Theorem 3.1.6 is a generalization of the following result from Welleck et al. (2020).
Theorem 3.1.7: Representation-based language models with bounded encodings
are tight
A locally-normalized representation-based language model, as defined in Definition 3.1.11,
with uniformly bounded ||encpyq||p(for somepě1) is tight.
For most of the language models that we consider, encpyqis bound due to the choice of activation
functions. In turn, Eencpyătqis bounded for all y. Further, by the definition of the softmax,
f∆|Σ|´1pEencpyătqqeosąηfor some constant η.
This concludes our investigation of general representation-based models. The next section
discusses learning parametrized models (as a special case, also symbol and context representations).
3.2. ESTIMATING A LANGUAGE MODEL FROM DATA 61
3.2 Estimating a Language Model from Data
Thelanguage modeling task refers to any attempt to estimate the parameters4of a modelpMof
the ground-truth probability distribution over natural language strings pLMusing data D“typnquN
n“1,
where we assume samples ypnqwere generated according to pLM. This task is often treated as an
optimization problem. Here we will discuss the various components of this optimization problem,
primarily the objective and the algorithm used to perform optimization. Note that the material
covered here corresponds to what is colloquially referred to as pre-training. The learning paradigm
for fine-tuning a language model for a downstream task will be covered later in the course.
3.2.1 Data
In this course, we consider objectives that are defined in terms of data D. Therefore, we will first
discuss the nature of this data which, more precisely, is a corpus of texts. Following the notation
used throughout the rest of these notes, let Σ be an alphabet. A corpus D“typnquN
n“1ĂΣ˚is a
collection of Nstrings. We will use the terms corpus and dataset interchangeably throughout this
section. We make the following assumption about the data-generating process of D:
Assumption 3.2.1: Independently and identically distributed assumption
The strings ypnqin our corpus Dare generated independently and identically distributed
(i.i.d.) by some unknown distribution pLM.
Note that ypnqare strings of an arbitrary length; they can be single words, sentences, paragraphs,
or even entire documents depending on how we choose Σ. For example, often our models’ architectural
designs make them unable to process document-length strings efficiently, e.g., they might not fit
into a context window that can be reasonably processed by a transformer language model; we will
elaborate on this statement in our discussion of transformers in §5.3. Thus in practice, we often
chunk documents into paragraphs that we treat as separate data points.5This means that our
model may not be able to learn properties of language such as discourse structure.
3.2.2 Language Modeling Objectives
Similarly to many other machine learning tasks, we can cast our problem as the search for the best
modelpMof the ground-truth distribution over strings pLM. In order to make this search tractable,
we must limit the models pMthat we consider. Explicitly, we make the following assumption:
Assumption 3.2.2: Parametrized model
pLMis a member of the parameterized family of models tpθ|θPΘu, the set of all distributions
representable by parameters θin a given parameter space Θ.
4Most of this course focuses on the parametric case, i.e., where pMis governed by a set of parameters θ. However,
we will briefly touch upon various non-parametric language models.
5This practice technically breaks Assumption 3.2.1, yet the negative (empirically-observed) effects of this violation
are minimal and perhaps outweighed by the additional data it allows us to make use of.
62 CHAPTER 3. MODELING FOUNDATIONS
As concrete examples, θcould be the conditional probabilities in a simple, standard n-gram model
for a given prefix of size n´1, i.e., θisn´1 simplices of size |Σ|.6As another example, θcould be
the weights of a neural network; the set Θwould then cover all possible valid weight matrices that
could parameterize our model.
Assumption 3.2.2 implies that we can equivalently write pLMaspθ‹for certain (unknown) pa-
rameters θ‹PΘ.7Further, an arbitrary model pMfrom this hypothesis space with parameters
θcan be written as pθ; we will use this notation for the remainder of the chapter to make the
parameterization of our distribution explicit. We now turn to the general framework for choosing
the best parameters θPΘso that our model pθserves as a good approximation of pθ‹.8
General Framework
We search for model parameters pθPΘsuch that the model induced by those parameters maximizes
a chosen objective, or alternatively, minimizes some loss function ℓ:ΘˆΘÑRě0. This loss can
be used to measure the quality of this model as an approximation to pθ‹. In simple math, we search
for the solution.
pθdef“argmin
θPΘℓpθ‹,θq (3.53)
where our loss function is chosen with the following principle in mind
Principle 3.2.1: Proximity Principle
We seek a model pθthat is “close” to pθ‹.
That is, we choose our loss function to be a measure Mof the difference between a distribution
parameterized by θand one parameterized by the true θ‹, i.e., those of our ground-truth distribution.
Yet we are immediately faced with a problem: computing an arbitrary Mbetween θandθ‹(or at
least the distributions induced by these sets of parameters) requires knowledge of both, the latter
for which we only have samples ypnqPD. We will therefore use our corpus Das an approximation
topθ‹, which is typically implemented by representing Das an empirical distribution—a collection
of Dirac Delta functions—which we will denote as Ăpθ‹. Formally, we define
Ăpθ‹pyqdef“1
NNÿ
n“1δypnqpyq (3.54)
where the Dirac Delta function δx1pxq“#
1ifx“x1
0elseis essentially a point mass with all probability
onx1. We can decompose this definition over symbols in our strings as well. I.e., we can compute
Ăpθ‹pyt|yătq“1
NyătNÿ
n“1δypnq
t|ypnq
ătpyt|yătq (3.55)
6One might be tempted to assume we only need |Σ|´1 parameters per simplex, but we condition over Σclasses
per prefix position.
7We discuss the implications of the case that pLMRtpθ|θPΘulater in this section.
8The modeling paradigms that we will discuss in this section are predominantly generative, i.e., these models try
to learn the underlying distribution of the data rather than the boundaries between different classes or categories.
The implication is that parameter estimation in language modeling typically makes use of unannotated text data, and
is therefore sometimes referred to as self-supervised .
3.2. ESTIMATING A LANGUAGE MODEL FROM DATA 63
whereNyătdef“řN
n“11typnq
ăt“yătu. Note that we can likewise define Eq. (3.55) in terms of
the one-hot encodings of symbols, i.e., using the definition in Example 3.1.2: Ăpθ‹p¨ |yătq “
1
NyătřN
n“1Jypnq
tK 1typnq
ăt“yătu. In fact, the empirical distribution is often also referred to in
machine learning as the one-hot encoding of a dataset.
Now that we are equipped with methods for representing both pθ‹andpθ, we can define a loss
function for approximating pθ‹usingpθ.
Cross-Entropy. A natural choice for a loss function is cross-entropy, a measure of the difference
between two probability distributions, which has its roots in information theory (Shannon, 1948b).
Specifically, in Eq. (3.53), we take ℓpθ‹,θq“HpĂpθ‹,pθqwhere the definition of the cross-entropy H
between distributions p1(with support Y) andp2is as follows:
Hpp1,p2q“´ÿ
yPYp1pyqlogp2pyq (3.56)
Further, most of the models that we will encounter in this course are locally normalized. Thus, it is
more common to see cross-entropy expressed as
Hpp1,p2q“´ÿ
yPYTÿ
t“1p1pypnq
tqlogp2pyt|yătq. (3.57)
Note that cross-entropy is not symmetric, i.e., H pp1,p2q‰Hpp2,p1q. To motivate cross-entropy
as a loss function, as well as the intuitive difference between the two argument orderings, we turn to
coding theory, a sub-field of information theory. In words, the cross-entropy between two probability
distributions is the expected number of bits needed to encode an event yPYfromp1when using
the optimal encoding scheme corresponding to distribution p2. Importantly, the optimal encoding
scheme for p1uses logp1pyqbits to encode an event ythat occurs with probability p1pyq, implying
that the minimal cross-entropy is achieved when p1“p2. This characteristic of cross-entropy
motivates another metric: the KL divergence DKL.
KLDivergence. Adivergence measure is a measure of statistical distance9between two
probability distributions. The KL divergence is defined as:
DKLpp1||p2q“ÿ
yPYp1pyqlogp2pyq´p1pyqlogp1pyq (3.58)
TheKLdivergence can intuitively be viewed as the cross-entropy shifted by the expected number
of bits used by the optimal encoding scheme for p1, i.e., it is the additional number of expected
bits needed to encode events from p1when using our encoding scheme from p2. Indeed, taking
ℓpθ‹,θq“DKLpĂpθ‹||pθqshould lead to the same solution as taking ℓpθ‹,θq“HpĂpθ‹,pθqbecause
theĂpθ‹pyqlogĂpθ‹pyqterm is constant with respect to model parameters θ.
9Divergences are not technically distances because they are not symmetric, i.e., it may be the case for divergence
measureDand probability distributions pandqthatDpp||qq‰Dpp||qq. However, they do meet the criteria that
Dpp||qqě0@p,qandDpp||qq“0ðñp“q.
64 CHAPTER 3. MODELING FOUNDATIONS
Relationship to Maximum Likelihood Estimation
An alternative way that we could frame our search for model parameters pθPΘis in terms of
data likelihood. Formally, the likelihood of the corpus Dunder the distribution pθis the joint
probability of all ypnq:
Lpθq“Nź
n“1pθpypnqq. (3.59)
The principle of maximum likelihood then dictates:
Principle 3.2.2: Maximum Likelihood
The optimal parameters for a model are those that maximize the likelihood of observing the
given data under that model. Formally:
pθMLEdef“argmax
θPΘLpθq (3.60)
Note that in practice, we typically work with the log-likelihood Lpθq“logLpθqrather than
the likelihood for a number of reasons, e.g., it is convex and more numerically stable given the small
probabilities we encounter when using Land the finite precision of the computing frameworks that
we employ. Since logis a monotonically increasing function, this would not change the solution
to Eq. (3.60). Further, as is the case with Eq. (3.57), we decompose our loss over symbol-level
distributions.
Notably, in our setting, finding parameters that maximize data log-likelihood is equivalent to
finding those that minimize cross-entropy. We show this equivalence below.
Proposition 3.2.1
The optimal parameters under Eq. (3.60) are equivalent to the optimal parameters when
solving for Eq. (3.53) with the cross-entropy loss between the empirical distribution Ăpθ‹and
the modelpθ.
Proof. Under the standard practice of taking 0 logp0q“0, the only elements of Ythat make a
nonzero contribution to H pĂpθ‹,pθqare sequences in the support of Ăpθ‹, making summing over Y
equivalent to summing over D:
HpĂpθ‹,pθq“´ÿ
yPΣ˚Ăpθ‹pyqlogpθpyq (3.61)
“´ÿ
yPΣ˚1
NNÿ
n“1δypnqpyqlogpθpyq (3.62)
“´ÿ
yPΣ˚1
N1typnqPDulogpθpyq (3.63)
9´ÿ
yPDlogpθpyq (3.64)
“´Lpθq (3.65)
3.2. ESTIMATING A LANGUAGE MODEL FROM DATA 65
Thus, we can see that the objectives are equivalent, up to a multiplicative constant that is independent
of model parameters. ■
The equivalence of cross-entropy, DKLdivergence , and maximum likelihood as learning objectives
provides intuition about our many goals when learning pθ: (1) we want a close (w.r.t. a given metric)
approximation of the data-generating distribution, and (2) this approximation should place high
probability on samples of real language data.
Properties of pθunder the cross-entropy loss. Assumption 3.2.2 may feel quite strong, as it
implies we know a great deal about the nature of pLM. However, it allows us to prove the optimality
ofppθunder certain conditions.
Theorem 3.2.1: Maximum likelihood estimate is consistent
Consider that our loss function ℓpθ‹,θq“HpĂpθ‹,pθq(or equivalently that ℓpθ‹,θq“DKLpĂpθ‹||
pθq). Given Assumption 3.2.1 and that the minimizer ppθof HpĂpθ‹,pθqis unique, then under
certain (quite strong) regularity conditions on tpθ|θPΘu,pθis a consistent estimator, i.e., it
converges to θ‹in probability as nÑ8 .
Arguably, in practice, Assumption 3.2.2 does not hold; we often make some incorrect modeling
assumptions. Naturally, this raises the following question: If we misspecify the family of models
thatpLMbelongs to, i.e., pLMRtpθ|θPΘu, then is our optimal model ppθunder the cross-entropy
loss at all meaningful? Fortunately, the answer here is yes. In this case, we can interpret ppθas a
projection of pLMonto the manifold of parametric models tpθ|θPΘu. This projection is formally
known as an information projection (Nielsen, 2018), which while we do not cover formally here,
we can intuit as a mapping of pLMonto its “closest” point in tpθ|θPΘu. In this setting, using
different metrics Mleads to different definitions of closeness, which in turn means that optimal
models under different Mexhibit different properties.
Potential drawbacks of cross-entropy loss. A closer inspection of Eq. (3.56) reveals that,
when we use H pĂpθ‹,pθqas our loss function, pθmust put probability mass on allsamples ypnqin
the support of Ăpθ‹; otherwise, our loss is infinite. Since the model is not explicitly penalized for
extraneous coverage, it will thus resort to placing mass over all of Σ˚to avoid such gaps;10this is
sometimes referred to as mean-seeking behavior. In practice, this means that sequences of symbols
that one might qualitatively describe as gibberish are assigned nonzero probability by pθ. It is
unclear whether this is a desirable property under a language model. While perhaps useful when
using such a model to assign probabilities to strings—in which case, we might be more interested
in how strings’ probabilities rank against each other and may not want to write off any string as
completely improbable—it could prove problematic when generating strings from these models, a
topic covered later in this course.
Teacher Forcing. The loss functions that we have considered thus far are all based on our
model’s predictions conditioned on prior context. Here we are faced with a choice during training:
10This behavior can also be (at least partially) attributed to the softmax used to transform model outputs into a
probability distribution over symbols. Since the softmax maps to the interior of the probability simplex, no symbol
can be assigned a probability of exactly 0.
66 CHAPTER 3. MODELING FOUNDATIONS
we could either use the model’s predictions from the previous time step(s) pθp¨|pyătq(e.g., the
most probable symbols) as the prior context or use the ground-truth prior context from our data
pθp¨|yătq. The latter method is often referred to as teacher forcing : Even if our model makes
an incorrect prediction at one step of training, we intervene and provide the correct answer for it
to make subsequent predictions with.
From a theoretical perspective, training with the cross-entropy loss mandates that we should
use the teacher-forcing approach since each conditional distribution is defined with respect to the
ground-truth context; this is elucidated, for example, in Eq. (3.57). Yet such meticulous guidance
can lead to poor performance in tasks where the model is required to accurately predict an entire
sequence of symbols on its own. For example, in language generation, since the model is not
exposed to its own generations during training, small errors in predictions can compound, leading
to degenerate text. This problem is known as exposure bias . Only the other hand, using previous
model outputs in order to make subsequent predictions can lead to serious instability during training,
especially if implemented from the start of training. Methods for alleviating exposure bias have
been proposed with more stable training dynamics, such as scheduled sampling Bengio et al. (2015),
which we discuss in §3.2.2.
Alternative Objectives
Masked Language Modeling. So far, our parameter estimation strategies have made use of
the decomposition of pθpyqinto individual symbol probabilities, conditioned on prior symbols, i.e.,
pθpyq“śT
t“1pθpyt|yătq. In other words, we do not give a model both sides of a symbol’s context
when asking it to estimate the probability distribution over that symbol. While this paradigm might
be more realistic when using a language model for tasks such as generation—for which we may want
to generate outputs sequentially to mimic human language production—access to both sides of a
symbol’s context could be critical when using the model for tasks such as acceptability judgments
or classification. This motivates the use of an alternative objective for parameter estimation.
Similarly to the maximum likelihood objective in Eq. (3.59), we can choose model parameters by
optimizing for the per-symbol log-likelihood of a dataset D, albeit in this case, using both sides of
the symbol’s context:
LMLMpθq“Nÿ
n“1Tÿ
t“1logpθpypnq
t|ypnq
ăt,ypnq
ątq (3.66)
Eq. (3.66) is sometimes referred to as the pseudo(log)likelihood (Besag, 1975), since it gives us an
approximation of the true log-likelihood, i.e.,řT
t“1logpθpyt|yăt,yątq«logpθpyq. Pseudolikelihood
has its origins in thermodynamics, where it was used as an approximate inference technique for
parameter estimation in Ising models. In such situations, computing pθpyt|y‰tqoften proved
computationally easier than computing the exact set of of conditional probabilities whose product
equaled the marginal.
Using Eq. (3.66) as a model’s training objective is also motivated by psychological tests of
language understanding—specifically, the Cloze (Taylor, 1953) task in psychology, in which the
goal is to predict the omitted symbol from a piece of text that constitutes a logical and coherent
completion. For example, in the string
3.2. ESTIMATING A LANGUAGE MODEL FROM DATA 67
Example 3.2.1: The Close task
The students [MASK] to learn about language models.
we predict want orlikewith high probability for the [MASK] position. When used as an objective
in NLP, estimating the probability distribution over symbols at the masked position is referred
to as masked language modeling; BERT (Devlin et al., 2019) is one well known example of a
masked language model. In practice, typically only the distributions over symbols at a percentage of
randomly-chosen positions in Dare estimated during training. As mentioned in §2.5, a model whose
parameters are estimated with the masked language modeling objective is not a valid language
model in the sense of Definition 2.3.7 because it does not provide a valid distribution over Σ˚. Yet,
masked language models have become increasingly popular as base models for fine-tuning on certain
downstream tasks, where they sometimes lead to superior performance over standard language
models.
Other Divergence Measures. From a given hypothesis space (see Assumption 3.2.2), the
distribution that minimizes a given divergence measure with pθ‹exhibits certain properties with
respect to how probability mass is spread over the support of that distribution. For example, the
modelpθthat minimizes DKLppθ‹||pθqexhibits mean-seeking behavior, as discussed earlier in
this section. These properties have been studied in depth by a number of works (Minka, 2005;
Theis et al., 2016; Husz´ ar, 2015; Labeau and Cohen, 2019). The implication of these findings is
that, depending on the use case for the model, other divergence measures may be better suited
as a learning objective. For example, prior work has noted frequency biases in models estimated
using the standard log-likelihood objective, i.e., these models exhibit an inability to accurately
represent the tails of probability distributions (Gong et al., 2018). This is particularly relevant in
the case of language modeling, as symbol-usage in natural language tends to follow a power-law
distribution (Zipf, 1935). Consequently, when we care particularly about accurately estimating the
probability of rare words, we may wish to instead use a loss function that prioritizes good estimation
of probability distribution tails. On the other hand, in the case of language generation, we may
desire models that only assign probability mass to outputs that are highly-likely according to pθ‹,
even if this means assigning probabilities of 0 to some outcomes possible under pθ‹. In other words,
we may want a model with mode-seeking behavior, which is characteristic of models trained to
minimizeDKLppθ||pθ‹q. However, there are a number of computational issues with using other
divergence measures—such as general power divergences, reverse DKLdivergence, and total variation
distance—for training neural probabilistic models over large supports, making them difficult to
work with in practice. For example, we can compute a Monte Carlo estimate of the forward DKL
divergence simply by using samples from pθ‹, which is exactly what we have in our dataset. However
an unbiased estimator of the reverse DKLdivergence would require the ability to query pθ‹for
probabilities, which we do not have.
Scheduled Sampling and Alternative Target Distributions. Scheduled sampling (Bengio
et al., 2015) is an algorithm proposed with the goal of alleviating exposure bias: after an initial period
of training using the standard teacher forcing approach, some percentage of the models’ predictions
are conditioned on prior model outputs, rather than the ground-truth context. However, under this
algorithm, pθdoes not lead to a consistent estimator of θ‹(Husz´ ar, 2015). Other methods likewise
aim to alleviate the discrepancy between settings during parameter estimation and those at inference
68 CHAPTER 3. MODELING FOUNDATIONS
time by specifying an alternative target distribution, for example, one that ranks “higher-quality”
text as more probable than average-quality text. Ultimately, these methods often make use of
techniques developed for reinforcement learning, i.e., the REINFORCE algorithm. These methods
fall under the category of fine-tuning criterion, which are discussed later in this course.
Auxiliary Prediction Tasks. Certain works jointly optimize for an additional objective when
performing parameter estimation. For example, the parameters for BERT were learned using both
the masked language modeling objective as well as a task referred to as next sentence prediction,
i.e., given two sentences, estimating the probability that the second sentence followed the first in a
document. A number of similar auxiliary tasks have subsequently been proposed, such as symbol
frequency prediction or sentence ordering (see Aroca-Ouellette and Rudzicz (2020) for summary).
However, these tasks do not have a formal relationship to language modeling and it is unclear what
their effects are on a model’s ability to serve as a valid probability distribution over strings. They
likely lead to models that no longer fulfill the formal criteria of §2.5.4.
3.2.3 Parameter Estimation
Given a loss function ℓand a parameter space Θfrom which to choose model parameters, we are
now tasked with finding the parameters pθ, i.e., solving Eq. (3.53). For the class of models that
we consider (those parameterized by large neural networks), finding an exact solution analytically
would be impractical, if not impossible. Thus, we must resort to numerical methods, where we
find approximately optimal parameters by iterating over solutions. This is known as parameter
estimation , or more colloquially as training our model.
Here we will review the various components of training a language model from start to finish.
Many of the techniques used for training language models are generally applicable machine learning
techniques, e.g., gradient-descent algorithms. Further, these techniques are constantly evolving and
often viewed as trade secrets, meaning that entities building and deploying models may not reveal
the combination of components that they employed. Thus, we give a more general overview of the
design choices involved in parameter estimation, along with the characteristics common to most
components.
Data Splitting
In any machine learning setting, we may overestimate model quality if we evaluate solely on its
performance w.r.t. the data on which its parameters were estimated. While we can often construct a
model that performs arbitrarily well on a given dataset, our goal is to build a model that generalizes
to unseen data. Thus, it is important to measure the final performance of a model on data that has
had no influence on the choice of model parameters.
This practice can be accomplished simply by splitting the data into several sets. The two basic
data splits are a training set Dtrainand test set Dtest; as the names imply, the training set is used
during parameter estimation while the test set is used for evaluating final performance. When
samples from Dtestcan be found in Dtrain, we call this data leakage . The training set can be further
divided to produce a validation set Dval. Typically, Dvalis not used to define the objective for which
parameters are optimized. Rather, it serves as a check during training for the generalization abilities
of a model, i.e., to see whether the model has started overfitting to the training data. The validation
set can be used, e.g., to determine when to stop updating parameters.
3.2. ESTIMATING A LANGUAGE MODEL FROM DATA 69
Numerical Optimization
From a starting point θ0PΘchosen according to our initialization strategy, we want to find pθin an
efficient manner. This is where numerical optimization algorithms come into play—a precise set
of rules for choosing how to move within Θin order to find our next set of parameters. The output
of a numerical optimization algorithm is a sequence of iterates tθsuT
t“0, with the property that as
TÑ8 we find the minimizer of our objective ℓ. Ideally, even after a finite number of iterations, we
will be sufficiently close to pθ.
The basic algorithm for searching the parameter space for pθfollows a simple formula: starting
fromθ0PΘ, we iteratively compute θ1,θ2,...as
θs`1“θs`update magnitude ˆupdate direction . (3.67)
where the update added to θsto obtain θs`1is intended to move us closer to pθ. Once some
maximum number of updates Sor a pre-defined desideratum has been met, e.g., our loss has not
improved in subsequent iterations, we stop and return the current set of parameters. Many of the
numerical optimization techniques in machine learning are gradient-based, i.e., we use the gradient
of the objective with respect to current model parameters (denoted as ∇θsℓpθsq) to determine our
update direction. Standard vanilla gradient descent takes the form of §3.2.3, where the learning rate
schedule η“xη0,¨¨¨,ηTydetermines the step size of our parameter update in the loss minimizing
direction—there is an inherent trade-off between the rate of convergence and overshooting—and the
stopping criterion Cdetermines whether we can terminate parameter updates before our maximum
number of iterations S. In vanilla gradient descent, we set η“c¨1for some constant cand
Algorithm 1 Gradient descent for parameter optimization.
Input:ℓobjective
θ0initial parameters
ηlearning rate schedule
C:ℓˆΘˆΘÑtTrue,Falseustopping criterion
1.fors“0,¨¨¨,S:
2.θs`1Ðθs´ηs¨∇θℓpθsq
3.ifCpℓ,θs,θs´1q:
4. break
5.return θs
Cpℓ,θs,θs´1q “ 1t|ℓpθsq´ℓpθs´1q| ăϵufor user-chosen ϵ—in words, we stop when the change
in loss between parameter updates is below a chosen threshold. In practice, more sophisticated
learning rate schedules η, e.g., square-root functions of the timestep (Hoffer et al., 2017) or adaptive
functions that take into account model parameter values (Duchi et al., 2011), and stopping criterion
Care employed.
Modern training frameworks rely on backpropagation—also known as reverse-mode automatic
differentiation (Griewank and Walther, 2008)—to compute gradients efficiently (and, as the name
implies, automatically!). In fact, gradients can be computed using backpropogation in the same
complexity as evaluation of the original function. We do not provide a formal discussion of
backpropagation here but see Griewank and Walther (2008) for this material.
Recall that our loss function—and consequently the gradient of our loss function—is defined with
respect to the entire dataset. Vanilla gradient descent therefore requires iterating through all of Dtrain
70 CHAPTER 3. MODELING FOUNDATIONS
in order to determine the direction to move parameters U, which is an incredibly time-consuming
computation for the large datasets employed in modern machine learning settings. Rather, an
optimization algorithm would likely take much less time to converge if it could rapidly compute
estimates of the gradient at each step. This is the motivation behind perhaps the most widely
employed class of optimization algorithms in machine learning: variations of stochastic gradient
descent (SGD), such as mini-batch gradient descent. Explicitly, these algorithms make use of the
fact that ED1i.i.d.„D∇θpθ,D1q“∇θpθ,Dq, where in slight abuse of notation, we use D1i.i.d.„Dto signify
that the multi-set D1consists of random i.i.d. samples from D. Thus we can instead base our loss
ℓ, and consequently U, off of a randomly selected subset of the data.11In practice though, this
sample is taken without replacement, which breaks the i.i.d. assumption. This in turn implies this
our gradient estimates are biased under the mini-batch gradient descent algorithm. However, this
bias does not seem to empirically harm the performance of such optimization strategies. Indeed, an
entire branch of machine learning called curriculum learning focuses on trying to find an optimal
data ordering with which to train models to achieve desirable characteristics such as generalization
abilities. Even when orderings are randomly selected, the chosen ordering can have a large impact
on model performance Dodge et al. (2020).
A number of optimization algorithms have since iterated on SGD, e.g., the momentum algorithm
(Polyak, 1964). In short, the momentum algorithm computes an exponentially decaying moving
average of past gradients, and continues updating parameters in this direction, which can drastically
speed up convergence. A widely-employed optimization algorithm called ADAM (Kingma and Ba,
2015) takes a similar approach. Just as in momentum, it computes update directions using a moving
average (first moment) of gradients, albeit it additionally makes use of the variance of gradients
(second moment) when computing update directions. ADAM is one of the most popular optimization
algorithms used for training large language models in modern ML frameworks.
Parameter Initialization
Our search for (approximately) optimal model parameters must start from some point in the
parameter space, which we denote as θ0. Ideally, starting from any point would lead us to the
same solution, or at least to solutions of similar quality. Unfortunately, this is not the case: both
training dynamics and the performance of the final model can depend quite heavily on the chosen
initialization strategy, and can even have high variance between different runs of the same strategy.
This makes sense at some intuitive level though: depending on the learning algorithm, an initial
starting point can heavily dictate the amount of searching we will have to do in order to find pθ,
and how many local optima are on the route to pθ. Consequently, a poor initial starting point may
lead to models that take longer to train and/or may lead our learning algorithm to converge to
sub-optimal solutions (i.e., an alternative local optimum) (Dodge et al., 2020; Sellam et al., 2022).
This can be the case even when only estimating the final layer of a network, e.g., when building
a classifier by appending a new layer to a pretrained model—a recent, widely-adopted practice in
NLP (Dodge et al., 2020).
Methods for initializing the parameters of neural language models are largely the same as those
11While this logic holds even for samples of size 1 (which is the sample size for standard SGD by definition), basing
updates off of single samples can lead to noisy updates. Depending on resource constraints, batch sizes of a few
hundred are often used, leading to much more stable training (although in the face of memory constraints, larger
batch sizes can be mimicked by accumulating, i.e., averaging, gradients across multiple batches when computing
update directions). Batch size itself is often viewed as a model hyperparameter that can have a significant effect on
model performance.
3.2. ESTIMATING A LANGUAGE MODEL FROM DATA 71
for initializing other neural networks. Perhaps the simplest approach is to randomly initialize all
parameters, e.g., using a uniform or normal random variable generator. The parameters of these
generators (mean, standard deviation, bounds) are considered hyperparameters of the learning
algorithm. Subsequent methods have iterated on this strategy to develop methods that take into
account optimization dynamics or model architectures. One consideration that is particularly
relevant for language models is that the input and output sizes of the embedding layer and the
fully connected layer can be very different; this exacerbate the problem of vanishing or exploding
gradients during training. For example, Glorot, Xavier and Bengio, Yoshua (2010) proposed Xavier
init, which keeps the variance of the input and output of all layers within a similar range in order
to prevent vanishing or exploding gradients; He et al. (2015) proposed a uniform initialization
strategy specifically designed to work with ReLU activation units. Using uniform random variables
during parameter initialization can likewise alleviate the problem of vanishing gradients. While
most deep learning libraries use thoughtfully-selected initialization strategies for neural networks, it
is important to internalize the variance in performance that different strategies can cause.
Early Stopping
As previously discussed, performance on Dtrainis not always the best indicator of model performance.
Rather, even if our objective continues to increase as we optimize over model parameters, performance
on held-out data, i.e., Dtestor even Dval, may suffer as the model starts to overfit to the training
data. This phenomenon inspires a practice called early stopping , where we stop updating model
parameters before reaching (approximately) optimal model parameter values w.r.t. Dtrain. Instead,
we base our stopping criterion Coff of model performance on Dvalas a quantification of generalization
performance, a metric other than that which model parameters are optimized for, or just a general
slow down in model improvement on the training objective.
Early stopping sacrifices better training performance for better generalization performance; in
this sense, it can also be viewed as a regularization technique, a topic which we discuss next. As
with many regularization techniques, early stopping can have adverse effects as well. Recent work
suggests that many models may have another period of learning after an initial period of plateauing
train/validation set performance. Indeed, a sub-field has recently emerged studying the “grokking”
phenomenon (Power et al., 2022), when validation set performance suddenly improves from mediocre
to near perfect after a long period in which it appears that model learning has ceased, or even that
the model has overfit to the training data. Thus, it is unclear whether early stopping is always a
good practice.
3.2.4 Regularization Techniques
Our goal during learning is to produce a model pθthat generalizes beyond the observed data; a model
that perfectly fits the training data but produces unrealistic estimates for a new datapoint is of little
use. Exactly fitting the empirical distribution is therefore perhaps not an ideal goal. It can lead to
overfitting , which we informally define as the situation when a model uses spurious relationships
between inputs and target variables observed in training data in order to make predictions. While
this behavior decreases training loss, it generally harms the model’s ability to perform on unseen
data, for which such spurious relationships likely do not hold.
To prevent overfitting, we can apply some form of regularization.
72 CHAPTER 3. MODELING FOUNDATIONS
Principle 3.2.3: Regularization
Regularization is a modification to a learning algorithm that is intended to increase a model’s
generalization performance, perhaps at the cost of training performance.a
aAdapted from Goodfellow et al. (2016), Ch. 7.
There are many ways of implementing regularization, such as smoothing a distribution towards
a chosen baseline or adding a penalty to the loss function to reflect a prior belief that we may have
about the values model parameters should take on Hastie et al. (2001); Bishop (2006). Further,
many regularization techniques are formulated for specific model architecture: for example, the
count-based smoothing methods used n-gram language models (Ney et al., 1994; Gale and Sampson,
1995). Here we specifically consider the forms of regularization often used in the estimation of neural
language models. Most fall into two categories: methods that try to ensure a model’s robustness
to yet unseen (or rarely seen) inputs—e.g., by introducing noise into the optimization process—or
methods that add a term to our loss function that reflects biases we would like to impart on our
model. This is by no means a comprehensive discussion of regularization techniques, for which we
refer the reader to Ch.7 of Goodfellow et al. (2016).
Weight Decay
A bias that we may wish to impart on our model is that not all the variables available to the model
may be necessary for an accurate prediction. Rather, we hope for our model to learn the simplest
mapping from inputs to target variables, as this is likely the function that will be most robust to
statistical noise.12This bias can be operationalized using regularization techniques such as weight
decay (Goodfellow et al., 2016)—also often referred to as ℓ2regularization. In short, a penalty
for theℓ2norm of θis added to ℓ. This should in theory discourage the learning algorithm from
assigning high values to model parameters corresponding to variables with only a noisy relationship
with the output, instead assigning them a value close to 0 that reflects the a non-robust relationship.
Entropy Regularization
One sign of overfitting in a language model pθis that it places effectively all of its probability mass
on a single symbol.13Rather, we may want the distributions output by our model to generally have
higher entropy, i.e., following the principle of maximum entropy: “the probability distribution which
best represents the current state of knowledge about a system is the one with largest entropy, in the
context of precisely stated prior data” Jaynes (1957). Several regularization techniques, which we
refer to as entropy regularizers , explicitly penalize the model for low entropy distributions.
Label smoothing (Szegedy et al., 2015) and the confidence penalty (Pereyra et al., 2017) add
terms toℓto penalize the model for outputting peaky distributions. Explicitly, label smoothing
reassigns a portion of probability mass in the reference distribution from the ground truth symbol
to all other symbols in the vocabulary. It is equivalent to adding a term DKLpu||pθqtoℓ, where
uis the uniform distribution. The confidence penalty regularizes against low entropy distribution
12This philosophy can be derived from Occam’s Razor, i.e., the principle that one should search for explanations
constructed using the smallest possible set of elements.
13The softmax transformation serves as somewhat of a regularizer against this behavior since it does not allow any
symbol be assigned a probability of 0.
3.2. ESTIMATING A LANGUAGE MODEL FROM DATA 73
by adding a term H ppθqtoℓthat encourage high entropy in model outputs. The general class of
entropy regularizers have proven effective in training neural models Meister et al. (2020).
Dropout
Regularization also encompasses methods that expose a model to noise that can occur in the data
at inference time. The motivation behind such methods is both to penalize a model for being overly
dependent on any given variable (whether directly from the input or somewhere further along in the
computational graph) for making predictions. Dropout does this explicitly by randomly “dropping”
variables from a computation in the network (Srivastava et al., 2014).
More formally, consider a model defined as a series of computational nodes, where any given
node is the product of the transformation of previous nodes. When dropout is applied to the module
that contains that node, then the node is zeroed out with some percentage chance, i.e., it is excluded
in all functions that may make use of it to compute the value of future nodes. In this case, the
model will be penalized if it relied completely on the value of that node for any given computation
in the network. Dropout can be applied to most variables within a model, e.g., the inputs to the
model itself, the inputs to the final linear projection in a feed-forward layer, or the summands in the
attention head of a Transformer. Note that at inference time, all nodes are used to compute the
model’s output.14
Batch and Layer Normalization
Rescaling variables within a network helps with training stability, and further, with generalization
by keeping variables within the same range and with unit variance. Specifically, batch normalization
helps regularize the problem of covariate shift, where the distribution of features (both the input
features and the variables corresponding to transformed features within a network) differs between
the training data and the data at inference time. Batch normalization alleviates this problem by
recentering (around 0) and scaling (such that data points have unit variance) data points such
that the data flowing between intermediate layers of the network follows approximately the same
distribution between batches. Layer normalization likewise performs centering and rescaling, albeit
across features rather than across data points. Specifically, normalization is performed so that all of
the feature values within a data point have mean 0 and unit variance.
14Some form of renormalization is typically performed to account for the fact that model parameters are learned
with only partial availability of variables. Thus when all variables are used in model computations, the scale of the
output will (in expectation) be larger than during training, potentially leading to poor estimates.
74 CHAPTER 3. MODELING FOUNDATIONS
Chapter 4
Classical Language Models
Next, we turn to two classical language modeling frameworks: finite-state language models (a
natural generalization of the well-known n-gram models) in §4.1 and pushdown language models
§4.2. Although the most successful approaches to language modeling are based on neural networks,
the study of older approaches to language modeling is invaluable. First, due to the simplicity of
the models, learning how they work helps distill concepts. And, moreover, they often serve as
important baselines in modern NLP and provide very useful insights into the capabilities of modern
architectures as we will see when we discuss modern architectures in Chapter 5.
In the spirit of our question-motivated investigation, we will focus on the following two questions.
Question 4.1: Representing conditional distributions
How can we tractably represent all conditional distributions of the form pSMpy|yqin a simple
way?
Question 4.2: Representing hierarchical structure
How can we tractably represent the hierarchical structure of human language?
4.1 Finite-state Language Models
After rigorously defining what language models are (and what they are not) and discussing how we
can estimate them, it is time to finally introduce our first class of language models—those based on
finite-state automata. Language models derived from probabilistic finite-state automata are some
of the simplest classes of language models because they definitionally distinguish a finite numbers
of contexts when modeling the conditional distribution of the next possible symbol pMpy|yq. We
first give an intuitive definition of a finite-state language model and then introduce a more formal
definition, which we will use throughout the rest of the section.
75
76 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Definition 4.1.1: Informal definition of a finite-state language model
A language model pLMisfinite-state if it defines only finitely many unique conditional
distributions pLMpy|yq. In other words, there are only finitely many contexts ywhich define
the distribution over the next symbol, pLMpy|yq.
Intuitively, this framework might be useful because it bounds the number of unique conditional
distributions we have to learn. However, as we will see later in this chapter, finite-state language
models are not sufficient for modeling human language. Nevertheless, they can still offer a baseline
for modeling more complex phenomena. They also offer a useful theoretical tool in the understanding
of neural language models, which we will discuss in Chapter 5.
4.1.1 Weighted Finite-state Automata
Before we introduce finite-state language models , we go on a brief detour into the theory of finite-state
automata . As we will see, finite-state automata are a tidy and well-understood formalism. As we will
see later in §5.2, they also provide a solid and convenient theoretical framework for understanding
modern neural language models, e.g., those based on recurrent neural networks and transformers.
We, therefore, begin by briefly introducing the theory of finite-state automata with real-valued
weights.
Finite-state Automata
In words, finite-state automata are one of the simplest devices for defining a formal language (cf.
Definition 2.3.5). We give a formal definition below.
Definition 4.1.2: Finite-state Automata
Afinite-state automaton (FSA) is a 5-tuple pΣ,Q,I,F,δqwhere
•Σ is an alphabet;
•Qis a finite set of states;
•IĎQis the set of initial states;
•FĎQthe set of final or accepting states;
•A finite multiset δĎQˆpΣYtεuqˆQ.aElements of δare generally called transitions .
aThe fact that it is a multiset reflects that it can contain multiple copies of the same element (i.e., transitions
between the same pair of states with the same symbol).
The name, finite-state automaton, stems from the requirement that the set of states Qis finite,
which stands in contrast to the remaining formalisms we will cover in this course, e.g., pushdown
automata and recurrent neural networks. We will denote a general finite-state automaton with
a (subscripted) A. We will also adopt a more suggestive notation for transitions by denoting a
transitionpq1,a,q 2qasq1aÝ Ñq2.
4.1. FINITE-STATE LANGUAGE MODELS 77
An FSA can be graphically represented as a labeled, directed multi-graph.1The vertices in the
graph represent the states qPQand the (labeled) edges between them the transitions in δ. The
labels on the edges correspond to the input symbols aPΣ which are consumed when transitioning
over the edges. The initial states qιPIare marked by a special incoming arrow while the final
statesqφPFare indicated using a double circle.
Example 4.1.1: An example of a finite-state automaton
An example of an FSA can be seen in Fig. 4.1. Formally, we can specify it as
•Σ“ta,b,cu
•Q“t1,2,3u
•I“t1u
•F“t3u
•δ“tp1,a,2q,p1,b,3q,p2,b,2q,p2,c,3qu
12
3a
bcb
Figure 4.1: Example of a simple FSA.
A finite-state automaton sequentially reads in individual symbols of an input string yPΣ˚
and transitions from state to state according to the transition function δ. The traversal through the
automaton starts in a state qιPI(more precisely, it acts as if starting from all of them in parallel).
It then transitions from state qinto the state q1upon reading the symbol aif and only if qaÝ Ñq1Pδ.
ε-labeled transitions, however, allow a finite-state machine to transition to a new state without
consuming a symbol. This is in line with ε’s definition as an empty string.
A natural question to ask at this point is what happens if for a state–symbol pair pq,aqthere is
more than one possible transition allowed under the relation δ. In such a case, we take allimplicit
transitions simultaneously, which leads us to a pair of definitions.
Definition 4.1.3: Deterministic finite-state automaton
A FSA A“pΣ,Q,I,F,δqisdeterministic if
•it does not have any ε-transitions;
1The multi- aspect of the multi-graph refers to the fact that we can have multiple transitions from any pair of
states and labeled refers to the fact that we label those transitions with symbols from the alphabet Σ.
78 CHAPTER 4. CLASSICAL LANGUAGE MODELS
•for everypq,aqPQˆΣ, there is at most one q1PQsuch thatqaÝ Ñq1Pδ;
•there is a single initial state, i.e., |I|“1.
Otherwise, Aisnon-deterministic .
An important, and perhaps not entirely obvious, result is that the classes of deterministic and
non-deterministic FSA are equivalent , in the sense that you can always represent a member of one
class with a member of the other.
If the automaton ends up, after reading in the last symbol of the input string, in one of the final
statesqφPF, we say that the automaton accepts that string. A finite-state automaton is therefore
a computational device that determines whether a string satisfies a condition (namely, the condition
that the automaton, by starting in an initial state and following one of the paths labeled with that
string, ends in a final state). A string that satisfies this condition is said to be recognized by the
automaton and the set of all strings satisfying this condition form the language of the automaton.2
Definition 4.1.4: Language of a finite-state automaton
LetA“pΣ,Q,I,F,δqbe an finite-state automaton. The language ofA,LpAqis defined as
LpAqdef“ty|yis recognized by Au (4.1)
Abstractly, a finite-state automaton is hence a specification of a set of rules that strings must
satisfy to be included in its language. The set of languages that finite-state automata can recognize
is known as the class of regular languages .
Definition 4.1.5: Regular language
A language LĎΣ˚isregular if and only if it can be recognized by an unweighted finite-state
automaton, i.e., if there exists a finite-state automaton Asuch thatL“LpAq.
Example 4.1.2: Additional examples of finite-state automata
Additional simple examples of FSAs are shown in Fig. 4.2. The FSA in Fig. 4.2a, for example,
can formally be defined with
•Σ“ta,b,cu
•Q“t1,2,3,4,5,6u
•I“t1u
•F“t6u
•δ“tp1,a,2q,p1,b,3q,p2,b,2q,p2,c,4q,p3,c,4q,p3,b,5q,p4,a,6q,p5,a,6qu
The FSA in Fig. 4.2a is deterministic while the one in Fig. 4.2b is non-deterministic.
2We also say that the automaton recognizes this set of strings (language).
4.1. FINITE-STATE LANGUAGE MODELS 79
A few examples of strings accepted by the A1includebba,bca,aca,abca,abbca ,abbbca,... .
In fact, due to the self-loop at state 2, the symbol bcan appear an arbitrary number of times
at position 2 in the accepted string abca. Notice that, starting from the state 1 and following
the transitions dictated by any of the accepted strings, we always end up in the only final
state, state 6. In particular, the string “ abbca ” is accepted with the following set of transitions
inA1:
1aÝ Ñ2,2bÝ Ñ2,2bÝ Ñ2,2cÝ Ñ4,4aÝ Ñ6.
12
34
56a
bc
bc
ba
a
(a) A deterministic FSA, A1. Each state only has
one outgoing transition labeled with the same
symbol.12
34
56a
ac
bb
ba
a
(b) A non-deterministic FSA, A2. State 1 has
two outgoing transitions labeled with awhereas
state 3 has two outgoing transitions labeled with
b.
Figure 4.2: Examples of a deterministic and a non-deterministic FSA.
Weighted Finite-state Automata
A common and very useful augmentation to finite-state automata is through the addition of weights
on the transitions. The general theory of weighted automata makes use of semiring theory, which is
beyond the scope of this course.3In this course, we will limit ourselves to the study of automata
with real-valued weights.
Definition 4.1.6: Real-weighted Finite-State Automaton
Areal-weighted finite-state automaton (WFSA) Ais a 5-tuplepΣ,Q,δ,λ,ρqwhere
•Σ is a finite alphabet;
•Qis a finite set of states;
•δĎQˆpΣYtεuqˆRˆQa finite multiset of transitions;a
•λ:QÑRa weighting function over Q;
•ρ:QÑRa weighting function over Q.
aAgain, we use the notation qa{wÝ ÝÝ Ñq1to denotepq,a,w,q1qPδ.
3Semirings and semiring-weighted formal languages are covered in detail in the Advanced Formal Language Theory
course offered at ETH as well.
80 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Notice that we omit the initial and final state sets from the definition of WFSAs. Those can
implicitly be specified by the states given non-zero initial or final weights by the λandρfunctions,
i.e.,I“tqPQ|λpqq‰0uandF“tqPQ|ρpqq‰0u. We might refer to them in the text later for
notational convenience and clarity of exposition. We will also sometimes denote transition weights
withωˆ
qa{wÝÝÑq1˙
def“w.
Graphically, we write the transition weights on the edges of the graph representing the WFSA
after the output symbol, separated by a “/” . The same separator is also used to separate the state
name from its final weight, which is written in the node. The initial weights, however, are written
on the incoming arrow denoting initial states.
Example 4.1.3: An example of a weighted finite-state automaton
Fig. 4.3 shows a weighted version of the FSA from Fig. 4.2a above.
1 0.32
34
56{1
ea{0.5
b{1πb{0.63
c{0.9
c{0.21
b{0.13a{1π¨e
a{0.29
Figure 4.3: The WFSA corresponding to the FSA from Fig. 4.2a.
The connection of WFSAs to graphs makes it natural to define a set of transition matrices
specified by a WFSA.
Definition 4.1.7: Transition matrix
LetA“pΣ,Q,δ,λ,ρqbe a WFSA. For any aPΣ, we define the symbol-specific transition
matrix Tpaqas the transition matrix of the graph restricted to a-labeled transitions. We also
define the (full) transition matrix asTdef“ř
aPΣTpaq.
4.1. FINITE-STATE LANGUAGE MODELS 81
Example 4.1.4: Examples of transition matrices
Consider the WFSA Ain Fig. 4.3. The (symbol-specific) transition matrices for Aare
Tpaq“¨
˚˚˚˚˚˚˝0 0.5 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 01
π¨e
0 0 0 0 0 0 .29
0 0 0 0 0 0˛
‹‹‹‹‹‹‚
Tpbq“¨
˚˚˚˚˚˚˝0 01
π0 0 0
0 0.63 0 0 0 0
0 0 0 0 0 .13 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0˛
‹‹‹‹‹‹‚
Tpcq“¨
˚˚˚˚˚˚˝0 0 0 0 0 0
0 0 0 0 .9 0 0
0 0 0 0 .21 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0˛
‹‹‹‹‹‹‚
T“Tpaq`Tpbq`Tpcq“¨
˚˚˚˚˚˚˝0 0.51
π0 0 0
0 0.63 0 0.9 0 0
0 0 0 0 .21 0.13 0
0 0 0 0 01
π¨e
0 0 0 0 0 0 .29
0 0 0 0 0 0˛
‹‹‹‹‹‹‚
Paths and Path Weights
A path is an important concept when talking about (weighted) finite-state automata as it defines
the basic structure by which a string is recognized or weighted. We now give a formal definition of a
path and discuss how to weight paths.
Definition 4.1.8: Path
Apath πis an element of δ˚with consecutive transitions , meaning that it is of the formˆ
q1‚{‚Ý Ý Ñq2,q2‚{‚Ý Ý Ñq3¨¨¨qn´1‚{‚Ý Ý Ñqn˙
, where‚is a placeholder.aThelength of a path is
the number of transition in it; we denote the length as |π|. We useppπqandnpπqto denote
the origin and the destination of a path, respectively. The yield of a path is the concatenation
of the input symbols on the edges along the path, which we will mark with spπq. Furthermore,
we denote sets of paths with capital Π. Throughout the text, we will use a few different
variants involving Π to avoid clutter:
82 CHAPTER 4. CLASSICAL LANGUAGE MODELS
•ΠpAqas the set of all paths in automaton A;
•ΠpA,yqas the set of all paths in automaton Awith yield yPΣ˚;
•ΠpA,q,q1qas the set of all paths in automaton Afrom stateqto stateq1.
aNotice we use the Kleene closure on the set δhere. It thus represents any sequence of transitions Pδ
One of the most important questions when talking about weighted formalisms like weighted
finite-state automata is how to combine weights of atomic units like transitions into weights of
complete structures .4We begin by multiplicatively combining the weights of individual transitions
in a path into the weights of the full path.
Definition 4.1.9: Path Weight
Theinner path weight wIpπqof a path π“q1a1{w1Ý ÝÝÝ Ñq2¨¨¨qN´1aN{wNÝ ÝÝÝÝ ÑqNis defined as
wIpπq“Nź
n“1wn. (4.2)
The(full) path weight of the path πis then defined as
wpπq“λpppπqqwIpπqρpnpπqq. (4.3)
A path πis called accepting orsuccessful if wpπq‰0.
The inner path weight is therefore the product of the weights of the transitions on the path,
while the (full) path weight is the product of the transition weights as well as the initial and final
weights of the origin and the destination of the path, respectively.
String Acceptance Weights and Weighted Regular Languages
When we introduced unweighted finite-state automata, we defined the important concept of recogniz-
ing a string and recognizing a language. We generalize these concepts to the very natural quantity
of the weight assigned by a WFSA to a string yPΣ˚, i.e., its acceptance weight, or stringsum, as
the sum of the weights of the paths that yield y.
Definition 4.1.10: Stringsum
Thestringsum , string weight, or acceptance weight of a string yPΣ˚under a WFSA Ais
defined as
Apyqdef“ÿ
πPΠpA,yqwpπq. (4.4)
4In the case of WFSAs, a structure is a path. In the next section, we will see how to combine weights from basic
units into trees.
4.1. FINITE-STATE LANGUAGE MODELS 83
This naturally generalizes the notion of acceptance by an unweighted FSA—whereas an un-
weighted FSA only makes a binary decision of accepting or rejecting a string, a weighted FSA always
accepts a string with a specific weight. This leads to the definition of the weighted language of the
WFSA.
Definition 4.1.11: Weighted language of a weighted finite-state automaton
LetAbe a WFSA. Its (weighted) language is defined as
LpAqdef“tpy,Apyqq|yPΣ˚u. (4.5)
We say a language is a weighted regular language if it is a language of some WFSA:
Definition 4.1.12: Weighted regular language
A weighted language Lis a weighted regular language if there exists a WFSA Asuch that
L“LpAq.
Lastly, we also define the full and state-specific allsum of the automaton. The former refers to
the total weight assigned to allpossible strings, or all possible paths whereas the latter refers to the
sum of the path weights of the paths stemming from a specific state.
Definition 4.1.13: State-specific allsum
LetA“pΣ,Q,δ,λ,ρqbe a WFSA. The allsum of a stateqPQis defined as
ZpA,qq“ÿ
πPΠpAq
q1“qwIpπqρpnpπqq. (4.6)
State-specific allsums are also referred to as the backward values in the literature and are
often denoted as βpqq.
Definition 4.1.14: WFSA allsum
LetA“pΣ,Q,δ,λ,ρqbe a WFSA. The allsum ofAis defined as
ZpAq“ÿ
yPΣ˚Apyq“ÿ
yPΣ˚ÿ
πPΠpA,yqwpπq“ÿ
πPΠpAqwpπq. (4.7)
The second equality in Eq. (4.7) comes from the crucial observation that the double sum in
the second term sums over precisely all paths of the automaton A, which is where the name of the
quantity comes from allsum .5This is easy to see if we consider that by summing over all possible
strings, we enumerate all possible path yields, and each path in the automaton has a yield PΣ˚.
ZpAqis again the result of summing over infinitely many terms (whether the set of strings in Σ˚
5Analogously, given some (implicitly defined) set of paths S, we will name the sum over the weights of the paths
inSthe allsum over S
84 CHAPTER 4. CLASSICAL LANGUAGE MODELS
of the infinitely many paths in a cyclic WFSA), and might therefore not necessarily be finite. For
reasons which will become clear shortly, we will say that a WFSA Aisnormalizable ifZpAqă8 .
Note that the sum in Eq. (4.4) only contains one term if the automaton is deterministic. Whenever
the automaton is non-deterministic, or when we are interested in the sum of paths with different
yields as in Eq. (4.7), the interactions (namely, the distributive law ) between the sum over the
different paths and the multiplications over the transitions in the paths play an important role when
designing efficient algorithms. Indeed, many algorithms defined for WFSAs rely on decompositions
of such sums enabled by the distributive law.6
Accessibility and Probabilistic Weighted Finite-state Automata
An important property of states of a WFSA which we will need when investigating the tightness of
finite-state language models is accessibility.
Definition 4.1.15: (Co)-Accessible and useful states
A stateqPQof a WFSA is accessible if there is a non-zero-weighted path to qfrom some
stateqιwithλpqιq‰0; it is co-accessible state if there is a non-zero-weighted path from q
to some state qφwithρpqφq‰0. It is useful if it is both accessible and co-accessible, i.e., q
appears on some non-zero-weighted accepting path.
Definition 4.1.16: Trim automaton
Trimming a WFSA means removing its useless states.aRemoving the non-useful states
means removing their rows and columns from Tas well as their rows fromÝ ÑλandÝ Ñρ, yielding
possibly smaller T1,Ý Ñλ1andÝ Ñρ1.
aThis does not affect the weights of the strings with w pyq‰0.
We will use WFSAs to specify language models. However, not every WFSA is a language model,
i.e., a distribution over strings. Generally, the weight of a string could be negative if we allow
arbitrary real weights. Thus, a restriction we will impose on allweighted automata that represent
finite-state language models is that the weights be non-negative .
Furthermore, a special class of WFSAs that will be of particular interest later is probabilistic
WFSAs.
Definition 4.1.17: Probabilistic Weighted Finite-State Automaton
A WFSA A“pΣ,Q,δ,λ,ρqisprobabilistic (a PFSA) if
ÿ
qPQλpqq“1 (4.8)
6Many such examples are covered in the Advanced Formal Language Theory course.
4.1. FINITE-STATE LANGUAGE MODELS 85
and, for all qPQand all outgoing transitions qa{wÝÝÑq1Pδit holds that
λpqqě0 (4.9)
ρpqqě0 (4.10)
wě0 (4.11)
and ÿ
qa{wÝÝÑq1w`ρpqq“1. (4.12)
This means that the initial weights of all the states of the automaton form a probability
distribution (the initial weight of a state corresponds to the probability of starting in it), as well as
that, for any state qin the WSFA, the weights of its outgoing transitions (with any label) together
with its final weight form a valid discrete probability distribution. In a certain way, probabilistic
finite-state automata naturally correspond to locally normalized language models, as we explore in
the next subsection.
The eossymbol and the final weights. Notice that the final weights in a PFSA play an
analogous role to the eossymbol: the probability of ending a path in a specific state q—and
therefore ending a string—is q’s final weight! That is, the probability ρpqφqfor someqφPQ,
representing the probability of ending the path in qφ, is analogous to the probability of ending a
string y,pSMpeos|yq, whereqφ“represents” the string (history) y.7When modeling language with
weighted finite-state automata, we will therefore be able to avoid the need to specify the special
symbol and rather rely on the final weights, which are naturally part of the framework.
4.1.2 Finite-state Language Models
We can now formally define what it means for a language model to be finite-state:
Definition 4.1.18: Finite-state language models
A language model pLMisfinite-state if it can be represented by a weighted finite-state automa-
ton, i.e., if there exists a WFSA A“pΣ,Q,δ,λ,ρqsuch thatLpAq“LppLMq. Equivalently,
we could say that pLMis finite-state if its language is a weighted regular language.
On the other hand, given a WFSA A, there are two established ways of defining a probability of
string.
String Probabilities in a Probabilistic Finite-state Automaton
In a probabilistic FSA (cf. Definition 4.1.17), any action from a state qPQis associated with a
probability. Since the current state completely encodes all the information of the input seen so far
in a finite-state automaton, it is intuitive to see those probabilities as conditional probabilities of
7Due to the possible non-determinism of WFSAs, the connection is of course not completely straightforward, but
the point still stands.
86 CHAPTER 4. CLASSICAL LANGUAGE MODELS
the next symbol given the input seen so far. One can, therefore, define the probability of a path as
the product of these individual “conditional” probabilities.
Definition 4.1.19: Path probability in a PFSA
We call the weight of a path πPΠpAqin a probabilistic FSA the probability of the path π.
This alone is not enough to define the probability of any particular string yPΣ˚since there
might be multiple accepting paths for y. Naturally, we define the probability of yas the sum of the
individual paths that recognize it:
Definition 4.1.20: String probability in a PFSA
We call the stringsum of a string yPΣ˚in a probabilistic FSA the probability of the string
y:
pApyqdef“Apyq. (4.13)
Crucially, notice that these two definitions did not require any normalization over all possible
paths or strings. This closely resembles the way we defined locally normalized models based on the
conditional probabilities of a sequence model. Again, such definitions of string probabilities are
attractive as the summation over all possible strings is avoided. However, a careful reader might
then ask themself: do these probabilities actually sum to 1, i.e., is a probabilistic FSA tight? As you
might guess, they might not.8We explore this question in §4.1.4.
String Probabilities in a General Weighted Finite-state Automaton
To define string probabilities in a general weighted FSA, we use the introduced notions of the
stringsum and the allsum. The allsum allows us to tractably normalize the stringsum to define the
globally normalized probability of a string yas the proportion of the total weight assigned to all
strings that is assigned to y.9
Definition 4.1.21: String probability in a WFSA
LetA“pΣ,Q,δ,λ,ρqbe a normalizable WFSA with non-negative weights. We define the
probability of a string yPΣ˚under Aas
pApyqdef“Apyq
ZpAq. (4.14)
Language Models Induced by a WFSA
With the notions of string probabilities in both probabilistic and general weighted FSAs, we can
now define the language model induced by Aas follows.
8Notice that, however, whenever a PFSA is tight, its allsum is 1.
9We will see how the allsum can be computed tractably in §4.1.3.
4.1. FINITE-STATE LANGUAGE MODELS 87
Definition 4.1.22: A language model induced by a WFSA
LetA“pΣ,Q,δ,λ,ρqbe a WFSA. We define the language model induced by Aas the
following probability distribution over Σ˚
pLMApyqdef“pApyq. (4.15)
It is easy to see that while global normalization requires the computation of the allsum, language
models induced by weighted FSAs through Eq. (4.14) are globally normalized and thus always tight.
In the next subsection, we consider how the quantities needed for computing Eq. (4.14) can be
computed. Of particular interest will be the quantity ZpAq, as it involves the summation over
possibly infinitely many terms and therefore requires some clever tricks to be computed.
4.1.3 Normalizing Finite-state Language Models
In this subsection, we develop an algorithm for normalizing a globally normalized language model (cf.
Definition 2.4.2) defined by a WFSA, i.e., an algorithm for computing the allsum ZpAqwhenever
this quantity is finite. Moreover, the derivation will also reveal necessary and sufficient conditions
for WFSAs to be normalizable.
Converting a matrix of pairwise pathsums to the allsum. Before we consider how to
computeZpAq, let us first consider a much simpler problem. Suppose we had a matrix M, which
contained at the entry Mijthe sum of all the inner weights over all paths between the states iand
j, i.e.,
Mij“ÿ
πPΠpA,i,jqwIpπq.
How could we then compute the quantity ZpAq?
ZpAq“ÿ
πPΠpAqwpπq (4.16)
“ÿ
πPΠpAqλpppπqqwIpπqρpnpπqq (4.17)
“ÿ
i,jPQÿ
πPΠpA,i,jqλpppπqqwIpπqρpnpπqq (4.18)
“ÿ
i,jPQÿ
πPΠpA,i,jqλpiqwIpπqρpjq (4.19)
“ÿ
i,jPQλpiq¨
˝ÿ
πPΠpA,i,jqwIpπq˛
‚ρpjq (4.20)
“ÿ
i,jPQλpiqMijρpjq (4.21)
“Ý ÑλMÝ Ñρ, (4.22)
whereÝ ÑλandÝ Ñρdenote the vectors resulting from the “vectorization” of the functions λandρ, i.e.,Ý Ñλn“λpnqandÝ Ñρn“ρpnq. This also explains the naming of the functions λandρ: the initial
88 CHAPTER 4. CLASSICAL LANGUAGE MODELS
weights function λ, “lambda” appears on the left side of the closed form expression for ZpAqand
the definition of the path weight (cf. Eq. (4.3)), whereas the final weights function ρ,rho, appears
on the r ight side of the expression and the definition of the path weight.
Computing the matrix of pairwise pathsums. LetTbe the transition matrix of the automaton
A. Notice that the entry Tijby definition contains the sum of the inner weights of all paths of
length exactly 1 (individual transitions) between the states iandj. We also define T0“I, meaning
that the sum of the weights of the paths between iandjof length zero is 0 if i‰jand 1 (the unit
for multiplication) if i“j. This corresponds to not transitioning, i.e., staying in place, if i“j. We
next state a basic result from graph theory.
Lemma 4.1.1
LetTbe the transition matrix of some weighted directed graph G. Then the matrix Td
contains the allsum of all paths of length exactlyd, i.e.,
Td
i,j“ÿ
πPΠpA,i,jq
|π|“dwIpπq. (4.23)
Proof. By induction on the path length. Left as an exercise for the reader. ■
It follows directly that the matrix
Tďddef“dÿ
k“1Tk
contains the pairwise pathsums of paths of length at mostd.
In general, the WFSA representing a n-gram language model can of course be cyclic. This means
that the number of paths in Π pAqmight be infinite and they might be of arbitrary length (which is
the result of looping in a cycle arbitrarily many times). To compute the pairwise pathsums over all
possible paths, we, therefore, have to compute
T˚def“lim
dÑ8Tďd“8ÿ
d“0Td. (4.24)
This is exactly the matrix form of the geometric sum . Similarly to the scalar version, we can
4.1. FINITE-STATE LANGUAGE MODELS 89
manipulate the expression Eq. (4.24) to arrive to a closed-form expression for computing it:
T˚“8ÿ
d“0Td(4.25)
“I`8ÿ
d“1Td(4.26)
“I`8ÿ
d“1TTd´1(4.27)
“I`T8ÿ
d“1Td´1(4.28)
“I`T8ÿ
d“0Td(4.29)
“I`TT˚. (4.30)
If the inverse of pI´Tqexists, we can further rearrange this equation to arrive at
T˚“I`TT˚(4.31)
T˚´TT˚“I (4.32)
T˚´T˚T“I (4.33)
T˚pI´Tq“I (4.34)
T˚“pI´Tq´1. (4.35)
This means that, if pI´Tqexists, we can compute the pairwise pathsums by simply inverting
it! Using the remark above on how to convert a matrix of pairwise pathsums into the full allsum,
we can therefore see that we can globally normalize an n-gram language model by computing a
matrix inversion! Since the runtime of inverting a NˆNmatrix is O`
N3˘
, andN“|Q|for a
transition matrix of a WFSA with states Q, we can globally normalize a n-gram language model in
time cubic in the number of its states. This is a special case of the general algorithm by Lehmann
(1977). Note, however, that this might still be prohibitively expensive: as we saw, the number of
states in a n-gram model grows exponentially with n, and even small n’s and reasonable alphabet
sizes might result in a non-tractable number of states in the WFSA with the cubic runtime.
We still have to determine when the infinite sum in Eq. (4.24) converges. One can see by writing
out the product Tdin terms of its eigenvalues that the entries of Tddiverge towards ˘8as soon as
the magnitude of any of T’s eigenvalues is larger than 1. This means that ∥T∥2ă1 (spectral norm)
is a necessary condition for the infinite sum to exist. This is, however, also a sufficient condition: if
∥T∥2ă1, all of T’s eigenvalues are smaller than 1 in magnitude, meaning that the eigenvalues of
I´Tare strictly positive and the matrix I´Tis invertible.10
Speed-ups of the Allsum Algorithm
The introduced algorithm for computing the allsum in a WFSA can, therefore, be implemented
as a matrix inverse. This means that its runtime is O´
|Q|3¯
, which can be relatively expensive.
101´λis an eigenvalues of I´Tiffλis an eigenvalue of T.
90 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Fortunately, faster algorithms exist for WFSAs with more structure (in their transition functions)—
for example, the allsum can be computed in time linear in the number of transitions if the automaton
is acyclic using a variant of the Viterbi algorithm (Eisner, 2016). Furthermore, if the automaton
“decomposes” into many smaller strongly connected components (i.e., subgraphs that are cyclic), but
the components are connected sparsely and form an acyclic graph of components, the allsum can
also be computed more efficiently using a combination of the algorithms described above and the
algorithm for acyclic WFSA, resulting in a possibly large speedup over the original algorithm.
Importantly, the allsum algorithm and all the speed-ups are differentiable , meaning that they
can be used during the gradient-based training (cf.§3.2.3) of a finite-state language model, where
the weights are parametrized using some learnable parameters—we will return to this point shortly.
Locally Normalizing a Globally Normalized Finite-state Language Model
As shown in Theorem 2.4.2, any language model (and thus, any globally-normalized model with a
normalizable energy function) can also be locally normalized. In the case of finite-state language
models, we can actually explicitly construct the WFSA representing the locally normalized variant
using a procedure that is conceptually similar to the allsum algorithm described here. In contrast
to the procedure we presented here, however, the local normalization algorithm computes the
pathsums of the paths stemming from every possible state qindividually and then “reweights” the
transitions depending on the pathsums of their target states r. You can think of this as computing
the contributions to the entire allsum from qmade by all the individual outgoing transitions from q
and then normalizing those contributions. This is an instance of the more general weight pushing
algorithm.11This can be summarized by the following theorem:
Theorem 4.1.1: PFSAs and WFSAs are equally expressive
Normalizable weighted finite-state automata with non-negative weights and tight probabilistic
finite-state automata are equally expressive.
In the proof of this theorem, we will make use of the following lemma.
Lemma 4.1.2
LetA“pΣ,Q,δ,λ,ρqandqPQ. Then
ZpA,qq“ÿ
qa{wÝÝÑq1PδALωˆ
qa{¨Ý Ý Ñq1˙
ZpA,q1q`ρpqq (4.36)
Proof. You are asked to show this in Exercise 4.1. ■
We can now prove Theorem 4.1.1
Proof. To prove the theorem, we have to show that any WFSA can be written as a PFSA and vice
versa.12
11See Mohri et al. (2008) for a more thorough discussion of weight pushing.
12By “written as”, we mean that the weighted language is the same.
4.1. FINITE-STATE LANGUAGE MODELS 91
ðSince any tight probabilistic FSA is simply a WFSA with ZpAq“1, this holds trivially.
ñLocal normalization is a general property of automata resulting from weight pushing. Here,
we describe the construction in the special case of working with real-valued weights. See Mohri et al.
(2008) for a general treatment.
LetA“pΣ,Q,δ,λ,ρqbe a normalizable WFSA with non-negative weights. We now show that,
for any WFSA, there exists a PFSA encoding the same language model. Let AG“pΣ,Q,δ,λ,ρq
be a trim WFSA that encodes a distribution over Σ˚using Eq. (4.14). We now construct a tight
probabilistic finite-state automaton AL“pΣ,Q,δAL,λAL,ρALqwhose language is identical. We
define the initial and final weights of the probabilistic FSA as follows.
λALpqqdef“λpqqZpA,qq
ZpAq(4.37)
ρALpqqdef“ρpqq
ZpA,qq(4.38)
We define the transitions of the probabilistic FSA as follows.
ωALˆ
qa{¨Ý Ý Ñq1˙
def“ωˆ
qa{¨Ý Ý Ñq1˙
ZpA,q1q
ZpA,qq(4.39)
This means that ALcontains the same transitions as A, they are simply reweighted. Note that the
assumption that Ais trimmed means that all the quantities in the denominators are non-zero.
It is easy to see that the weights defined this way are non-negative due to the non-negativity of
A’s weights. Furthermore, the weights of all outgoing arcs from any qPQand its final weight sum
to 1:
ÿ
qa{wÝÝÑq1PδALw`ρALpqq (4.40)
“ÿ
qa{wÝÝÑq1PδALωˆ
qa{¨Ý Ý Ñq1˙
ZpA,q1q
ZpA,qq`ρpqq
ZpA,qq(definition of δAL) (4.41)
“1
ZpA,qq¨
˚˚˝ÿ
qa{wÝÝÑq1PδALωˆ
qa{¨Ý Ý Ñq1˙
ZpA,q1q`ρpqq˛
‹‹‚(4.42)
“1 (Lemma 4.1.2) (4.43)
It is also easy to see that the initial weights form a probability distribution over the states of the
92 CHAPTER 4. CLASSICAL LANGUAGE MODELS
constructed automaton.
ÿ
qPQλALpqq“ÿ
qPQλpqqZpA,qq
ZpAq(4.44)
“1
ZpAqÿ
qPQλpqqZpA,qq (4.45)
“1
ZpAqZpAq“1 (4.46)
We now have to show that the probabilities assigned by these two automata match. We
will do that by showing that the probabilities assigned to individual paths match, implying that
stringsums match as well. The probability of a path is defined analogously to a probability of
a string, i.e., pApπq “wpπq
ZpAq(whereZpAq “ 1 for tight probabilistic FSAs). Let then π“ˆ
q1a1{w1Ý ÝÝÝ Ñq2,...,qN´1aN´1{wN´1Ý ÝÝÝÝÝÝÝ Ñ qN˙
PΠpAq“ΠpALq. Then, by the definitions of ωAL,λAL,
andρAL
pALpπq“λALpq1q˜N´1ź
n“1wn¸
ρALpqNq (4.47)
“λpq1qZpA,q1q
ZpAqN´1ź
n“1ωˆ
qna{¨Ý Ý Ñqn`1˙
ZpA,qn`1q
ZpA,qnqρpqNq
ZpA,qNq. (4.48)
Notice that the state-specific allsums of all the inner states of the path (all states apart from q1
andqN)cancel out as the product moves over the transitions of the path. Additionally, the terms
ZpA,q1qandZpA,qNqcancel out with the definitions of λALandρAL. This leaves us with
pALpπq“λpq1q1
ZpAqN´1ź
n“1ωˆ
qna{¨Ý Ý Ñqn`1˙
ρpqNq“pApπq, (4.49)
finishing the proof. ■
While Theorem 2.4.2 shows that any language model can be locally normalized, Theorem 4.1.1
shows that in the context of finite-state language models, the locally normalized version of a
globally-normalized model is alsoa finite-state model.
Defining a Parametrized Globally Normalized Language Model
Having learned how an arbitrary normalizable finite-state language model can be normalized,
we now discuss how models in this framework can be parametrized to enable fitting them to
some training data. Crucial for parameterizing a globally normalized model is a score function
fδ
θ:QˆΣˆQÑR, which parametrizes the transitions between the states and thus determines the
weights of the (accepting) paths. Additionally, we also parameterized the initial and final functions
fλ
θandfρ
θ. These parametrized functions then define the automaton Aθdef“pΣ,Q,δ θ,λθ,ρθq, where
δθdef“"
q1y{fδ
θpq1,y,q2qÝÝÝÝÝÝÝÝÑ q2*
,λθpqιqdef“fλ
θpqιq, andρθpqφqdef“fρ
θpqφq. Note that we can parametrize
4.1. FINITE-STATE LANGUAGE MODELS 93
the function fθin any way we want; for example, the function could be a neural network using
distributed representations (we will see a similar example at the end of this section), or it could
simply be a lookup table of weights. The fact that the function fθ:pq1,y,q 2qcan only “look at”
the identities of the states and the symbol might seem limiting; however, the states alone can
encode a lot of information: for example, in n-gram models we describe below, they will encode the
information about the previous n´1 symbols and the transitions will then encode the probabilities
of transitioning between such sequences of symbols.
The globally parametrized model then simply takes in any string yPΣ˚and computes its
stringsum value under the parametrized automaton, which in turn, as per Eq. (4.15), defines
probabilities of the strings. The quantity ZpAθqcan be computed with the allsum algorithm
discussed in §4.1.3. Importantly, since the algorithms for computing the string probabilities are
differentiable, the model defined this way can also be trained with gradient-based learning as
described in §3.2.3.
You might notice that this formulation does not exactly match the formulation of globally
normalized models from Definition 2.4.2—the function A:Σ˚ÑRdoes not exactly match the
form of an energy function as its values are not exponentiated as in Eq. (2.11). However, we tie
this back to the definition of globally normalized models by defining an actual energy function as a
simple transformation of the stringsum given by Aθ. We can define the globally normalizing energy
function ppAθ
GNas
ppAθ
GNpyqdef“´logpApyqq, (4.50)
which can be easily seen to, after exponentiating it as in Eq. (2.11), result in the same expression as
Eq. (4.15). With this, we have formulated finite-state language models as general globally normalized
models.
Having introduced WFSAs as a formal and abstract computational model which can define a set
of weighted strings, we now show how it can be used to explicitly model a particularly simple family
of languages. We arrive at this family of language models when we impose a specific assumption on
the set of conditional distributions of the language models that ensures that they are finite-state:
then-gram assumption.
4.1.4 Tightness of Finite-state Models
Any normalizable globally normalized finite-state language model is tight by definition because
the sum of the scores over all finite strings is finite, and since they are normalized, they sum to 1.
We, therefore, focus on locally normalized finite-state models and provide necessary and sufficient
conditions for their tightness. Locally normalized finite-state models are exactly probabilistic WFSAs
(Definition 4.1.17). Luckily, the tightness of probabilistic WFSAs can be easily characterized, as the
following theorem shows.
Theorem 4.1.2: A sufficient condition for tightness of finite-state language models
A probabilistic FSA is tight if and only if all accessible states are also co-accessible.
Proof. We prove each direction in turn.
94 CHAPTER 4. CLASSICAL LANGUAGE MODELS
(ñ):Assume the WFSA is tight. Let qPQbe an accessible state, which means qcan be reached
after a finite number of steps with positive probability. By tightness assumption, then there must be
a positive probability path from qto termination, or else the WFSA will not be able to terminate
after reaching q, resulting in non-tightness. This means that qis also co-accessible. So, assuming
that the WFSA is tight, every accessible state is also co-accessible.
(ð):Assume that all accessible states are co-accessible. First, one may consider a Markov chain
consisting only of the set of accessible states QAĎQ, since all other states will have probability 0
at every step. Recall a fundamental result in finite-state Markov chain theory which states that, if
there exists a unique absorbing state which is reachable from every state, then the Markov process
is absorbed by this state with probability 1 (see, e.g., Theorem 11.3 in Grinstead and Snell, 1997).
We already have that
•eosis an absorbing state, and that
•by assumption, every state in QAis co-accessible which implies that they can reach eos.
Hence, it remains to show that eosis the unique absorbing state. Suppose there is another state
(or group of states) in QAdistinct from eosthat is absorbing, i.e., cannot leave once entered. Then,
these states cannot reach eosby assumption, which means they are not co-accessible, contradicting
the assumption that every state in QAis co-accessible. Hence, eosis the only absorbing state in QA
and by the property of an absorbing Markov chain, the process is absorbed by eoswith probability
1. In other words, the WFSA is tight. ■
Notice that trimming a PFSA results in a model that satisfies ρpqq`ř
qa{wÝÝÑq1wď1, but might
no longer achieve equality as required by Definition 4.1.17. We call such models substochastic
WFSAs.
Definition 4.1.23: Substochastic Weighted Finite-State Automaton
A WFSA A“pΣ,Q,δ,λ,ρqissubstochastic if for allqPQand all outgoing transitions
qa{wÝÝÑq1Pδit holds that
λpqqě0 (4.51)
ρpqqě0 (4.52)
wě0 (4.53)
and
ρpqq`ÿ
qa{wÝÝÑq1wď1. (4.54)
We can then express the termination probability of a WFSA in simple linear algebra terms.
4.1. FINITE-STATE LANGUAGE MODELS 95
Theorem 4.1.3: A sufficient condition for the tightness of a sub-stochastic WFSA
LetT1be the transition sum matrix of a trimmed substochastic WFSA. Then I´T1is
invertible and ppxPΣ˚q“Ý Ñλ1JpI´T1q´1Ý Ñρ1ď1.
In the following, we will make use of the spectral radius of a matrix.
Definition 4.1.24: Spectral radius
Thespectral radius of a matrix MPCNˆNwith eigenvalues λ1,...,λNis defined as
ρspMqdef“maxt|λ1|,...,|λN|u. (4.55)
To prove Theorem 4.1.3, we will make use of the following useful lemma.
Lemma 4.1.3
LetT1be the transition sum matrix of a trimmed substochastic WFSA, then ρspT1qă1.
To begin with, we wish to apply the following result which connects the row sums of a matrix to its
spectral radius. Below, MNdenotes the set of NˆNmatrices, and ∥A∥8“max 1ďnďNřN
i“1|Ani|
denotes the infinity matrix norm.
Proposition 4.1.1: §6.2.P8; Horn and Johnson, 2012
For any APMN,ρspAqď∥A∥8. Additionally, if Ais irreducible and not all absolute row
sums of Aare equal, then ρspAqă∥A∥8.
However, the transition sum matrix Pof a substochastic WFSA may be reducible whereas the
irreducibility condition in Proposition 4.1.1 cannot be dropped. Hence, we need to “decompose” T1
in a way to recover irreducibility. We use the Frobenius normal form (also known as irreducible
normal form ) to achieve this.
Proposition 4.1.2: §8.3.P8; Horn and Johnson, 2012
LetAPMNbe non-negative. Then, either Ais irreducible or there exists a permutation
matrix Psuch that
PJAP“»
—–A1˚
...
0 A Kfi
ffifl (4.56)
is block upper triangular, and each diagonal block is irreducible (possibly a 1-by-1 zero matrix).
This is called an Frobenius normal form (orirreducible normal form ) ofA. Additionally,
ΛpAq“ΛpA1qY¨¨¨Y ΛpAKqwhere Λp¨qdenotes the set of eigenvalues of a matrix.
We now proceed to the proof of Lemma 4.1.3.
96 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Proof. Notice that, by way of a similarity transformation via a permutation matrix, the Frobenius
normal form is equivalent to a relabeling of the states in the trimmed WFSA in the sense of
pPJÝ Ñλ1qJpPJT1PqKpPJÝ Ñρ1q“pÝ Ñλ1JPqpPJT1KPqpPJÝ Ñρ1q (4.57a)
“Ý Ñλ1JT1KÝ Ñρ1(4.57b)
where the equalities follow from the fact that the inverse of a permutation matrix Pis its transpose.
Hence, with an appropriate relabeling, we may assume without loss of generality that Pis already
put into a Frobenius normal form
T1“»
—–T1
1˚
...
0 T1
Kfi
ffifl (4.58)
where each T1
kis irreducible.
Since the transition sum matrix T1of a trimmed substochastic WFSA is a substochastic matrix,
eachT1
kis also substochastic. In fact, each T1
kisstrictly substochastic, meaning that there is at least
a row that sums to less than 1. To see this, suppose to the contrary that there is a probabilistic T1
k.
Since the WFSA is trimmed, every state is both accessible and co-accessible. Being accessible implies
that there is a positive probability of reaching every state in T1
k. However, the probabilisticity
ofT1
kforces the correspondingÝ Ñρ1entries to be 0. Hence, none of these states can transition
toeos, meaning that they’re not co-accessible, contradicting the assumption. Hence, every T1
k
is strictly substochastic and has at least one strictly less than 1 row sum. Then, either all row
sums of T1
kare less than 1 or some row sums are 1 and some are less than 1. In either cases,
Proposition 4.1.1 implies that ρspT1
kqă1 for all 1ďkďK. Finally, as Proposition 4.1.2 entails,
ρspT1q“maxtρspT1
1q,...,ρ spT1
Kquwhere each ρspT1
kqă1. Hence,ρspT1qă1. ■
We now use the stated results to finally prove Theorem 4.1.3.
Proof. By Lemma 4.1.3, ρspT1qă1, in which case I´T1is invertible and the Neumann series
I`T1`T12`¨¨¨ converges topI´T1q´1(§5.6, Horn and Johnson, 2012). Hence, we can write
pI´T1q´1“ř8
k“0T1k. Then,
ppΣ˚q“8ÿ
k“0PpΣkq (4.59a)
“8ÿ
k“0Ý Ñλ1JT1kÝ Ñρ1(4.59b)
“Ý Ñλ1J˜8ÿ
k“0T1k¸
Ý Ñρ1(4.59c)
“Ý Ñλ1JpI´T1q´1Ý Ñρ1. (4.59d)
■
4.1. FINITE-STATE LANGUAGE MODELS 97
The quick brown fox jumps over . . .
pLMpfox|The quick brown q
pLMpjumps|quick brown fox q
pLMpover|brown fox jumps q¨
¨
¨
Figure 4.4: An illustration of how an 4-gram LM computes the probability of a string. All conditional
probabilities can be computed in parallel and then multiplied into the probability of the entire
string.
4.1.5 The n-gram Assumption and Subregularity
We now turn our attention to one of the first historically significant language modeling frameworks:
n-gram models. While they are often taught completely separately from (weighted) finite-state
automata, we will see shortly that they are simply a special case of finite-state language models and
thus all results for the more general finite-state language models also apply to the specific n-gram
models as well.
As we saw in Theorem 2.4.2, we can factorize the language model pLMfory“y1...yTPΣ˚as
pLMpyq“pLNpyq“pSMpeos|yqTź
t“1pSMpyt|yătq, (4.60)
wherepSMpy|yqare specified by a locally normalized model (Definition 2.4.5).
Recall that SMs specify individual conditional distributions of the next symbol ytgiven the
previoust´1 symbols for all possible t. However, as tgrows and the history of seen tokens
accumulates, the space of possible histories (sequences of strings to condition on) grows very large
(and indeed infinite as tÑ8 ). This makes the task of modeling individual conditional distributions
for largetcomputationally infeasible. One way to make the task more manageable is by using the
n-gram assumption.
Assumption 4.1.1: n-gram assumption
In words, n-gram assumption states that the conditional probability of the symbol ytgiven
yătonly depends on n´1 previous symbols yt´1
t´n`1def“yt´1,...,yt´n`1:
pSMpyt|yătq“pSM`
yt|yt´1
t´n`1˘
. (4.61)
We will refer to yt´1
t´n`1as the history ofyt. The sequence yt´1¨¨¨yt´n`1qis often called
thehistory or the context .
In plain English, this means that the probability of a token only depends on the previous n´1
tokens. n-gram assumption is, therefore, an alias of pn´1qthorder Markov assumption in the
language modeling context.
Handling edge cases by padding. Given our definition in Eq. (4.61) where the conditional
probability pSMpyt|yt´n´1:t´1qdepends on exactly n´1 previous symbols, we could run into an
98 CHAPTER 4. CLASSICAL LANGUAGE MODELS
issue with negative indices for tăn. To handle edge cases for tăn, we will pad the sequences with
thebossymbols at the beginning, that is, we will assume that the sequences y1...ytfortăn´1
are “transformed” as
y1y2...ytÞÑbos...boslooooomooooon
n´1´ttimesy1y2...yt (4.62)
Notice that with such a transformation, we always end up with strings of length n´1, which is
exactly what we need for conditioning in an n-gram model. In the following, we will assume that all
such sequences are already transformed, but at the same time, we will assume that
pSM¨
˝y|bos...boslooooomooooon
n´1´ttimesy1y2...yt˛
‚“y0y1y2...yt (4.63)
By definition, n-gram language models can only model dependencies spanning ntokens or less.
By limiting the length of the relevant context when determining pSMpyt|yătqto the previous n
tokens, the n-gram assumption limits the number of possible probability distributions that need to
be tracked to O`
|Σ|n´1˘
.
Despite their simplicity, n-gramLMs have a storied place in language modeling (Shannon, 1948a;
Baker, 1975a,b; Jelinek, 1976; Bahl et al., 1983; Jelinek, 1990; Bengio et al., 2000, 2003a, 2006;
Schwenk, 2007; Heafield, 2011; Heafield et al., 2013). Because the conditional probabilities of
n-gramLMs only depend on the previous n´1 symbols, different parts of the string can be processed
independently, i.e., in parallel. This facilitates a natural connection to transformer LMs since
parallelizability is a prevalent feature of the architecture and one of its main advantages over other
neural LMs such as RNN LMs (Vaswani et al., 2017).
A particularly simple case of the n-gram model is the bigram model where n“2, which
means that the probability of the next word only depends on the previous one, i.e., pSMpyt|yătq“
pSMpyt|yt´1q.13
Example 4.1.5: A simple bigram model
Let us look at a specific example of a simple bigram model. Suppose our vocabulary consists
of the words “large” ,“language” , and “models” , thus,|Σ| “3. To specify the bigram
model, we have to define the conditional probabilities pMpyj|yiqforyiPΣYtbos,eosuand
yjPΣYteosu(remember that we do not have to model the probability of the next token
being bos). In the case of bigrams, we can represent those in a table, where the entry at
positioni,jrepresents the probability pMpyj|yiq:
“large” “language” “models” “ EOS”
bos 0.4 0.2 0 .2 0.2
“large” 0.1 0.4 0 .2 0.3
“language” 0.1 0.1 0 .4 0.4
“models” 0.2 0.2 0 .1 0.5
“EOS” 0.4 0.2 0 .2 0.2
13What would the uni-gram ( n“1) model look like? What conditional dependencies between words in a sentence
could be captured by it?
4.1. FINITE-STATE LANGUAGE MODELS 99
Under our model, the probability of the sentence “large language models” would be
pSMp“large”|bosq
¨pSMp“language”|“large”q
¨pSMp“models”|“language”q
¨pSMpeos|“models”q
“0.4¨0.4¨0.4¨0.5“0.032
while the probability of the sentence “large large large” would be
pSMp“large”|bosq
¨pSMp“large”|“large”q
¨pSMp“large”|“large”q
¨pSMpeos|“large”q
“0.4¨0.1¨0.1¨0.3“0.0012.
Note that the probabilities in the above table are made up and not completely reasonable. A
realn-gram model would not allow for probabilities of exactly 0 to avoid pathological behavior.
Representing n-gram Models as WFSAs
We define n-gram language models as models that only consider a finite amount of context when
defining the conditional probabilities of the next token. This means that the set of possible conditional
distributions pSMpy|yqis also finite which very naturally connects them to weighted finite-state
automata—indeed, every n-gram language model is a WFSA—specifically, a probabilistic finite-state
automaton (or a substochastic one). We will make this connection more formal in this subsection,
thus formally showing that n-gram models are indeed finite-state. Note that this is different from
§4.1.3, where we discussed how to parametrize a general WFSA and use it as a globally normalized
model—in contrast, in this section, we consider how to fit a (locally normalized) n-gram model into
the finite-state framework.
The intuition behind the connection is simple: the finite length of the context implies a finite
number of histories we have to model. These histories represent the different states the corresponding
automaton can reside in at any point. Given any history ywith|y| ănand the state qPQ
representing y, then, the conditional distribution of the next token given ydictate the transition
weights into the next states in the WFSA, representing the new, updated history of the input.
Importantly, since we want PFSAs to represent globally-normalized models, we will also remove
theeossymbol from the n-gram model before transforming it into a PFSA—as the remark above
about the relationship between the eossymbol and the final states hints, the latter will fill in
the role of the eossymbol. The way we do that is the following. From the semantics of the eos
symbol discussed in the section on tightness (cf. Eq. (2.44)), we also know that to model the
probability distribution over finite strings in Σ˚, we only require to keep track of strings up to
the first occurrence of the eossymbol. Therefore, when converting a given n-gram model to a
WFSA, we will only model sequences up to the first occurrence of the special symbol, meaning that
eoswill never occur in the context of any conditional distribution pSMpy|yq. We now detail this
100 CHAPTER 4. CLASSICAL LANGUAGE MODELS
construction.
LetpLNbe a well-defined n-gram language model specified by conditional distributions pSMas
defined by §4.1.5. We will now construct a WFSA representing pLN. Intuitively, its states will
represent all possible sequences of words of length nwhile the transitions between the states q1
andq2will correspond to the possible transitions between the n-grams which those represent. This
means that the only possible (positively weighted) transitions will be between the n-grams which
can follow each other, i.e. yt´n:t´1andyt´n`2:tfor someyt´n,ytPΣ(until the first occurrence of
eos). The transition’s weight will depend on the probability of observing the “new” word y0in the
second n-gram given the starting n-gramy´ny´pn´1q...y´1. Further, the final weights of the states
will correspond to ending the string in them. In pLN, this is modeled as the probability of observing
eosgiven the context yt´n:t´1—this, therefore, is set as the final weight of the state representing
the history yt´n:t´1. Formally, we can map a n-gram model into a WFSA A“pΣA,QA,δA,λA,ρAq
by constructing Aas follows.
•Automaton’s alphabet:
ΣAdef“Σ (4.64)
•The set of states:
QAdef“n´1ď
t“0tbosun´1´tˆΣt(4.65)
•The transitions set
δAdef“tyt´n:t´1yt{pSMpyt|yt´n:t´1qÝ ÝÝÝÝÝÝÝÝÝÝÝÝ Ñ yt´n`1:t| (4.66)
yt´n`1:t´1Pn´2ď
t“0tbosun´2´tˆΣt;yt´n,ytPΣu
•The initial function:
λA:yÞÑ$
&
%1ify“bos...boslooooomooooon
n´1 times
0otherwise(4.67)
•The final function
ρA:yÞÑpSMpeos|yq,yPQA (4.68)
The definition of the states set QAcaptures exactly the notion of padding with the bossymbol for
handling the edge cases we described above. This shows that n-gram language models are indeed
finite-state (we leave the formal proof showing that LpAq“LppLNqto the reader.
Defining a n-gram language model through a parametrized WFSA. We now consider
how we can use the framework of WFSA to define a more “flexible” parametrized globally normalized
model. In this case, we do not start from an existing locally normalized set of distributions forming
pSM. Rather, we would like to model the “suitability” of different n-grams following each other—that
is, we would like to somehow parametrize the probability that some n-gram y1will follow an n-gram
ywithout having to worry about normalizing the model at every step. This will allow us to then fit
the probability distributions of the model to those in the data, e.g., with techniques described in
§3.2.3. Luckily, the flexibility of the WFSA modeling framework allows us to do exactly that.
4.1. FINITE-STATE LANGUAGE MODELS 101
Subregularity
We saw that language models implementing the very natural n-gram assumption can be represented
using weighted finite-state automata. However, n-gram models do not “need the full expressive
power” of WFSAs—they can actually be modeled using even simpler machines than finite-state
automata. This, along with several other examples of simple families of formal languages, motivates
the definition of subregular languages.
Definition 4.1.25: Subregular language
A language is subregular if it can be recognized by a finite-state automaton or any weaker
machine.
Most subregular languages can indeed be recognized by formalisms which are much simpler than
FSAs. Many useful and interesting classes of subregular languages have been identified—recently,
especially in the field of phonology. Naturally, due to their simpler structure, they also allow for
more efficient algorithms—this is why we always strive to represent a language with the simplest
formalism that still captures it adequately. See J¨ ager and Rogers (2012); Avcu et al. (2017) for
comprehensive overviews of subregular languages.
Subregular languages actually form multiple hierarchies of complexity within regular languages.
Interestingly, n-gram models fall into the simplest level of complexity in one of the hierarchies,
directly above finite languages. This class of subregular languages is characterized by patterns that
depend solely on the blocks of symbols that occur consecutively in the string, which each of the
blocks considered independently of the others—it is easy to see that n-gram models intuitively
fall within such languages. This family of subregular languages is suggestively called strictly local
languages .
Definition 4.1.26: Strictly local languages
A language Lisstrictly n-local (SLn) if, for every string yof length|y|“n´1, and all
strings x1,x2,z1,z2PΣ˚, it holds that if x1yz1PLandx2yz2PL, then also x1yz2PL
(andx2yz1PL).
A language is strictly local (SL) if it is strictly n-local for any n.
Note that we could of course also define this over with the eos-augmented alphabet Σ. You can
very intuitively think of this definition as postulating that the history more than nsymbols back
does not matter anymore for determining or specifying whether a string is in a language (or its
weight, in the weighted case)—this is exactly what the n-gram assumption states.
4.1.6 Representation-based n-gram Models
So far, we have mostly talked about the conditional probabilities and the WFSA weights defining
a language model very abstractly. Apart from describing how one can generally parametrize the
weights of the underlying WFSA with the scoring function in §4.1.5, we only discussed what values
the weights can take for the language model to be well-defined and what implications that has
on the distribution defined by the WFSA. In this section, we consider for the first time what an
actual implementation of a finite-state, or more precisely, a n-gram language model might look
102 CHAPTER 4. CLASSICAL LANGUAGE MODELS
like. Concretely, we will define our first parameterized language model in our General language
modeling framework (cf. §3.1) by defining a particular form of the encoding function encas a simple
multi-layer feed-forward neural network.14
However, before we dive into that, let us consider as an alternative possibly the simplest way to
define a (locally normalized) n-gram language model: by directly parametrizing the probabilities of
each of the symbols yin the distribution pSMpy|yqfor any context y, that is
θdef“$
&
%θy|ydef“pSMpy|yq|yPΣ,yPΣn´1,θy|yě0,ÿ
y1PΣθy|y“1,
.
-. (4.69)
The following proposition shows that the maximum likelihood solution (Eq. (3.60)) to this parametriza-
tion is what you would probably expect.
Proposition 4.1.3
The MLE solution of Eq. (4.69) is
pSMpyn|yănq“Cpy1,...,y nq
Cpy1,...,y n´1q(4.70)
whenever the denominator ą0, whereCpy1,...,y nqdenotes the number of occurrences of all
possible strings of the form y1,...,y nandCpy1,...,y nqdenotes the number of occurrences of
all possible strings of the form y1,...,y n´1.
Proof. LetDdef“␣
yp1q,...,ypMq(
be the training dataset. The log-likelihood of a single example
ypmqis
logppLNpyqq“ log¨
˝|ypmq|ź
t“1pSM´
ypmq
t|ypmq
t´n:t´1¯˛
‚ (4.71)
“|ypmq|ÿ
t“1logpSM´
ypmq
t|ypmq
t´n:t´1¯
(4.72)
which means that the log-likelihood of the entire dataset is
ℓℓpDq“Mÿ
m“1|ypmq|ÿ
t“1logpSM´
ypmq
t|ypmq
t´n:t´1¯
(4.73)
“Mÿ
m“1|ypmq|ÿ
t“1logθyn|yăn. (4.74)
14While we introduce particular architectures of neural networks, for example, recurrent neural networks and
transformers later in Chapter 4, we assume some familiarity with neural networks in general. See Chapter 6 of
Goodfellow et al. (2016) for an introduction.
4.1. FINITE-STATE LANGUAGE MODELS 103
Exercise 4.2 asks you to show that this can be rewritten with the token to type switch as
ℓℓpDq“ÿ
y
|y|“nCpyqθyn|yăn. (4.75)
The maximum likelihood parameters can then be determined using Karush–Kuhn–Tucker (KKT)
conditions15to take into account the non-negativity and local normalization constraints:
∇θ¨
˚˚˝ℓℓpDq´ÿ
yPΣ˚
|y|“n´1λyy˜ÿ
yPΣθy|y´1¸
´ÿ
yPΣ˚
|y|“n´1ηyyθy|y˛
‹‹‚“0. (4.76)
Recall that the KKT conditions state that a θis an optimal solution of ℓℓif and only if´
θ,tλyy1uyPΣn´1,yPΣ,tηyy1uyPΣn´1,yPΣ¯
satisfy Eq. (4.76). Since this is simply a sum over the dataset with no interactions of parameters for
individual contexts ywith|y|“n´1 inθy|y, it can be solved for each context yindividually.
Moreover, as you are asked to show Exercise 4.3, it holds that
ÿ
y1PΣC`
y1...y n´1y1˘
“Cpy1...y n´1q (4.77)
for any y“y1...y n´1PΣn´1. This leaves us with the following system for each yPΣn´1:
ÿ
y1PΣC`
yy1˘
logθy1|y´λy˜ÿ
y1PΣθy1|y´1¸
´ÿ
y1PΣηyy1θy1|y. (4.78)
It is easy to confirm that θy|y“Cpyyq
Cpyqwithλy“Cpyqandηyy1“0 is a saddle point of Eq. (4.76).
This means that θy|y“Cpyyq
Cpyqis indeed the maximum likelihood solution. ■
This results in a locally normalized n-gram model. To avoid issues with division-by-zero and
assigning 0 probability to unseen sentences, we can employ methods such as smoothing and backoff ,
which are beyond the scope of the course.16
While this model might seem like an obvious choice, it comes with numerous drawbacks. To see
what can go wrong, consider the following example.
Example 4.1.6: n-gram model
Suppose we have a large training corpus of sentences, among which sentences like “We are
going to the shelter to adopt a dog.” ,“We are going to the shelter to adopt a puppy.” , and
“We are going to the shelter to adopt a kitten.” , however, without the sentence “We are going
to the shelter to adopt a cat.” Fitting an n-gram model using the count statistics and individual
tables of conditional probabilities pSMpy|yq, we would assign the probability
pSMpyt“cat|yăt“We are going to the shelter to adopt a q
15Seehttps://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions .
16See (Chen and Goodman, 1996) and Chapter 4 in (Jurafsky and Martin, 2009).
104 CHAPTER 4. CLASSICAL LANGUAGE MODELS
the value 0 (or some “default” probability if we are using smoothing). However, the words
“dog”, “puppy”, “kitten”, and “cat” are semantically very similar—they all describe pets often
found in shelters. It would therefore be safe to assume that the word “cat” is similarly probable
given the context “We are going to the shelter to adopt a” as the other three words observed
in the training dataset. However, if we estimate all the conditional probabilities independently ,
we have no way of using this information—the words have no relationship in the alphabet,
they are simply different indices in a lookup table. Additionally, statistics gathered for the
sentences above will not help us much when encountering very similar sentences, such as
“We went to a nearby shelter and adopted a kitten.” The issue is that there are simply many
ways of expressing similar intentions. We would thus like our language models to be able
to generalize across different surface forms and make use of more “semantic” content of the
sentences and words. However, if the model is parametrized as defined in Eq. (4.69), it is not
able to take advantage of any such relationships.
The model defined by Eq. (4.69) is therefore unable to take into account the relationships and
similarities between words. The general modeling framework defined in §3.1 allows us to remedy
this using the distributed word representations . Recall that, in that framework, we associate
each word ywith its vector representation epyq(itsembedding ), and we combine those into the
embedding matrix E. Importantly, word embeddings are simply additional parameters of the model
and can be fiton the training dataset together with the language modeling objective. One of the
first successful applications of enc p¨qis due to Bengio et al. (2003b), which we discuss next.
To be able to use the embeddings in our general framework, we now just have to define the
concrete form of the context-encoding function enc. In the case of the neural n-gram model which
we consider here and as defined by (Bengio et al., 2003b), the representations of the context yăt,
encpyătq, are defined as the output of a neural network which looks at the previous n´1 words in
the context:
encpyătqdef“encpyt´1,yt´2,...,yt´n`1q, (4.79)
where encis a neural network we define in more detail shortly. The full language model is therefore
defined through the conditional distributions
pSMpyt|yătqdef“softmax´
enc`
yt´1,yt´2,...,yt´n`1˘JE`b¯
yt(4.80)
resulting in the locally normalized model
pLNpyq“softmax´
enc`
yT,yT´1,...,yT´n`2˘JE`b¯
eos(4.81)
¨Tź
t“1softmax´
encpyt´1,yt´2,...,yt´n`1qJE`b¯
yt(4.82)
foryPΣ˚.
Importantly, notice that although this is a neural model, it is nonetheless still an n-gram model
with finite context—Eq. (4.79) is simply a restatement of the n-gram assumption in terms of the
neural encoding function enc. It therefore still suffers from some of the limitations of regular n-gram
models, such as the inability to model dependencies spanning more than nwords. However, it solves
the problems encountered in Example 4.1.6 by considering word similarities and sharing parameters
across different contexts in the form of an encoding function rather than a lookup table.
4.1. FINITE-STATE LANGUAGE MODELS 105
While encoding function encin Eq. (4.79) could in principle take any form, the original model
defined in Bengio et al. (2003b) defines the output as for the string y“yt,yt´1,...,yt´n`1as
encpyt,yt´1,...,yt´n`1qdef“b`Wx`Utanhpd`Hxq, (4.83)
where xdef“concatpepytq,epyt´1q,...,epyt´n`1qqdenotes the concatenation of the context symbol
embeddings into a long vector of size pn´1q¨R, and b,d,W, and Udefine the parameters of the
encoding function. This completes our definition of the model in the general language modeling
framework—the model can then simply be trained on the language modeling objective as defined in
§3.2.2.
We can also see that such a model also reduces the number of parameters required to specify
an-gram model: whereas a lookup-table-based n-gram model with no parameter sharing requires
Op|Σ|nqparameters to be defined, the number of parameters required by a representation-based
n-gram model scales linearly with n—all we have to do is add additional rows to the matrices defined
in Eq. (4.83). We will later see how this can be reduced to a constant number of parameters w.r.t.
the sequence length in the case of recurrent neural networks in §5.1.2.
Pictorially, we can imagine the model as depicted in Fig. 4.5 (taken from the original publication).
This shows that the n-gram modeling framework is not limited to counting co-occurrence statistics.
The model from Eq. (4.79) can also be represented by a WFSA just like the simpler models we
discussed above, with the weights on the transitions parametrized by the neural network. This
allows us to both understand well with insights from formal language theory, as well as to train
it in a flexible way allowed for by the non-linear encoding function. However, the model from
Eq. (4.79) is still limited to statistics of the last ntokens or less. If we want to model arbitrarily long
dependencies and hierarchical structures, we have to leave the space of finite-state languages behind
and develop formalisms capable of modeling more complex languages. The next section explores the
first of such frameworks: context-free languages with the computational models designed to model
them.
106 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Figure 4.5: A pictorial depiction of the n-gram neural language model from the original publication
(Bengio et al., 2003b). Note that the quantity Cpwqcorresponds to epyqfor a wordyin our notation.
4.2. PUSHDOWN LANGUAGE MODELS 107
4.2 Pushdown Language Models
An strong limitation of finite-state language models is that they can definitionally only distinguish a
finite set of contexts. However, human language has inherently more structure than what a finite set
of contexts can encode. For example, human language contains arbitrarily deep recursive structures
which cannot be captured by a finite set of possible histories—we will see an example of this soon in
§4.2.1.
To be able to model these structures we are climbing a rung higher on the ladder of the hierarchy
of formal languages: we are going to consider context-free languages , a larger class of languages
than regular languages. Luckily, we will see that a lot of the formal machinery we introduce in
this section closely follows analogs from the finite-state section and we invite the reader to pay
close attention to the parallels. For example, similarly to how we weighted a string in a regular
language by summing over the weights of the paths labeled with that string, we will weight strings
in context-free languages by summing over analogous structures.
To be able to recognize context-free languages, will have to extend finite-state automata from
§4.1.1 with an additional data structure—the stack. Finite-state automata augmented with a stack
are called pushdown automata. We introduce them in §4.2.7. Before giving a formal treatment of
pushdown automata, however, we will discuss an arguably more natural formalism for generating
the context-free languages—context-free grammars.17
In the last part of the section, we will then further extend the regular pushdown automaton with
anadditional stack. Interestingly, this will make it more powerful: as we will see, it will raise its
expressive power from context-free languages to all computable languages, as it is Turing complete.
While this augmentation will not be immediately useful from a language modeling perspective, we
will then later use this machine to prove some theoretical properties of other modern language
models we consider later in the course.
4.2.1 Human Language Is not Finite-state
As hinted above, human language contains structures that cannot be modeled by finite-state
automata. Before we introduce ways of modeling context-free languages, let us, therefore, first
motivate the need for a more expressive formalism by more closely considering a specific phenomenon
often found in human language: recursive hierarchical structure. We discuss it through an example,
based on Jurafsky and Martin (2009).
Example 4.2.1: Center embeddings
Consider the sentence:
“The cat likes to cuddle.”
It simply describes a preference of a cat. However, we can also extend it to give additional
information about the cat:
“The cat the dog barked at likes to cuddle.”
17You might wonder what non-context-free grammars are: a superclass of context-free grammars is that of context-
sensitive grammars, in which a production rule may be surrounded by a left and right context. They are still however
a set of restricted cases of general grammars, which are grammars that can emulate Turing machines.
108 CHAPTER 4. CLASSICAL LANGUAGE MODELS
This sentence, in turn, can be extended to include additional information about the dog:
“The cat the dog the mouse startled barked at likes to cuddle.”
Of course, we can continue on:
“The cat the dog the mouse the rat frightened startled barked at likes to cuddle.”
and on:
“The cat the dog the mouse the rat the snake scared frightened startled barked at likes to cuddle.”
In theory, we could continue like this for as long as we wanted—all these sentences are
grammatically correct —this is an instance of the so-called center embeddings .
Crucially, such sentences cannot be captured by a regular language, i.e., a language based
on an automaton with finitely many states. While we would need formal machinery beyond
the scope of this course to formally prove this, the intuition is quite simple. By adding more
and more “levels” of recursion to the sentences (by introducing more and more animals in
the chain), we unboundedly increase the amount of information the model has to “remember”
about the initial parts of the sentence while processing it sequentially, to be able to process
or generate the matching terms on the other end of the sentence correctly. Because such
hierarchies can be arbitrarily deep (and thus the sentences arbitrarily long), there is no bound
on the number of states needed to remember them, which means they cannot be captured by
a finite-state automaton.
Note that this example also touches upon the distinction of the grammatical competence versus
grammatical performance (Chomsky, 1959; Chomsky and Sch¨ utzenberger, 1963; Chomsky,
1965). The former refers to the purely theoretical properties of human language, for example,
the fact that such hierarchical structures can be arbitrarily long and still grammatically correct.
Grammatical performance, on the other hand, studies language grounded more in the way
people actually use it. For example, nested structures like the one above are never very deep
in day-to-day speech—indeed, you probably struggled to understand the last few sentences
above. We rarely come across nestings of depth more than three in human language (Miller
and Chomsky, 1963; Jin et al., 2018; Karlsson, 2007).
4.2.2 Context-free Grammars
How can we capture recursive structures like those in Example 4.2.1 and the long-term dependencies
arising from them? The first formalism modeling such phenomena we will introduce is context-free
grammars: a generative formalism which can tell us how to generate or “compose“ strings in the
language it describes. Later in the section ( §4.2.7), we will introduce the context-free analog of
finite-state automata, which will tell us how to recognize whether a string is in a context-free
language (rather than generate a string): pushdown automata.
Definition 4.2.1: Context-free Grammar
Acontext-free grammar (CFG) is a 4-tuple G“pΣ,N,S,Pqwhere Σ is an alphabet of
terminal symbols, Nis a non-empty set of non-terminal symbols with NXΣ“H, SPNis
4.2. PUSHDOWN LANGUAGE MODELS 109
the designated start non-terminal symbol and Pis the set of production rules, where each
rulepPPis of the form X Ñαwith XPNandαPpNYΣq˚.a
aAs is the case for initial states in FSAs, multiple start symbols could be possible. However we consider
only one for the sake of simplicity.
Example 4.2.2: A simple context-free grammar
LetG“pΣ,N,S,Pqbe defined as follows:
•Σ“ta,bu
•N“tXu
•S“X
•P“tXÑaXb,XÑεu
This defines a simple context-free grammar. We will return to it later, when we will formally
show that it generates the language L“tanbn|nPNě0u.
Rule Applications and Derivations
Context-free grammars allow us to generate strings yPΣ˚byapplying production rules on its
non-terminals. We apply a production rule XÑαto XPNin a rulepby taking X on the
right-hand side of pand replacing it with α.18
Definition 4.2.2: Rule Application
A production rule Y Ñβ,βPpNYΣq˚,isapplicable to Y in a rule p, ifptakes the form
XÑαYγ,α,γPpNYΣq˚.
Theresult of applying YÑβtoαYγisαβ γ .
Starting with S, we apply SÑαto S for some pSÑαqPP, then take a non-terminal in αand
apply a new production rule.19To generate a string we follow this procedure until all non-terminal
symbols have been transformed into terminal symbols. The resulting string, i.e., the yield , will be
the string taken by concatenating all terminal symbols read from left to right. More formally, a
derivation can be defined as follows.
18We say that X is on the right-hand side of a rule pifptakes the form p“pYÑαXγq, where α,γPpNYΣq˚.
We will sometimes refer to X as the head of the production rule XÑα, and the right-hand side αas the body of the
production rule.
19We will write X Pα, which formally means a substring of αwith length 1. Unless otherwise stated, X can be
either a non-terminal or a terminal.
110 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Definition 4.2.3: Derivation
Aderivation in a grammar Gis a sequence α1,...,αM, where α1PN,α2,...,αM´1P
pNYΣq˚andαMPΣ˚, in which each αm`1is formed by applying a production rule in Pto
αm.
We say that αPpNYΣq˚is derived from X PNif we can apply a finite sequence of production
rules to generate αstarting from X. We will denote this as X˚ñGα. See the following formal
definition.
Definition 4.2.4: Derives
LetGdef“pΣ,N,S,Pqbe a CFG. We say that X derives βunder the grammar G, denoted
as XñGβifDpPPsuch thatp“pXÑαβ γq,α,γPpNYΣq˚andβPpNYΣq˚ztεu. The
special case X ñGεholds iff XÑεPP. We denote the reflexive transitive closure of the ñG
relation as˚ñG. We say that βisderived from X if X˚ñGβ.
The (context-free) language of a CFG Gis defined as all the strings yPΣ˚that can be derived
from the start symbol S of G, or alternatively, the set of all yields possible from derivations in G
that start with S. We will denote the language generated by GasLpGq.
Definition 4.2.5: Language of a Grammar
Thelanguage of a context-free grammar Gis
LpGq“tyPΣ˚|S˚ñGyu (4.84)
Parse Trees and Derivation Sets
A natural representation of a derivation in a context-free grammar is a derivation tree d(also
known as a parse tree). A derivation tree represents the sequence of applied rules in a derivation
with a directed tree. The tree’s internal nodes correspond to the non-terminals in the derivation,
and each of their children corresponds to a symbol (from Σ YN) on the right side of the applied
production in the derivation. The leaves, representing terminal symbols, “spell out” the derived
string—the tree’s yield. More formally, for each production rule XÑα, the node corresponding to
the specific instance of the non-terminal X in the derivation is connected to the nodes corresponding
to YPαwhere YPΣYN.
We will mostly be interested in representing derivations starting with S—the root node of a tree
representing any such derivation will correspond to S. We will denote the string generated by a tree
d—its yield—by spdq. See Fig. 4.6 for examples of parse trees for the grammar from Example 4.2.2.
Importantly, a grammar may in fact admit multiple derivations and hence multiple derivation
trees for any given string.
4.2. PUSHDOWN LANGUAGE MODELS 111
X
εX
bX
εaX
b X
bX
εaaX
b X
b X
bX
εaaa
Figure 4.6: A sequence of derivation trees for the strings in tanbn|n“0,1,2,3uin the grammar
from Example 4.2.2.
X
εX
Y
ε
Figure 4.7: Two parse trees in the modified grammar Gyieldingε.
Example 4.2.3: Multiple derivation strings
It is relatively easy to see that in the grammar G, each string anbnis only generated by a single
derivation tree—each new pair of symbols aandbcan only be added by applying the rule
XÑaXband the string anbncan only be generated by the application of the rule XÑaXbn
times and the rule X Ñεonce in this order.
However, we can modify Gby adding, for instance, a non-terminal Y and rules XÑY,YÑε.
The empty string εmay then be derived either by pXÑεq, orpXÑYq,pYÑεq, corresponding
to two separate derivation trees, as shown in Fig. 4.7. The set of these two trees comprises
what we call the derivation set of ε.
We denote a derivation set of a string y, generated by the grammar G, asDGpyq.
Definition 4.2.6: String derivation set
LetyPΣ˚. Itsderivation set , denoted by DGpyqis defined as
DGpyqdef“td|spdq“yu. (4.85)
We say that a grammar is unambiguous if, for every string that can be generated by the
grammar, there is only one associated derivation tree.
112 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Definition 4.2.7: Unambiguity
A grammar Gisunambiguous if for all yPLpGq,|DGpyq|“1.
The converse holds for ambiguous grammars.
Definition 4.2.8: Ambiguity
A grammar Gisambiguous ifDyPLpGqsuch that|DGpyq|ą1.
The set of all derivation trees in a grammar is its derivation set.
Definition 4.2.9: Grammar derivation set
Thederivation set of a grammar ,DG, is the set of all derivations possible under the
grammar. More formally, it can be defined as the union over the derivation set for the strings
in its language,
DGdef“ď
y1PLpGqDGpy1q (4.86)
Definition 4.2.10: Non-terminal derivation set
Thederivation set of a non-terminal YPNinG, denoted DGpYq, is defined as the set of
derivation subtrees with root node Y.
Note that DGcould be defined as DGpSq. For a terminal symbol aPΣ, we trivially define the
derivation set DGpaqto be empty.20
In cases where it is irrelevant to consider the order of the production rules in a derivation tree,
we will writepXÑαqPdto refer to specific production rules in the tree—viewing trees as multisets
(or bags) over the production rules they include.
Example 4.2.4: Nominal Phrases
CFGs are often used to model natural languages. Terminals would then correspond to words in
the natural language, strings would be text sequences and non-terminals would be abstractions
over words. As an example, consider a grammar Gthat can generate a couple of nominal
phrases. We let N“tAdj,Det,N,Nominal,NPu, Σ“ta, big, female, giraffe, male, tall, the u,
S“Nominal and define the following production rules:
NominalÑDet NP
NPÑN|Adj NP
DetÑa|the
NÑfemale|giraffe|male
AdjÑbig|female|male|tall
20Empty derivation sets for terminal symbols is defined solely for ease of notation later.
4.2. PUSHDOWN LANGUAGE MODELS 113
Nominal
NP
N
giraffeDet
aNominal
NP
NP
N
maleAdj
bigDet
theNominal
NP
NP
NP
N
giraffeAdj
femaleAdj
tallDet
a
Figure 4.8: Derivation trees for natural language nominal phrases.
See Fig. 4.8 for a few examples of derivation trees in this grammar.
Example 4.2.5: The generalized Dyck languages Dpkq
A very widely studied family of context-free languages are the Dyck- klanguages, Dpkq, the
languages of well-nested brackets of ktypes. They are, in some ways, archetypal context-free
languages (Chomsky and Sch¨ utzenberger, 1963). Formally, we can define them as follows.
Definition 4.2.11: Dpkqlanguages
LetkPN. The Dpkqlanguage is the language of the following context-free grammar
Gdef“pΣ,N,S,Pq
•Σdef“txn|n“1,...,kuYtyn|n“1,...,ku
•Ndef“tSu
•Sdef“S
•Pdef“tSÑε,SÑSSuYt SÑxnSyn|n“1,...,ku
Examples of strings in the language Dp3qwould bex3y3x2y2x1y1,x3y3x1x2x2y2y2y1, and
x1x2x2y2y2x3x1y1y3y1. The stringx2x2y1y2is not in the language D p3q.
To give you a taste of what formally working with context-free grammars might look like,
we now formally show that the grammar from Example 4.2.2 really generates the language L“
tanbn|nPNě0u, as we claimed.
114 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Example 4.2.6: Recognizing anbn
The language L“tanbn|nPNuis not regular.aHowever, we can show that it is context-free
and recognized exactly by the simple grammar from Example 4.2.2. We restate it here for
convenience: G“pΣ,N,S,PqwithN“tXu, Σ“ta,bu, S“X,P“tXÑaXb,XÑεu.
Lemma 4.2.1
Given the grammar Gdefined above, we have LpGq“tanbn|nPNu.
Proof. We will show that L“LpGqin two steps: (i) showing that LĎLpGqand (ii) showing
thatLpGqĎL. Define yn“anbn.
(i) We first need to show that each yPLcan be generated by G, which we will do by induction.
Base case ( n“0)We have that y0“ε, which is generated by d“pXÑεq.
Inductive step ( ną1)We have that ynis generated by
d“pXÑaXbq¨¨¨p XÑaXbqloooooooooooooooomoooooooooooooooon
ntimespXÑεq.
It is then easy to see that yn`1is generated by the derivation we get by replacing the last
rulepXÑεqwithpXÑaXbqpXÑεq—they are exactly the trees illustrated in Fig. 4.6.
(ii) Next, we show that for each dPDG, we have that ypdqPL.
Base case ( d“pXÑεq)It is trivial to see that the derivation d“pXÑεqyields ypdq“ε.
Inductive step Now observe that Ponly contains two production rules and one non-terminal.
Starting with X, we can either apply XÑaXbto get one new non-terminal X, or apply XÑε
to terminate the process. Hence, if we fix the length of the sequence of production rules, there
is no ambiguity in which string will be generated. Thus, by induction, we conclude that if
we have a derivation tree given by pXÑaXbq,...,pXÑaXbqlooooooooooooooooomooooooooooooooooon
ntimes,pXÑεqgenerating anbn, the
derivation tree given by pXÑaXbq,...,pXÑaXbqlooooooooooooooooomooooooooooooooooon
n`1 times,pXÑεqwill generate an`1bn`1.■
aAgain, while the intuition behind this is similar to our reasoning from Example 4.2.1, this would have to
be proven using the so-called pumping lemma for regular languages.
Reachable Non-terminals and Pruning
Similarly to how some states in a WFSA can be useless in the sense that they are not accessible
from an initial state or might not lead to a final state, so too can non-terminals in a CFG be useless
by not beaing reachable from the start symbol or might not lead to any string of terminals. In the
context of CFGs, we typically use a different terminology: “reachable” instead of “accessible” and
“generating” instead of “co-accessible” .
4.2. PUSHDOWN LANGUAGE MODELS 115
Definition 4.2.12: Accessibility for CFGs
A symbol XPNYΣ isreachable (or accessible) if Dα,α1PpNYΣq˚such that S˚ñαXα1.
Definition 4.2.13: Co-accessibility for CFGs
A non-terminal Y is generating (or co-accessible) if DyPΣ˚such that Y˚ñy.
In words, reachable symbols are those that can be derived from the start symbol, whereas
generating non-terminals are those from which at least one string (including the empty string) can
be derived. Note that we define reachable for both non-terminals and terminals while generating is
only defined for non-terminals.
This allows us to define a pruned context-free grammar, which is the CFG version of a trimmed
WFSA.
Definition 4.2.14: Pruned CFG
A CFG is pruned (or trimmed) if it has no useless non-terminals, i.e. all non-terminals
are both reachable and generating. Pruning (or trimming) refers to the removal of useless
non-terminals.
4.2.3 Weighted Context-free Grammars
As we did with finite-state automata, we will augment the classic, unweighted context-free grammars
with real-valued weights. We do that by associating with each rule XÑαa weight WpXÑαqPR.
Definition 4.2.15: Weighted Context-free Grammar
Areal-weighted context-free grammar is a 5-tuplepΣ,N,S,P,Wqwhere Σ is an alphabet
of terminal symbols, Nis a non-empty set of non-terminal symbols with NXΣ“H, SPNis
the designated start non-terminal symbol, Pis the set of production rules, and Wa function
W:PÑR, assigning each production rule a real-valued weight.
For notational brevity, we will denote rules pPPasp“XwÝ Ñαfor XPN,αPpNYΣq˚and
w“WpXÑαqPR.
Example 4.2.7: A simple weighted context-free grammar
Consider the grammar G“pΣ,N,S,P,Wqdefined as follows:
•Σ“ta,bu
•N“tXu
•S“X
•P“tXÑaXb,XÑεu
116 CHAPTER 4. CLASSICAL LANGUAGE MODELS
•W“␣
XÑaXbÞÑ1
2,XÑεÞÑ1
2(
This defines a simple weighting of the CFG from Example 4.2.2.
Weights assigned to productions by WFCGs can be arbitrary real numbers. Analogous to
probabilistic WFSAs (Definition 4.1.17) describing locally normalized finite-state language models,
we also define probabilistic WCFGs, where the weights of applicable production rules to any non-
terminal form a probability distribution.
Definition 4.2.16: Probabilistic Context-free grammar
A weighted context-free grammar G“pΣ,N,S,P,Wqisprobabilistic if the weights of the
productions of every non-terminal are non-negative and sum to 1, i.e., for all X PN, it holds
that
@XÑαPP,WpXÑαqě0 (4.87)
and ÿ
XÑαPPWpXÑαq“1 (4.88)
Intuitively, this means that all the production weights are non-negative and that, for any left
side of a production rule X, the weights over all production rules XÑαsum to 1. The grammar
from Example 4.2.7 is, therefore, also probabilistic.
Again analogously to the WFSA case, we say that a string yis in the language of WCFG Gif
there exists a derivation tree dinGcontaining only non-zero weights with yield spdq“y.
Tree Weights, String Weights, and Allsums
In the case of regular languages, we discussed how individual strings are “produced” by paths
in the automaton (in the sense that each path yields a string). As Example 4.2.4 showed, the
structures that “produce” or yield strings in a context-free grammar are trees—those, therefore, play
an analogous role in context-free grammars to paths in finite-state automata.
Just like we asked ourselves how to combine individual transition weights in a WFSA into weights
of entire paths and later how to combine those into weights of strings, we now consider the questions
of how to combine the weights of individual production rules into the weight of entire trees and
later also individual strings. We start by giving a definition of the weight of a tree as the product
over the weights of all the rules in the tree, i.e., as a multiplicatively decomposable function over the
weights of its rules. As you can probably foresee, we will then define the weight of a string as the
sum over all the trees that yield that string.
Definition 4.2.17: Weight of a derivation tree
Theweight of a derivation tree dPDGdefined by a WCFG Gis
wpdq“ź
pXÑαqPdWpXÑαq. (4.89)
4.2. PUSHDOWN LANGUAGE MODELS 117
The stringsum or the string acceptance weight of a particular string under a grammar is then
defined as follows:
Definition 4.2.18: Stringsum in a context-free grammar
Thestringsum Gpyqof a string ygenerated by a WCFG Gis defined by
Gpyq“ÿ
dPDGpyqwpdq (4.90)
“ÿ
dPDGpyqź
pXÑαqPdWpXÑαq (4.91)
Lastly, analogously to the allsum in WFSAs, an allsum is the sum of the weights of all the trees
in a WCFG. We first define the allsum for symbols (non-terminals and terminals).
Definition 4.2.19: Nonterminal allsum in a context-free grammar
Theallsum for a non-terminal Y in a grammar Gis defined by
ZpG,Yq“ÿ
dPDGpYqwpdq (4.92)
“ÿ
dPDGpYqź
pXÑαqPdWpXÑαq (4.93)
The allsum for a terminal aPΣYtεuis defined to be
Zpaqdef“1. (4.94)
The allsum for a grammar is then simply the allsum for its start symbol.
Definition 4.2.20: Allsum in a context-free grammar
Theallsum of a weighted context-free grammar G“pΣ,N,S,P,Wqis
ZpGq“ZpG,Sq (4.95)
“ÿ
dPDGpSqwpdq (4.96)
“ÿ
dPDGpSqź
pXÑαqPdWpXÑαq (4.97)
When the grammar Gwe refer to is clear from context, we will drop the subscript and write e.g.
ZpSq.
Although we can in some cases compute the allsum of a WCFG in closed form, as we will see in
the example below, we generally require some efficient algorithm to be able to do so.
118 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Example 4.2.8: Geometric Series as an Allsum
Consider the WCFG G“pΣ,N,S,P,Wq, given by N“tXu, Σ“tau, S“X, and the rules:
X1{3Ý Ý ÑaX
X1Ý Ñε
The language generated by GisLpGq“tan|ně0u. Further note that this grammar is
unambiguous – each string y“am, for somemě0, is associated with the derivation tree
given bypX1{3Ý Ý ÑaXq,...,pX1{3Ý Ý ÑaXqloooooooooooooooooomoooooooooooooooooon
mtimes,pX1Ý Ñεq. Due to the multiplicative decomposition over
the weights of the rules, the weight associated with each derivation tree dwill hence be
wpdq“ˆ1
3˙m
ˆ1“ˆ1
3˙m
Accordingly, we can compute the allsum of Gusing the closed-form expression for geometric
series:
ZpGq“8ÿ
m“0ˆ1
3˙m
“1
1´1{3“3
2
Just like we defined normalizable WFSAs, we also define normalizable WCFSs in terms of their
allsum.
Definition 4.2.21: Normalizable Weighted Context-free Grammar
A weighted context-free grammar Gisnormalizable ifZpGqis finite, i.e., ZpGqă8 .
4.2.4 Context-free Language Models
This brings us to the definition of context-free language models.
Definition 4.2.22: Context-free language model
A language model pLMiscontext-free if its weighted language equals the language of some
weighted context-free grammar, i.e., if there exists a weighted context-free grammar Gsuch
thatLpGq“LppLMq.
Going the other way—defining string probabilities given a weighted context-free grammar—there
are again two established ways of defining the probability of a string in its language.
String Probabilities in a Probabilistic Context-free Grammar
In a probabilistic CFG (cf. Definition 4.2.16), any production from a non-terminal X PNis
associated with a probability. As the probabilities of continuing a derivation (and, therefore, a
derivation tree) depend solely on the individual terminals (this is the core of context-free grammars!),
4.2. PUSHDOWN LANGUAGE MODELS 119
it is intuitive to see those probabilities as conditional probabilities of the new symbols given the
output generated so far. One can, therefore, define the probability of a path as the product of these
individual “conditional” probabilities.
Definition 4.2.23: Tree probability in a PCFG
We call the weight of a tree dPDGin a probabilistic CFG the probability of the tree d.
This alone is not enough to define the probability of any particular string yPΣ˚since there
might be multiple derivations of y. Naturally, we define the probability of yas the sum of the
individual trees that generate it:
Definition 4.2.24: String probability in a PCFG
We call the stringsum of a string yPΣ˚in a probabilistic CFG Gtheprobability of the
string y:
pGpyqdef“Gpyq. (4.98)
These definitions and their affordances mirror the ones in probabilistic finite-state automata (cf.
§4.1.2): they again do not require any normalization and are therefore attractive as the summation
over all possible strings is avoided. Again, the question of tightness of such models comes up: we
explore it question in §4.2.5.
String Probabilities in a General Weighted Context-free Grammar
To define string probabilities in a general weighted CFG, we use the introduced notions of the
stringsum and the allsum—we normalize the stringsum to define the globally normalized probability
of a string yas the proportion of the total weight assigned to all strings that is assigned to y.
Definition 4.2.25: String probability in a WCFG
LetG“pΣ,N,S,P,Wqbe a normalizable WCFG with non-negative weights. We define the
probability of a string yPΣ˚under Gas
pGpyqdef“Gpyq
ZpGq. (4.99)
Language Models Induced by a Weighted Context-free Grammar
With the notions of string probabilities in both probabilistic and general weighted CFGs, we can
now define the language model induced by Gas follows.
120 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Definition 4.2.26: A language model induced by a WCFG
LetG“pΣ,N,S,P,Wqbe a WCFG. We define the language model induced by Gas the
following probability distribution over Σ˚
pLMGpyqdef“pGpyq. (4.100)
Again, it is easy to see that while global normalization requires the computation of the allsum,
language models induced by weighted FSAs through Eq. (4.99) are globally normalized and thus
always tight. The tightness of probabilistic WCFGs is discussed next, after which we investigate the
relationship between globally- and locally-normalized context-free grammars.
4.2.5 Tightness of Context-free Language Models
Again, an advantage of globally normalized context-free language models (grammars) is that they
are always tight, as the derivation trees are explicitly normalized with the global normalization
constant such that they sum to 1 over the set of possible sentences.
In this section, we, therefore, consider the tightness of probabilistic context-free grammars. We
follow the exposition from Booth and Thompson (1973). The proof requires the use of multiple new
concepts, which we first introduce below.
Definition 4.2.27: Generation level
We define the level of a generation sequence inductively as follows. The zeroth level γ0of
a generation sequence is defined as S. Then, for any ną0,γncorresponds to the string is
obtained by applying the applicable productions onto allnonterminals of γn´1.
Example 4.2.9: Generation levels
LetG“pΣ,N,S,Pqwith Σ“ta,bu,N“tS,X,Yu,
andP“tSÑaX Y,XÑY X,XÑbY Y,YÑaaY,YÑau. Then the generation sequence
of the string aabaaaaa would be
γ0“S (definition)
γ1“aXY (applying S ÑaX Y)
γ2“aYXaaY (applying X ÑY X, YÑaaY)
γ3“aabYYaaaa Y (applying Y Ña, XÑbY Y, XÑaaY)
γ3“aabaaaaaaa (applying YÑa, YÑa, YÑa)
We will also rely heavily on generating functions. A generating function is simply a way of
representing an infinite sequence by encoding its elements as the coefficients of a formal power series .
Unlike ordinary series such as the geometric power series from Example 4.2.8, a formal power series
does not need to converge: in fact, at its core a generating function is not actually regarded as a
function —its “variables” are indeterminate and they simply serve as “hooks” for the numbers in the
sequence.
4.2. PUSHDOWN LANGUAGE MODELS 121
Definition 4.2.28: Production generating function
LetGdef“pΣ,N,S,P,Wqbe a PCFG and Ndef“|N|. For each X nPN, define its production
generating function as
gps1,...,sNqdef“ÿ
XnÑαWpXnÑαqsr1pαq
1sr2pαq
2¨¨¨¨¨srNpαq
N, (4.101)
where rmpαqdenotes the number of times the nonterminal X mPNappears in αPpΣYNq˚.
Example 4.2.10: Tightness of a context-free grammar
Let G“ p Σ,N,S,Pqwith Σ“ ta,bu,N“ t S,Xu, and P“
tSÑaS X,SÑb,XÑaX X,XÑaau. Then
g1ps1,s2q“WpSÑaSXqs1s2`WpSÑbq
g2ps1,s2q“WpXÑaXXqs2
2`WpXÑaaq
Definition 4.2.29: Generating function
Thegenerating function of thelthlevel is defined as
G0ps1,...,sNqdef“s1 (4.102)
G1ps1,...,sNqdef“g1ps1,...,sNq (4.103)
Glps1,...,sNqdef“Gl´1pg1ps1,...,sNq,...,gNps1,...,sNqq, (4.104)
that is, the lth-level generating function is defined as the l´1st-level generating function
applied to production generating functions as arguments.
Example 4.2.11: Tightness of a context-free grammar
For the grammar from Example 4.2.10, we have
G0ps1,s2q“s1
G1ps1,s2q“gps1,s2q“WpSÑaSXqs1s2`WpSÑbq
G2ps1,s2q“WpSÑaSXqrg1ps1,s2qsrg2ps1,s2qs`WpSÑbq
“WpSÑaSXq2WpXÑaXXqs1s3
2
`WpSÑaSXq2WpXÑaaqs1s2
`WpSÑaSXqWpSÑbqWpXÑaXXqs2
2
`WpSÑaSXqWpSÑbqWpXÑaaq
`WpSÑbq
122 CHAPTER 4. CLASSICAL LANGUAGE MODELS
We can see that a generating function Glps1,...,sNqcan be expressed as
Glps1,...,sNq“Dlps1,...,sNq`Cl (4.105)
where the polynomial Dlps1,...,sNqdoes not contain any constant terms. It is easy to see that the
constantClthen corresponds to the probability of all strings that can be derived in llevels or fewer.
This brings us to the following simple lemma.
Lemma 4.2.2
A PCFG is tight if and only if
lim
lÑ8Cl“1. (4.106)
Proof. Suppose that limlÑ8Clă1. This means that the generation process can enter a generation
sequence that has a non-zero probability of not terminating—this corresponds exactly to it not
being tight.
On the other hand, limlÑ8Cl“1 implies that no such sequence exists, since the limit represents
the probability of all strings that can be generated by derivations of a finite number of production
rules. ■
The rest of the section considers necessary and sufficient conditions for Eq. (4.106) to hold. For
this, we first define the first-moment matrix of a PCFG.
Definition 4.2.30: First-moment matrix
LetGdef“pΣ,N,S,P,Wqbe a PCFG. We define its first-moment matrix (its mean matrix)
EPRNˆNas
Enmdef“Bgnps1,...,sNq
Bsmˇˇˇˇˇ
s1,...,sN“1. (4.107)
Note thatEnmrepresents the expected number of occurrences of the non-terminal X min the set
of sequences αwith XnñGα, i.e., the set of sequences X ncan be rewritten into:
Enm“ÿ
XnÑαWpXnÑαqrmpαq. (4.108)
The informal intuition behind this is the following: each of the terms WpXnÑαqsr1pαq
1sr2pαq
2¨¨¨¨¨
srNpαq
N ingncontains the information about how many times any non-terminal X mappears in the
production rule XnÑαas well as what the probability of “using” or applying that production
rule to X nis. Differentiating WpXnÑαqsr1pαq
1sr2pαq
2¨¨¨¨¨srNpαq
N w.r.t.smthen “moves” the
coefficient rmcorresponding to the number of occurrences of X minXnÑαin front of the term
WpXnÑαqsr1pαq
1sr2pαq
2¨¨¨¨¨srNpαq
N ingn, effectively multiplying the probability of the occurrence
of the rule with the number of terms X min the rule—this is exactly the expected number of
occurrences of X mfor this particular rule, averaging over all possible rules that could be applied.
Summing over all applicable production rules for X n(which form a probability distribution) gives us
the total expected number of occurrences of X m. This brings us to the core theorem of this section
characterizing the tightness of PCFGs.
4.2. PUSHDOWN LANGUAGE MODELS 123
Theorem 4.2.1: A sufficient condition for the tightness of probabilistic context-free
grammars
A PCFG is tight if |λmax|ă1 and is non-tight if |λmax|ą1, whereλmaxis the eigenvalue of
Ewith the largest absolute value.
Proof. The coefficient of the term sr1
1sr2
2¨¨¨¨¨srN
Nin the generating function Glps1,...,sNqcorresponds
to the probability that there will be r1non-terminal symbols X 1, . . . ,rNnon-terminal symbols X N
in thelthlevel of the generation sequence. In particular, if the grammar is tight, this means that
lim
lÑ8Glps1,...,sNq“lim
lÑ8rDlps1,...,sNq`Cls“1. (4.109)
This, however, is only true if
lim
lÑ8Dlps1,...,sNq“0 (4.110)
and this, in turn, can only be true if limlÑ8rn“0 for alln“1,...N . The expected value of rnat
levellis
rl,n“BGlps1,...,sNq
Bsnˇˇˇˇˇ
s1,...,sN“1. (4.111)
Reasoning about this is similar to the intuition behind the first-moment matrix, with the difference
that we are now considering the number of occurrences after a sequence of lapplications. Denoting
rldef“rrl,1,..., rl,Ns (4.112)
we have
rl“«Nÿ
j“1BGl´1pg1ps1,...,sNq,...,gNps1,...,sNqq
Bgj(4.113)
¨Bgj
Bsnps1,...,sNq|n“1,...,Nffˇˇˇˇˇ
s1,...,sN“1(4.114)
“rl´1E. (4.115)
Applying this relationship repeatedly, we get
rl“r0El“r1,0,..., 0sEl, (4.116)
meaning that
lim
lÑ8rl“0iff lim
lÑ8El“0. (4.117)
The matrix Esatisfies this condition if |λmax|ă1. On the other hand, if |λmax|ą1, the limit
diverges. ■
Note that the theorem does not say anything about the case when |λmax|“1.
We conclude the subsection by noting that, interestingly, weighted context-free grammars trained
on data with maximum likelihood are always tight (Chi and Geman, 1998; Chi, 1999). This is not
the case for some models we consider later, e.g., recurrent neural networks (cf. §5.1.2).
124 CHAPTER 4. CLASSICAL LANGUAGE MODELS
4.2.6 Normalizing Weighted Context-free Grammars
Having investigated probabilistic context-free grammars in terms of their tightness, we now turn out
attention to general weighted context-free grammars, which define string probabilities using global
normalization (cf. Eq. (4.99)). To be able to compute these probabilities, require a way to compute
the normalizing constant ZpGqand the stringsum Gpyq. In the section on finite-state automata, we
explicitly presented an algorithm for computing the normalizing constant ZpAq. The derivation of
a general allsum algorithm for weighted context-free grammars, on the other hand, is more involved
and beyond the scope of this course.21Here, we simply assert that there are ways of computing the
quantities in Eq. (4.99) and only consider the following result:
Theorem 4.2.2: PCFGs and WCFGs are equally expressive (Smith and Johnson,
2007)
Normalizable weighted context-free grammars with non-negative weights and tight probabilistic
context-free grammars are equally expressive.
Proof. To prove the theorem, we have to show that any WCFG can be written as a PCFG and vice
versa.22
ðSince any tight probabilistic context-free grammar is simply a WCFG with ZpGq“1, this
holds trivially.
ñWe now show that, for any WCFG, there exists a PCFG encoding the same language model.
LetGG“pΣ,N,S,P,Wqbe a pruned WCFG that encodes a distribution over Σ˚using Eq. (4.99).
We now construct a tight probabilistic context-free grammar GL“pΣ,N,S,P,WLqwhose language
is identical. Notice that all components of the grammar remain identical apart from the weighting
function. This means that the derivations of the strings in the grammars remain the same (i.e.,
DGG“DGL)—only the weights of the derivations change, as we detail next. We define the production
weights of the probabilistic CFG as follows.
WGLpXÑαqdef“WpXÑαqś
YPαZpG,Yq
ZpG,Xq(4.118)
Remember that Zpaq“1 foraPΣ. Note that the assumption that Gis pruned means that all the
quantities in the denominators are non-zero.
It is easy to see that the weight defined this way are non-negative due to the non-negativity of
G’s weights. Furthermore, the weights of all production rules for any non-terminal X PNsum to 1,
21The allsums of individual non-terminals can be expressed as solutions to a nonlinear set of equations. Again, the
interested reader should have a look at the Advanced Formal Language Theory course.
22Again, by “written as”, we mean that the weighted language is the same.
4.2. PUSHDOWN LANGUAGE MODELS 125
as by the definitions of WGLandZpG,Xqwe have
ÿ
XÑαWGLpXÑαq“ÿ
XÑαWpXÑαqś
YPαZpG,Yq
ZpG,Xq(4.119)
“1
ZpG,Xqÿ
XÑαWpXÑαqź
YPαZpG,Yq (4.120)
“1
ZpG,XqZpG,Xq (4.121)
“1 (4.122)
We now have to show that the probabilities assigned by these two grammars match. We will
do that by showing that the probabilities assigned to individual derivations match, implying that
stringsums match as well. The probability of a derivation is defined analogously to a probability
of a string, i.e., pGpdq “wpdq
ZpGq(whereZpGq “ 1 for tight probabilistic grammars). Let then
dPDG“DGL. Then
pGLpdq“ź
XÑαPdWGLpXÑαq (4.123)
“ź
XÑαPdWpXÑαqś
YPαZpG,Yq
ZpG,Xq(definition of WGL). (4.124)
Notice that by multiplying over the internal nodes of the derivation tree, Eq. (4.123) includes the
non-terminal allsum of each internal (non-root and non-leaf) non-terminal in the derivation twice :
once as a parent of a production in the denominator, and once as a child in the numerator. These
terms, therefore, all cancel out in the product. The only terms which are left are the allsums of the
leaf nodes—the terminals—which are 1, and the allsum of the root node—S—which equals ZpGGq
and the weights of the individual productions, which multiply into the weight assigned to dby the
original grammar GG. This means that
pGLpdq“1
ZpG,Xqź
XÑαPdWpXÑαq“1
ZpG,Xqwpdq“pGGpdq, (4.125)
finishing the proof. ■
This means that the classes of probabilistic and weighted context-free grammars are in fact equally
expressive . In other words, this result is analogous to Theorem 4.1.1 in WFSAs: it shows that in
the context of context-free language models, the locally normalized version of a globally-normalized
model is alsocontext-free.
4.2.7 Pushdown Automata
We presented context-free grammars as a formalism for specifying and representing context-free
languages. Many algorithms for processing context-free languages, for example, the allsum algorithms
and their generalizations, can also be directly applied to context-free grammars. However, it is also
convenient to talk about processing context-free languages in terms of computational models in the
126 CHAPTER 4. CLASSICAL LANGUAGE MODELS
form of automata, i.e., the recognizer of the language.23As we mentioned, the types of automata
we considered so far, (weighted) finite-state automata, can only recognize regular languages. To
recognize context-free languages, we must therefore extend finite-state automata.24We do that by
introducing pushdown automata (PDA), a more general and more expressive type of automata.
Single-stack Pushdown Automata
Pushdown automata augment finite-state automata by implementing an additional stack memory
structure for storing arbitrarily long strings from a designated alphabet, which allows them to work
with unbounded memory effectively. Abstractly, this unbounded memory is the only difference to
finite-state automata. However, the definition looks slightly different:
Definition 4.2.31: Pushdown automaton
Apushdown automaton (PDA) is a tuple Pdef“pΣ,Q,Γ,δ,pqι,γιq,pqφ,γφqq, where:
•Qis a finite set of states;
•Σ is a finite set of input symbols called the input alphabet;
•Γ is a finite set of stack symbols called the stack alphabet;
•δĎQˆΓ˚ˆpΣYtεuqˆQˆΓ˚is a multiset representing the transition function;
•pqι,γιqis called the initial configuration and pqφ,γφqis called the final configuration,
whereqι,qφPQandγι,γφPΓ˚.
The initial and final configurations in pushdown play analogous roles to the sets of initial and
final sets of finite-state automata. Compared to the latter, they also allow for different starting
configurations of the stack coupled with each possible initial or final state.
Stacks are represented as strings over Γ, from bottom to top. Thus, in the stack γ“X1X2¨¨¨Xn,
the symbol X1is at the bottom of the stack, while Xnis at the top. γ“H denoses the empty
stack.
Definition 4.2.32: Configuration of a pushdown automaton
Aconfiguration of a PDA is a pair pq,γq, whereqPQis the current state and γPΓ˚is the
current contents of the stack.
The initial and final configurations of a PDA are examples of configurations; it is possible to
generalize the initial and final stacks to (say) regular expressions over Γ, but the above definition
suffices for our purposes.
23This relationship between a formalism specifying how to generate (i.e., a grammar) and a model of recognizing a
language can be seen in multiple levels of the hierarchy of formal languages. In the case of context-free languages, the
former are context-free grammars, while the latter are pushdown automata discussed in this subsection. Regular
languages as introduced in the previous section, however, are simply defined in terms of their recognizers—finite-state
automata.
24Formally, we would of course have to prove that finite-state automata cannot model context-free languages. This
can be done with the so-called pumping lemma, which are outside the scope of this class.
4.2. PUSHDOWN LANGUAGE MODELS 127
A PDA moves from configuration to configuration by following transitions of the form qa,γ1Ñγ2Ý ÝÝÝÝÝ Ñr,
which represents a move from the state qto stater, while popping the sequence of symbols γ1PΓ˚
from the top of the stack and pushing the sequence γ2PΓ˚. The PDA transition function therefore
not only depends on the current state qand input symbol a, but also on some finite sequence
of symbols on the topof the stack. The stack hence determines the behavior of the automaton,
and since the set of possible configurations of the stack is infinite, the set of configurations of the
automaton is infinite, in contrast to finite-state automata.
To describe how pushdown automata process strings, we introduce the concepts of scanning and
runs.
Definition 4.2.33: Scanning
We say that τ“pp,γ1,a,q,γ2qPδscansa, and ifa‰ε, we callτscanning ; otherwise, we
call it non-scanning .
Definition 4.2.34: Pushdown automaton transitions
Ifpq1,γγ1qandpq2,γγ2qare configurations, and τis a transition q1a,γ1Ñγ2Ý ÝÝÝÝÝ Ñq2, we write
pq1,γγ1qñτpq2,γγ2q.
Since the behavior of a pushdown automaton does not only depend on the states encountered by
it but also on the content of the stack, we generalize the notion of a path to include the configuration
of the automaton. This is called a run.
Definition 4.2.35: Run of a pushdown automaton
Arun of a PDA Pis a sequence of configurations and transitions
π“pq0,γ0q,τ1,pq1,γ1q,...,τn,pqN,γNq
where, forn“1,...,N , we havepqn´1,γn´1qñτnpqn,γnq.aA run is called accepting if
pq0,γ0qis the initial configuration and pqN,γNqis the final configuration. If, for n“1,...,N ,
τnscansan, then we say that πscans the string a1¨¨¨aN. We write Π pP,yqfor the set of
runs that scan yand ΠpPqfor the set of all accepting runs of P.
aSometimes it will be convenient to treat πas a sequence of only configurations or only transitions.
Definition 4.2.36: Recognition of a string by a pushdown automaton
We say that the PDA Precognizes the string yif ΠpP,yq ‰ H , i.e., if there exists an
accepting run with the yield y. The set of all strings recognized by Pis the language
recognized by P, which we denote by LpPq, i.e.,
LpPqdef“ty|ΠpP,yq‰Hu. (4.126)
128 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Example 4.2.12: Example of a pushdown automaton
Fig. 4.9 shows an example of a pushdown automaton Paccepting the language LpPq “
tanbn|nPNu.´
1a,εÑXÝÝÝÝÑ 1,1ε,εÑεÝ ÝÝÝ Ñ 2¯
is a run of P;´
1a,εÑXÝÝÝÝÑ 1,1ε,εÑεÝ ÝÝÝ Ñ 2,2b,XÑεÝÝÝÝÑ 2¯
is
anaccepting run of P.
1 2ε,εÑε
a,εÑXb,XÑε
Figure 4.9: The PDA that accepts the language tanbn|nPNu.
Lastly, we define deterministic pushdown automata, analogously to their finite-state version
(Definition 4.1.3). Recall that in the case of finite-state automata, a deterministic machine has at
most one possible next move for each state. Similalry, a deterministic pushdown automaton has at
most one possible next move for each configuration .
Definition 4.2.37: Deterministic pushdown automaton
A PDA P“pΣ,Q,Γ,δ,pqι,γιq,pqφ,γφqqisdeterministic if
•there are no transitions of the type pq,ε,γ,p,γq;
•for everypq,a,γqPQˆΣYtεuˆΓ˚, there is at most one transition pq,a,γ,p,γ1qPδ;
•if there is a transition pq,a,γ,p,γ1q Pδfor someaPΣ, then there is no transition
pq,ε,γ,p,γ2qPδ.
Otherwise, Pisnon-deterministic .
Importantly, not all context-free languages can be recognized by deterministic pushdown au-
tomata. That is, in contrast to finite-state automata, where deterministic machines are just as
powerful as non-deterministic ones (at least in the unweighted case—interestingly, some weighted
non-deterministic FSAs cannot be determinized), non-deterministic pushdown automata are more
expressive than deterministic ones. Specifically, as stated in Theorem 4.2.3, non-deterministic push-
down automata recognize exactly context-free languages, while deterministic pushdown automata
only recognize a subset of them (Sipser, 2013).
Weighted Pushdown Automata
Analogously to the finite-state case, and the case of context-free grammars, we now also extend the
definition of a pushdown automaton to the weighted case. The formal definition is:
4.2. PUSHDOWN LANGUAGE MODELS 129
Definition 4.2.38: Weighted pushdown automaton
Areal-weighted pushdown automaton (WPDA) is a tuple P“
pQ,Σ,Γ,δ,pqι,γιq,pqφ,γφqq, where:
•Qis a finite set of states;
•Σ is a finite set of input symbols called the input alphabet;
•Γ is a finite set of stack symbols called the stack alphabet;
•δĎQˆΓ˚ˆpΣYtεuqˆQˆΓ˚ˆRis a multi-set representing the transition weighting
function;
•pqι,γιqis called the initial configuration and pqφ,γφqis called the final configuration,
whereqι,qφPQandγι,γφPΓ˚.
As you can see, the only difference between the weighted and the unweighted case is the transition
function, which in the weighted case weights the individual transitions instead of specifying the set
of possible target configurations.
As with WFSAs (Definition 4.1.17) and WCFGs (Definition 4.2.16), we now define probabilistic
WPDAs. This definition, however, is a bit more subtle. Notice that the transition weighting
“function” δin a WPDA is crucially still a finite —there is only a finite number of actions we can
ever do. Similarly, when defining a probabilistic PDA, we have to limit ourselves to a finite number
of configurations over which we define probability distributions over the next possible actions.
We define a probabilistic pushdown automaton given an equivalence relation as follows.
Definition 4.2.39: Probabilistic pushdown automaton
A WPDA P“pQ,Σ,Γ,δ,pqι,γιq,pqφ,γφqqisprobabilistic if it holds that
@qa,γ1Ñγ2{wÝÝÝÝÝÝÝÑ rPδ:wě0 (4.127)
and for any qPQandγPΓ˚ÿ
qa,γ1Ñγ2{wÝÝÝÝÝÝÝÑ r
s.t.γ1Ÿγw“1. (4.128)
Definition 4.2.40: Transitions of a weighted pushdown automaton
Ifpq1,γγ1qandpq2,γγ2qare configurations, and τis a transition q1a,γ1Ñγ2{wÝÝÝÝÝÝÝÑ q2withw‰0,
we writepq1,γγ1qñτpq2,γγ2q.
130 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Definition 4.2.41: Transition weights in a pushdown automaton
Ifδpp,γ1,a,q,γ2q“w, then we usually write
τpp,γ1aÝ Ñq,γ2q“w (4.129)
or thatδhas transition pqa,γ1Ñγ2{wÝÝÝÝÝÝÝÑ pq. We sometimes let τstand for a transition, and we
defineδpτq“w.
And again, just like we combined the weights of individual transitions into the weights of paths
in WFSAs, and we combined the weights of production rules into the weights of the trees in WCFGs,
we now multiplicatively combine the weights of individual transitions in a run to define the weight
of a run in a WPDA:
Definition 4.2.42: Run weight
Theweight wpπqof a run
π“pq0,γ0q,τ1,pq1,γ1q,...,τN,pqN,γNq
is the multiplication of the transition weights, i.e.,
wpπqdef“Nź
n“1δpτnq (4.130)
Analogously to a stringsum in WFSAs, we define the stringsum for a string yin a WPDA Pas
the sum over the weights of all runs scanning y.
Definition 4.2.43: Stringsum in a pushdown automaton
LetPbe a WPDA and yPΣ˚a string. The stringsum foryinPis defined as
Ppyqdef“ÿ
πPΠpP,yqwpπq (4.131)
Definition 4.2.44: Recognition by a weighted pushdown automaton
We say that the PDA Precognizes the string ywith the weight Ppyq.
With this, we can define the weighted language defined by a WPDA.
Definition 4.2.45: Weighted language of a weighted pushdown automaton
LetPbe a WPDA. The (weighted) language LpPqofPis defined as
LpPqdef“tpy,Ppyqq|yPΣ˚u (4.132)
4.2. PUSHDOWN LANGUAGE MODELS 131
Finally, we also define the WPDA allsum and normalizable WPDAs.
Definition 4.2.46: Allsum of a weighted pushdown automaton
Theallsum of a WPDA Pis defined as
ZpPqdef“ÿ
πPΠpPqwpπq (4.133)
Definition 4.2.47: Normalizable weighted pushdown automaton
A WPDA Pisnormalizable ifZpPqis finite, i.e., if ZpPqă8 .
Relationship to Context-free Grammars
We motivated the introduction of pushdown automata as a means of recognizing context-free
languages. However, this correspondence is not obvious from the definition! Indeed, the equivalence
of the expressive power of context-free grammars and pushdown automata is a classic result in
formal language theory, and it is summarised by the theorem below:
Theorem 4.2.3: Context-free grammars and pushdown automata are equally ex-
pressive
A language is context-free if and only if some pushdown automaton recognizes it.
Proof. See Theorem 2.20 in Sipser (2013). ■
This result extends to the probabilistic case.
Theorem 4.2.4: Probabilistic context-free grammars and probabilistic pushdown
automata are equally expressive
A language is generated by a probabilistic context-free grammar if and only if some probabilistic
pushdown automaton recognizes it.
Proof. See Theorems 3 and 7 in Abney et al. (1999). ■
Lastly, analogously to how Theorem 4.2.2 showed that weighted context-free grammars are
equally expressive as probabilistic context-free grammars, the following theorem asserts the same
about pushdown automata:
Theorem 4.2.5: Globally normalized weighted pushdown automata can be locally
normalized
Any globally normalized weighted pushdown automaton can be locally normalized. More
precisely, this means the following. Let Pbe a weighted pushdown automaton. Then, there
132 CHAPTER 4. CLASSICAL LANGUAGE MODELS
exists a probabilistic pushdown automaton Ppsuch that
Pppyq“Ppyq
ZpPq(4.134)
for all yPΣ˚.
Proof. The proof is not straightforward: it can be shown that one cannot simply convert an arbitrary
weighted pushdown automaton into a locally-normalized one directly. Rather, the construction of
the latter goes through their context-free grammars : given a WPDA P, one first constructs the
WCFG equivalent to P, and then converts that to a locally normalized one (cf. Theorem 4.2.2), i.e.,
a PCFG. Then, this PCFG can be converted to a structurally quite different probabilistic pushdown
automaton Pp, which nevertheless results in the language we require (Eq. (4.134)).
This construction is described in more detail in Abney et al. (1999) and Butoi et al. (2022). ■
4.2.8 Pushdown Language Models
We can now define pushdown language models, the title of this section.
Definition 4.2.48: Pushdown language model
Apushdown language model is a language model whose weighted language equals the
language of some weighted pushdown automaton, i.e., if there exists a weighted pushdown
automaton Psuch thatLpPq“LppLMq.
Similarly, pushdown automata also induce language models.
Definition 4.2.49: Language model induced by a pushdown automaton
LetPbe a weighted pushdown automaton. We define the language model induced by P
as the probability distribution induced by the probability mass function
pLMPpyqdef“Ppyq
ZpPq, (4.135)
for any yPΣ˚.
You might wonder why we specifically define pushdown language models and models induced
by them if WPDAs are equivalent to WCFGs (cf. §4.2.7). In that sense, a language model is
context-free if and only if it is a pushdown language model. However, this holds only for single-stack
pushdown automata which we have discussed so far. We make this explicit distinction of pushdown
language models with an eye to the next section, in which we introduce multi-stack WPDAs. Those
are, as it turns out, much more powerful (expressive) than context-free grammars. We will, however,
reuse this definition of a pushdown language model for those more powerful machines.
4.2. PUSHDOWN LANGUAGE MODELS 133
4.2.9 Multi-stack Pushdown Automata
We now consider an extension of (weighted) pushdown automata, namely, machines that employ
multiple stacks. While this might not seem like an important distinction, we will see shortly that
this augmentation results in a big difference in the expressiveness of the framework!
Definition 4.2.50: Two-stack pushdown automaton
Atwo-stack pushdown automaton (2-PDA) is a tuple P“
pΣ,Q,Γ1,Γ2,δ,pqι,γι1,γι2q,`
qφ,γφ1,γφ2˘
q, where:
•Σ is a finite set of input symbols called the input alphabet;
•Qis a finite set of states;
•Γ1and Γ 2are finite sets of stack symbols called the stack alphabets;
•δĎQˆΓ˚
1ˆΓ˚
2ˆpΣYtεuqˆQˆΓ˚
1ˆΓ˚
2is a multiset representing the transition
function;
•pqι,γι1,γι2qis called the initial configuration and`
qφ,γφ1,γφ2˘
is called the final
configuration, where qι,qφPQ,γι1,γφ1PΓ˚
1, and γι2,γφ2PΓ˚
2.
Note that we could more generally define a k-stack PDA by including kstacks in the definition,
but the restriction to two stacks will be sufficient for our needs, as we will see in the next subsection.
The transition function now depends on the values stored in both of the stacks. The definitions of
the configuration and run of a two-stack PDA are analogous to the single-stack variant, with the
addition of the two stacks. We again extend this definition to the weighted and the probabilistic
case.
Definition 4.2.51: Two-stack weighted pushdown automaton
Atwo-stack real-weighted pushdown automaton (2-WPDA) is a tuple P“
pΣ,Q,Γ1,Γ2,δ,pqι,γι1,γι2q,`
qφ,γφ1,γφ2˘
q, where:
•Σ is a finite set of input symbols called the input alphabet;
•Qis a finite set of states;
•Γ1and Γ 2are finite sets of stack symbols called the stack alphabets;
•δĎQˆΓ˚
1ˆΓ˚
2ˆpΣYtεuqˆQˆΓ˚
1ˆΓ˚
2ˆRis a multiset representing the transition
weighting function;
•pqι,γι1,γι2qis called the initial configuration and`
qφ,γφ1,γφ2˘
is called the final
configuration, where qι,qφPQ,γι1,γφ1PΓ˚
1, and γι2,γφ2PΓ˚
2.
And lastly, we define probabilistic two-stack PDAs:
134 CHAPTER 4. CLASSICAL LANGUAGE MODELS
Definition 4.2.52: Probabilistic two-stack pushdown automaton
A 2-WPDA P“pQ,Σ,Γ1,Γ2,δ,pqι,γι1,γι2q,`
qφ,γφ1,γφ2˘
qisprobabilistic if for any con-
figurationpq,γ1,γ2qit holds that
@qa,γ1Ñγ1
1,γ2Ñγ1
2{wÝÝÝÝÝÝÝÝÝÝÝÝÑ rPδ:wě0 (4.136)
and for any qPQandγPΓ˚
1,γ1PΓ˚
2
ÿ
qa,γ1Ñγ1
1,γ2Ñγ1
2{wÝÝÝÝÝÝÝÝÝÝÝÝÑ r
s.t.γ1Ÿγandγ2Ÿγ1w“1. (4.137)
Turing Completeness of Multi-stack Pushdown Automata
Besides modeling more complex languages than finite-state language models from §4.1, (multi-stack)
pushdown automata will also serve an important part in analyzing some modern language models that
we introduce later. Namely, we will show that Recurrent neural networks (cf. §5.1.2) can simulate
any two-stack PDA. This will be useful when reasoning about the computational expressiveness of
recurrent neural networks because of a fundamental result in the theory of computation, namely,
that two-stack PDAs are Turing complete :
Theorem 4.2.6: Two-stack pushdown automata are Turing complete
Any 2-stack pushdown automaton is Turing complete.
Proof. The equivalence is quite intuitive: the two stacks (which are infinite in one direction) of the
2-PDA can simulate the tape of a Turing machine by popping symbols from one stack and pushing
symbols onto the other one simultaneously. The head of the Turing machine then effectively reads
the entries at the top of one of the two stacks. For a formal proof, see Theorem 8.13 in Hopcroft
et al. (2006). ■
This is also the reason why we only have to consider two-stack PDAs—they can compute
everything that can be computed, meaning that additional stacks do not increase their expressiveness!
Since unweighted pushdown automata are simply special cases of weighted PDAs, which are
equivalent to probabilistic PDAs, we can therefore also conclude:
Corollary 4.2.1
Any weighted 2-stack pushdown automaton is Turing complete.
Corollary 4.2.2
Any probabilistic 2-stack pushdown automaton is Turing complete.
4.2. PUSHDOWN LANGUAGE MODELS 135
A straightforward consequence of the Turing completeness of two-stack PPDAs is that their
tightness is undecidable.
Theorem 4.2.7: Tightness of 2-PPDA is undecidable
The tightness of a probabilistic two-stack pushdown automaton is undecidable.
Proof. We start with a simple observation: a pushdown automaton Pis tight if and only if it halts
on inputs with measure 1 (given the probability measure on Σ˚YΣ8defined in §2.5.4), as this, by
the definition of the language accepted by the WPDA (cf. §4.2.7), corresponds to its language only
containing finite strings with probability 1.
LetMthen be a Turing machine and Pbe a 2-PPDA which simulates it. Then, Pis tight if
and only if it halts with probability 1 (again, based on the probability measure from above). This is
equivalent to the problem of Mhalting with probability 1—this, however, is a variant of the halting
problem , which is one of the fundamental undecidable problems. We have therefore reduced the
problem of determining the tightness of 2-PPDAs to the halting problem, implying that the former
is undecidable. ■
You might wonder what this means for the (weighted) languages recognized by multiple-stack
(weighted) automata. Turing machines can recognize recursively enumerable languages . This means
that weighted multi-stack pushdown automata model distributions over recursively enumerable
languages. To see why this might be useful, let us finish the discussion of context-free languages
with an example of a language model that is notcontext-free:
Example 4.2.13: Example of a non-context-free distribution over strings
Let Σ“tauandpLMpanq“e´λλn
n!fornPNě0, i.e.,LppLMqQy“an„Poissonpλqfor some
λą0. This language is not context-free: the proof, however, is not trivial. We direct the
reader to Icard (2020b) for one.
136 CHAPTER 4. CLASSICAL LANGUAGE MODELS
4.3 Exercises
Exercise 4.1
Prove the following lemma.
Lemma 4.3.1
LetA“pΣ,Q,δ,λ,ρqandqPQ. Then
ZpA,qq“ÿ
qa{wÝÝÑq1PδALωˆ
qa{¨Ý Ý Ñq1˙
ZpA,q1q`ρpqq (4.138)
Exercise 4.2
Show that the expression for the log-likelihood of the n-gram model can be rewritten as
ℓℓpDq“Mÿ
m“1|ypmq|ÿ
t“1logθyn|yăn“ÿ
y
|y|“nCpyqθyn|yăn (4.139)
with the quantities as defined in Proposition 4.1.3. This is a common trick. It is also known as the
token to type switch because we switch from counting over the individual tokens to counting
over their identities (types)
Exercise 4.3
LetCpyqbe the string occurrence count for yPΣ˚occurrence count as defined in Proposition 4.1.3.
Show (or simply convince yourself) that, in a given training corpus D
ÿ
y1PΣC`
y1...y n´1y1˘
“Cpy1...y n´1q (4.140)
Chapter 5
Neural Network Language Models
Chapter 4 introduced two classical language modeling frameworks: finite-state language models and
context-free language models. While those served as a useful introduction to the world of language
modeling, most of today’s state-of-the-art language models go beyond the modeling assumptions
of these two frameworks. This chapter dives into the diverse world of modern language modeling
architectures, which are based on neural networks. We define two of the most common architectures—
recurrent neural networks and transformers—and some of their variants. The focus is again on
rigorous formalization and theoretical understanding—we analyze the introduced models in terms of
the theoretical foundations so far (e.g., expressiveness and tightness)—but, due to their practical
applicability, we also study some practical aspects of the models.
We begin with recurrent neural networks.
137
138 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
5.1 Recurrent Neural Language Models
The first neural language modeling architecture we consider is one based on recurrent neural networks.
Recurrent neural networks capture the idea of the sequential processing of strings relatively naturally
while also making decisions based on an infinite context. Before delving into the technical details of
recurrent neural networks (RNNs), however, we first motivate the introduction of modeling contexts
of unbounded length. Then, we formally define recurrent neural networks and devote a large portion
of the section to their theoretical properties. The most important of those will be the Turing
completeness of this architecture, as it has numerous consequences on the solvability of many of the
tasks we might be interested in, such as finding the most probable string in the language model
represented by a recurrent neural network and determining whether an RNN is tight.
5.1.1 Human Language is Not Context-free
Recall that we motivated the introduction of context-free languages by observing that finite memory
is insufficient to model all formal phenomena of human language, e.g., infinite recursion (cf. Ex-
ample 4.2.1). Context-free languages described by context-free grammars and pushdown automata
were able to capture those. However, human language is more expressive than that—it includes
linguistic phenomena that cannot be described by context-free grammars. A typical example is
called cross-serial dependencies , which are common in Swiss German .
Example 5.1.1: Cross-Serial Dependencies in Swiss German, Shieber, 1985
Swiss German is a textbook example of a language with grammatical cross-serial dependencies,
i.e., dependencies in which the arcs representing them, cross. In the example sentence below,
the words connected with arcs are objects and verbs belonging to the same predicates (verb
phrases). Because of that, they have to agree on the form—they depend on one another. As
we show next, context-free languages cannot capture such dependencies.
...mer d’chind em Hans s’ huus l¨ ond h¨ alfe aastriiche
...we the children Hans the house let help paint
Why are cross-serial dependencies non-context-free? Before reasoning about the phe-
nomenon of cross-serial dependencies, we revisit Example 4.2.1 with a somewhat more formal
approach. The arbitrarily deep nesting can, for example, be abstractly represented with the
expression
xAnBny (5.1)
5.1. RECURRENT NEURAL LANGUAGE MODELS 139
with1
x““The cat”
A““the dog”
B““barked at”
y““likes to cuddle” .
From this abstract perspective, center embeddings are very similar to the Dp1qlanguage (Exam-
ple 4.2.5), in that every noun phrase “the dog” has to be paired with a verb phrase “barked at” ,
which cannot be represented by any regular language.
In a similar fashion, Example 5.1.1 can abstractly be represented with the expression
xAmBnCmyDnz (5.2)
with
x““...mer”
A““d’chind”
B““em Hans”
y““s’ huus”
C““l¨ond”
D““h¨alfe”
z““aastriiche” .
Admittedly, this is a relatively uncommon formulation even with n“m“1. It should be taken with
a grain of salt, as the title of the original publication discussing this phenomenon, Evidence against
context-freeness (Shieber, 1985), also suggests. However, theoretically, the number of repetitions of
“d’chind” and“l¨ond” , as well as “em Hans” and“h¨alfe” , can be increased arbitrarily. Repeating
the former would correspond to having many groups of children. The last of the groups would let
Hans help paint the house, whereas each of the previous groups would let the group after them either
let Hans paint the house or recurse onto another group of children. Similarly, repeating “em Hans”
and“h¨alfe” would correspond to a number of Hanses, each either helping another Hans or helping
paint the house. Then, using the pumping lemma for context-free languages, it can be shown that
the expressions of the form in Eq. (5.2) cannot be recognized by any context-free grammar. We
refer the readers to Hopcroft et al. (2006, Example 7.20) for detailed proof.
Example 5.1.1 means that to model a human language formally, we need more expressive
formalisms than context-free grammars or pushdown automata as described in the previous sections.2
However, instead of defining a more expressive formalism motivated by formal language theory (like
we did with context-free grammars and center embeddings), we now introduce recurrent neural
networks, which, as we will see, under certain assumptions, have the capacity to model all computable
languages (i.e., they are Turing complete). Moreover, they can also model infinite lengths of the
context yătin a very flexible way. In the next section, we define them formally.
1In this case, we of course only consider an arbitrarily long sequence of barking dogs.
2On the other hand, note that we would ideally also like to upper-bound the expressive power of the formal models,
as this introduces useful inductive biases for learning and sparks insights into how humans process language. This
means that we would not simply like to jump to Turing-complete models in such an exploration of language models.
140 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
5.1.2 Recurrent Neural Networks
As discussed, natural languages are beyond the descriptive power of regular and context-free
languages. Now, we turn to a class of models that is theoretically capable of recognizing all
computable languages: recurrent neural networks (RNNs).3
An Informal Introduction to Recurrent Neural Networks
Human language is inherently sequential: we produce and consume both spoken as well as written
language as a stream of units.4This structure is reflected in some of the algorithms for processing
language we have seen so far. For example, finite-state automata (cf. §4.1) process the input string
one symbol at a time and build the representations of the string seen so far in the current state of
the automaton. Pushdown automata function similarly, but additionally keep the stack as part of
the configuration.
Recurrent neural networks are neural networks that capture the same idea of iterative processing
of the input but do so in a more flexible way than the finite-memory finite-state automata and the
stack-based pushdown automata. Very abstractly, a recurrent neural network sequentially processes
a sequence of inputs and, while doing so, produces a sequence of hidden states , which we will
denote as h, based on a transition function in form of a recurrent dynamics map , which acts
similarly to a (deterministic) transition function in a finite-state machine: given the current hidden
state and an input symbol, it (deterministically) determines the next hidden state. The hidden states
play, as we will see, an analogous role to the states of a finite-state automaton or the configuration of
a pushdown automaton: The current hidden state of a recurrent neural network at time tdetermines,
together with the input at time t, through the dynamics map, the hidden state at time t`1—indeed,
very similar to how finite-state automata process strings and transition between their states. Again,
the hidden state can be thought of as a compact (constant-size) summary of the input yďtseen so
far and should ideally characterize yďtas well as possible (in the sense of retaining all information
required for continuing the string). Remember from §4.2.1 that the finite number of states of a
finite-state automaton presented a serious limitation to its ability to model human language. As
we will see, the main difference between (weighted) finite-state automata and RNNs is that the
latter can work with infinite state spaces, for example, RDin the abstract formulation, or QDin a
digital computing system, such as a computer. This, together with the flexibility of the transition
function between hidden states, will allow RNNs to represent more complex languages than those
recognized by finite-state automata or context-free grammars. In fact, the large state space and the
flexible transition functions endow RNNs, under some assumptions, with the possibility to model
infinitely-long-term dependencies on the input string, distinguishing them from the Markovian
n-gram models.
You might wonder why we refer to the current state of an RNN as hidden states instead of
only as states , as with finite-state automata. Indeed, when analyzing recurrent neural networks
in terms of their expressivity and connections to classical models of computation, we will regard
the hidden states as completely analogous to states in a finite-state or pushdown automaton. The
hidden part comes from the fact that the hidden states hare usually not what we are interested
in when modeling language with an RNN. Rather, his simply seen as a component in a system
3In this subsection, we focus on the applications of recurrent neural networks to language modeling. However,
recurrent neural networks have been widely used to process sequential data and time series, thanks to their power of
taking in arbitrary-length inputs.
4So far, we have simply referred to those units as symbols .
5.1. RECURRENT NEURAL LANGUAGE MODELS 141
y1y2y3h0 h1 h2 h3¨¨¨ State ht
Inputyt
(a) An abstract depiction of how an RNN processes one symbol in a string. The hidden state htsummarizes
the inputsy1y2...yt.
h0 h1 h2 h3¨¨¨y1 y2 y3 y4
(b) An abstract depiction of an RNN as an automaton. The transitions between the possibly infinitely-many
hidden states are determined by the dynamics map.
hy
(c) An abstract depiction of an RNN as a system updating the hidden state hdepending on the input y.
Figure 5.1: Different possible depictions of an abstract RNN model. The way that the hidden states
are updated based on the input symbol ytis abstracted away.
that produces individual conditional probabilities over the next symbol, as in sequence models (cf.
Definition 2.5.2)—these conditional probabilities are the actual “visible” parts, while the “internal”
states are, therefore, referred to as hidden.
RNNs are abstractly illustrated in different ways in the literature. Often, they are represented
as a sequence of hidden states and the input symbols consumed to arrive at those states—this
is shown in Fig. 5.1a. They can also be presented more similarly to automata, with (a possibly
infinite) labeled graph, where the transition labels again correspond to the symbols used to enter
the individual states. This is presented in Fig. 5.1b. Lastly, due to the infinite state space, one can
also think of an RNN as a system that keeps the most current hidden state in memory and updates
it as new symbols are consumed—this is shown in Fig. 5.1c.5
A Formal Definition of Recurrent Neural Networks
Having introduced RNNs and their motivations informally, we now move to their formal definition.
Our definition and treatment of recurrent neural networks might differ slightly from what you
might normally encounter in the literature. Namely, we define RNNs below as abstract systems
transitioning between possibly infinitely-many states. Our definition will allow for an intuitive
connection to classical language models such as finite-state and pushdown language models as
5More precisely, these illustrations correspond to first-order RNNs, which are by far the most common. Later, we
will also briefly consider higher-order RNNs, whose hidden state update depends on multiple previous hidden states.
142 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
well as for tractable theoretical analysis in some special cases. Specifically, when analyzing RNNs
theoretically, we will make use of their connections to automata we saw in Chapter 4.
In an abstract sense, recurrent neural networks can be defined as a system transitioning between
possibly infinitely-many states, which we will assume to be vectors in a vector space. Specifically,
we will distinguish between realand rational recurrent neural networks.
Definition 5.1.1: Real-valued Recurrent Neural Network
Let Σ be an alphabet. A (deterministic) real-valued recurrent neural network Ris a
four-tuplepΣ,D,f,h0qwhere
•Σ is the alphabet of input symbols;
•Dis the dimension of R;
•f:RDˆΣÑRDis the dynamics map, i.e., a function defining the transitions between
subsequent states;
•h0PRDis the initial state .
We analogously define rational-valued recurrent neural networks as recurrent neural
networks with the hidden state space QDinstead of RD. You might wonder why we make the
distinction. Soon, when we take on theoretical analysis of RNNs, it will become important over
which state spaces the models are defined. RNNs implemented in a computer using floating-point
numbers, of course, cannot have irrational-valued weights—in this sense, all implemented recurrent
neural networks are rational. However, defining the models over the real numbers crucially allows us
to perform operations from calculus for which some sort of continuity and smoothness is required,
for example, differentiation for gradient-based learning (cf. §3.2.3).
Example 5.1.2: A rational-valued RNN
An example of a rational-valued RNN is the series
ht“1
2ht´1`1
ht´1(5.3)
which we considered in Example 3.1.1. In this case
•Σ“tau
•D“1
•f:px,aqÞÑ1
2x`1
x
•h0“2
5.1. RECURRENT NEURAL LANGUAGE MODELS 143
Example 5.1.3: Another example of an RNN
The tuple R“pΣ,D,f,h0qwhere
•Σ“ta,bu
•D“2
•f:px,yqÞÑ$
’’’’&
’’’’%˜
cosϕ´sinϕ
sinϕcosϕ¸
x if y= a
˜
cosψ´sinψ
sinψ cosψ¸
x otherwise
•h0“ˆ
1
1˙
is an example of a real-valued RNN which rotates the current hidden state by the angle ϕif
the input symbol is aand rotates it by ψif the symbol is b.
Example 5.1.4: Another example of an RNN
Another example of an RNN would be the tuple
•Σ“GOODYBAD“t“great”,“nice”,“good”uYt “awful”,“bad”,“abysmal”u
•D“2
•f:ph,aqÞÑ$
’’’’&
’’’’%h`˜
1
0¸
ifaPGOOD
h`˜
0
1¸
otherwise
•h0“ˆ
0
0˙
which counts the number of occurrences of positive and negative words.
To define language models using recurrent neural networks, we will use them as the encoder
functions encin our general language modeling framework (cf. §3.1). To connect Definition 5.1.1
with the general LM framework, we define the RNN encoding function.
Definition 5.1.2: Recurrent Neural Encoding Function
LetR“pΣ,D,f,h0qbe a recurrent neural network. A recurrent neural encoding function
encRis a representation function (cf. §3.1.1) that recursively encodes strings of arbitrary
lengths using its dynamics map f:
encRpyăt`1qdef“fpencRpyătq,ytqPRD(5.4)
144 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
and
encRpyă1qdef“h0PRD(5.5)
Intuitively, an RNN Rtakes an input string yand encodes it with the encoding function
encRby sequentially applying its dynamics map f. The representations of individual prefixes (cf.
Definition 2.3.6) of the input string are called hidden states.
Definition 5.1.3: Hidden State
LetR“pΣ,D,f,h0qbe an RNN. The hidden state htPRDdescribes state of Rafter reading
yt. It is recursively computed according to the dynamics map fas follows:
htdef“encRpyăt`1q“fpht´1,ytq (5.6)
Example 5.1.5: Hidden states
The hidden states of the RNN from Example 5.1.2 are the individual values ht, which, as t
increases, approach?
2.
Recurrent Neural Sequence Models
A recurrent neural network based on Definition 5.1.1 on its own does not yet define a sequence
model, but simply a context encoding function encR:Σ˚ÑRD. To define a sequence model based
on an RNN, we simply plug in the RNN encoding function Definition 5.1.2 into the General language
modeling framework from §3.1.
Definition 5.1.4: Recurrent neural sequence model
LetR“pΣ,D,f,h0qbe a recurrent neural network and EPR|Σ|ˆDa symbol representation
matrix. AD-dimensional recurrent neural sequence model over an alphabet Σ is a tuple
pΣ,D,f,E,h0qdefining the sequence model of the form
pSMpyt|yătqdef“f∆|Σ|´1pEencRpyătqqyt“f∆|Σ|´1pE ht´1qyt. (5.7)
By far the most common choice of the projection function is the softmax yielding the sequence
model
pSMpyt|yătqdef“softmaxpEencRpyătqqyt“softmaxpE ht´1qyt. (5.8)
For conciseness, we will refer to RNN sequence models whose next-symbol probability distri-
butions are computed using the softmax function as softmax RNN sequence models .
From this perspective, we see that RNNs are simply a special case of our general language
modeling framework with parameterized representations of tokens yPΣand the history yPΣ˚(cf.
§3.1)—an RNN simply defines how the encoding function encis specified. The three figures from
Fig. 5.1 are presented again with this probabilistic perspective in Fig. 5.2.
5.1. RECURRENT NEURAL LANGUAGE MODELS 145
y1 y2h0 h1 h2¨¨¨
¨¨¨y1„pSMp¨|h0qy2„pSMp¨|h1qState ht
Inputyt
(a) An abstract depiction of how an RNN generates a string one symbol at a time. The hidden state ht
summarizes the string y1y2...ytgenerated so far. The dotted lines denote the sampling steps.
h0 h1 h2¨¨¨y1„pSMp¨|h0qy2„pSMp¨|h1q
(b) An abstract depiction of a generative RNN as an automaton.
htyt„pSMp¨|htq
(c) An abstract depiction of an RNN as a system updating the hidden state htdepending on the generated
symbolyt.
Figure 5.2: Different possible depictions of an abstract RNN model generating symbols. The way
that the hidden states are updated based on the input symbol ytis abstracted away.
146 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
A Few More Definitions
In the following, we will often use the so-called one-hot encodings of symbols for concise notation.
We define them here.
Definition 5.1.5: One-hot encoding
Let Σ be an alphabet and n: ΣÑt1,...,|Σ|ua bijection (i.e., an ordering of the alphabet,
assigning an index to each symbol in Σ). A one-hot encoding J¨Kis a representation function
of the symbols in Σ which assigns the symbol yPΣ thenpyqthbasis vector:
JyKdef“dnpyq, (5.9)
where here dnis thenthcanonical basis vector, i.e., a vector of zeros with a 1 at position n.
Example 5.1.6: One-hot encoding
Let Σ“t“large”,“language” ,“models”uandn“t“large” :1,“language” :2,“models” :3u.
The one-hot encoding of the vocabulary is:
J“large” K“¨
˝1
0
0˛
‚,J“language” K“¨
˝0
1
0˛
‚,J“models” K“¨
˝0
0
1˛
‚ (5.10)
Many specific variants of recurrent neural networks define the dynamics map fin a specific way:
the output of the function is some element-wise (non-linear) transformation of some “inner” function
g. The dynamics map of such an RNN is then the composition of gand the non-linearity.
Definition 5.1.6: Activation function
LetR“pΣ,D,f,E,h0qbe an RNN. If the hidden states htof the RNN are computed as
ht“σpgpht´1,yqq (5.11)
for some function g:RDˆΣÑRDand some function σ:RÑRwhich is computed element-
wise (that is,σpxqd“σpxdqfor alld“1,...,D andxPRD), we callσanactivation
function .
This finishes our formal definition of recurrent neural networks. We next consider some of their
theoretical properties, starting with tightness.
5.1.3 General Results on Tightness
We now discuss a general result on the tightness of recurrent neural sequence models, as defined
in Definition 5.1.4. The analysis is straightforward and is a translation of the generic results on
tightness (cf. §3.1.5) to the case of the norm of the hidden states of an RNN, htas the encodings of
the prefixes yďt, but it requires us to focus specifically on softmax RNN sequence models.
5.1. RECURRENT NEURAL LANGUAGE MODELS 147
Theorem 5.1.1: Tightness of Recurrent Neural Sequence Model
A softmax recurrent neural sequence model is tight if for all time steps tit holds that
s∥ht∥2ďlogt, (5.12)
wheresdef“maxyPΣ∥epyq´epeosq∥2.
Proof. This is simply a restatement of Theorem 3.1.6 for the case when enctakes the form of a
general RNN encoding function, enc R. ■
Corollary 5.1.1: RNNs with bounded dynamics maps are tight
Asoftmax recurrent neural sequence model R“pΣ,D,f,h0qwith a bounded dynamics map
f, i.e, with a dynamics map fsuch that
|fpxqd|ďM (5.13)
for someMPR, for alld“1,...,D and all xPRD, is tight.
Proof. If the dynamics map is bounded, the norm of the hidden state, ∥ht∥2, is bounded as well.
This means that the left-hand-side of Eq. (5.12) is constant with respect to tand the condition
holds trivially. ■
A special case of Corollary 5.1.1 is RNNs with bounded activation functions (cf. Definition 5.1.6).
Those are tight if the activation function itself is bounded. This implies that all standard sigmoid
andtanh activated recurrent neural networks are tight. However, the same does not hold for RNNs
with unbounded activation functions, which have lately been more popular (one of the reasons for
this is the vanishing gradient problem (Glorot et al., 2011)).
Example 5.1.7: RNNs with unbounded activation functions may not be tight
A very popular unbounded activation function is the so-called rectified linear unit (ReLU ),
defined as
ReLUpxqdef“maxp0,xq. (5.14)
This function is clearly unbounded.
Now suppose we had the following RNN over the simple alphabet Σ “tau.
ht“ht´1``
1˘
, (5.15)
initial state
h0“`
0˘
(5.16)
and the output matrix
E“ˆ
´1
1˙
(5.17)
148 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
where the top row of Ecomputes the logit of the eossymbol and the bottom one the one of
a. It is easy to see that
ht“`
t˘
. (5.18)
This already does not look promising for tightness—the norm of the hidden state, which is, in
this case,/vextenddouble/vextenddouble`
t˘/vextenddouble/vextenddouble“t, and is, therefore, increasing at a much higher rate than Oplogtqrequired
by Theorem 5.1.1. We encounter a similar hint against tightness if we compute the conditional
probabilities of the eossymbol and the symbol a.
pSMpeos|yătq“softmaxˆˆ
´1
1˙`
t´1˘˙
eos“expr´t`1s
expr´t`1s`exprt´1s(5.19)
pSMpeos|yătq“softmaxˆˆ
´1
1˙`
t´1˘˙
a“exprt´1s
expr´t`1s`exprt´1s(5.20)
The probability of ending the string at time step tis, therefore
pSMpeos|yătq“1
1`expr2pt´1qs. (5.21)
Intuitively, this means that the probability of ending the string (generating eos) diminishes
rapidly with t—in this case much faster than any diverging sum required by Theorem 2.5.3.
All signs, thus, point towards the RNN from Eq. (5.15) not being tight. Indeed, for this
specific case, one can show using some algebraic manipulations that
ÿ
yPΣ˚pLNpyq“ÿ
nPNě0pLNpanqă0.15 (5.22)
wherepLNis the locally normalized model induced by the RNN. This means that the RNN
from Eq. (5.15) assigns less than 0 .15 probability to finite strings—all other probability mass
leaks to infinite sequences.
Example 5.1.8: RNNs with unbounded activation functions can still be tight
Example 5.1.7 showed that RNNs with unbounded activation functions can indeed result in
non-tight sequence models. However, this is not necessarily the case, as this simple modification
of the RNN from Example 5.1.7 shows. The only aspect of the RNN that we modify is the
output matrix E, which we change by flipping its rows:
E“ˆ
1
´1˙
(5.23)
Now the probability of ending the string at time step tis
pSMpeos|yătq“exprt´1s
expr´t`1s`exprt´1s“1
expr´2pt´1qs`1. (5.24)
Compared to Eq. (5.21), the probability of eosin Eq. (5.24) does not diminish. Indeed, since
5.1. RECURRENT NEURAL LANGUAGE MODELS 149
1
expr´2pt´1qs`1ą1
2for allt, the sum
8ÿ
t“0pSMpeos|yătq (5.25)
diverges, which, according to Proposition 2.5.6 implies that the sequence model is tight.
5.1.4 Elman and Jordan Networks
The characterization of dynamics maps we gave in Definition 5.1.4 allows for fto be an arbitrary
mapping from the previous state and the current input symbol to the new state. In this section, we
introduce two seminal and particularly simple parameterizations of this map—the simplest recurrent
neural sequence models. We term them Elman sequence models and Jordan sequence models, as
each is inspired by architectures proposed by Elman (1990) and Jordan (1986), respectively. The
definitions we present here are slightly different than those found in the original works—most notably,
both Elman and Jordan networks were originally defined for transduction (mapping an input string
to an output string, as with translation) rather than language modeling.
Put simply, these two models restrict the form of the dynamics map fin the definition of an
RNN (cf. Definition 5.1.1). They define particularly simple relationships between the subsequent
hidden states, which are composed of affine transformations of the previous hidden state and
the representation of the current input symbol passed through a non-linear activation function
(cf. Definition 5.1.6). The affine transformations are performed by different matrices and bias
vectors—the parameters of the model (cf. Assumption 3.2.2)—each transforming a separate part of
the input to the dynamics map.
Definition 5.1.7: Elman Sequence Model (Elman, 1990)
AnElman sequence model R“pΣ,D,U,V,E,bh,h0qis aD-dimensional recurrent neural
sequence model over an alphabet Σ with the following dynamics map
ht“σ`
Uht´1`Ve1pytq`bh˘
. (5.26)
Here, e1¨:ΣÑRRis the input symbol embedding function which represents each symbol
yPΣ as aR-dimensional vector and σis an element-wise non-linearity.abhPRD,UPRDˆD,
andVPRDˆR.
aThe symbol representations e1yare often also referred to as static symbol embeddings because they do
not depend on the string surrounding or preceding y. Here, we treat them as any other parameters of the
model which can be learned using gradient-based learning (cf. §3.2). However, note that learning good static
embeddings was a very active field before the emergence of large end-to-end systems we see today. Very popular
examples include Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and FastText (Bojanowski
et al., 2017).
Due to its simplicity, the Elman RNN is also known as the vanilla RNN variant, emphasizing it
is one of the most fundamental variants of the framework.
150 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
On the symbol representations. Notice that in Eq. (5.26), the input symbols ytare first
transformed into their vector representations e1pytqand then additionally linearly transformed
using the matrix V. This results in an over-parametrized network—since the symbols are already
embedded using the representation function e1, the matrix Vis theoretically superfluous and could
be replaced by the identity matrix. However, the matrix Vcould still be useful if the representations
e1pytqare fixed—in this case, the matrix can be used by the RNN to transform the representations
during training to fit the training data better. This is especially useful if the symbol representations
e1pytqalready represent the input symbols in a compact representation space in which the parameters
can be shared across different symbols. Alternatively, we could represent the symbols using their
one-hot encodings, i.e., e1pytq“ JytK, in which case the columns of the matrix Vwould correspond to
the symbol representations (analogously to the representation matrix Efrom Eq. (3.46)). However,
notice that in this case, the representations on the symbols do not share any parameters, and
each column of the matrix is therefore an unconstrained vector. Such matrix-lookup-based input
symbol representations from Eq. (5.26) are sometimes tied, i.e., e1¨“ep¨q, with the output symbol
representations from the embedding matrix Ein the definition of the sequence model induced by an
RNN (cf. Definition 3.1.11 and Eq. (5.7)).
However, embedding tying is non-essential to representation-based LMs. The input symbol
embedding function can always be chosen independently with the output symbol embedding function
Definition 3.1.6.
The Jordan network is somewhat different in that it feeds the output logits computed through
the output matrix Einto the computation of the next state, and not directly the hidden state.
Definition 5.1.8: Jordan Sequence Model (Jordan, 1986)
AJordan sequence model is aD-dimensional recurrent neural sequence model over an
alphabet Σ with the following dynamics map
ht“σ`
Urt´1`Ve1pytq`bh˘
(5.27)
rt“σopEhtq (5.28)
Again, e1¨:ΣÑRRis the input symbol embedding function which represents each symbol
yPΣ as aR-dimensional vector while σandσoare element-wise non-linearities. bhPRD,
UPRDˆD, and VPRDˆR.
Notice that the hidden state htin Eq. (5.27) is not computed based on the previous hidden
state ht´1, but rather on the transformed outputs rt´1—this is analogous to feeding back in the
logits computed in Eq. (5.7) into the computation of hrather than the previous hidden state. The
sequence model induced by a Jordan network is then directly induced by the logits rt(i.e., the
conditional probabilities are computed by putting v tthrough the softmax.
In both architectures, the activation function σcan be any suitable element-wise function. The
canonical choices for it have been the sigmoid and tanh functions, however, a more common choice
nowadays is the ReLU function or any of its more modern variants.6
Since we will refer to the individual matrices defining the dynamics maps in Elman and Jordan
networks quite a lot in the next subsections, we give them specific names. The matrix U, which
linearly transforms the previous hidden state (or the output) is the recurrence matrix . The
6See Goodfellow et al. (2016, §˜6.3.1) for an overview of modern activation functions used in neural networks.
5.1. RECURRENT NEURAL LANGUAGE MODELS 151
matrix V, which linearly transforms the representations of the input symbol, is called the input
matrix . Lastly, the matrix which linearly transforms the hidden state before computing the output
values rtwith an activation function is called the output matrix .bhis the hidden bias vector .
Tightness of Elman and Jordan Recurrent Neural Networks As a simple corollary of
Corollary 5.1.1, we can characterize the tightness of Elman and Jordan recurrent neural networks as
follows.
Corollary 5.1.2: Tightness of simple RNNs
Elman and Jordan RNNs with a bounded activation function σand the softmax projection
function are tight.
152 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
5.1.5 Variations on Recurrent Networks
In the previous sections we introduced the two simplest RNN variants: the Elman (Elman, 1990)
and Jordan (Jordan, 1997) networks. Even though such simple RNNs in theory are all we need to
model any computable language, empirically those architectures face many challenges. One of the
biggest are the vanishing and exploding gradient problems (Hochreiter and Schmidhuber, 1997),
which in practice is linked with the issue of learning long-term dependencies in language.
In this subsection, we expand our repertoire of RNN variants by going beyond the simple recurrent
dynamics defined by the Elman and Jordan update rules. To do so, we take a step back and return
to Definition 5.1.1 of a recurrent neural network Ras the tuplepΣ,D,f,h0q. We will define more
elaborate dynamics maps fwhich both aim to tackle some of the (empirically encountered) challenges
of simpler variants as well as improve some theoretical aspects of the networks. Importantly, keep
in mind that the only aspect of the RNN we will strive to modify is the dynamics map—that is,
the mapping from ht´1toht. Given a hidden state, the definition of a sequence model will remain
identical.
A common component of the more complex dynamics maps we explore in this section is the
gating mechanism, which is why we start with it.
Gating
The update equations of Elman and Jordan RNNs define relatively simple transformations of the
hidden states as an affine transformation of the previous hidden state and the new input, followed
by some form of non-linearity. In this sense, the interaction between the previous hidden state and
the input symbol is relatively limited—the hidden state is transformed by the recurrence matrix U
at every time step invariant to the input symbol being read. To see why this could be a limiting
factor, consider the following example.
Example 5.1.9: RNN Gates
Consider the language L“tanbncnxambmcm|n,mPNě0u. It intuitively consists of two-part
strings, where the two parts are separated by a symbol x. The part on the left side of x
contains a sequence of na’s followed by nb’s, which is followed by nc’s. The substring on the
right side of xcontains a sequence of ma’s which is again followed by mb’s, and later by mc’s.
Both parts of the string can be arbitrarily long, and, intuitively, to correctly recognize a string
in this language, a computational model has to keep the information about the number of a’s
while reading in b’s to be able to ensure there is a correct number of c’s as well. This creates
a long-term dependency across the entire block of b’s. However, notice that, after reading the
symbolx, the information about the number of a’s becomes irrelevant to the recognition of
the string: the model can, therefore, discard it and solely focus on modeling the rest of the
string, which again requires keeping track of the number of the symbol occurrences. In other
words: after a certain amount of time, previous information becomes irrelevant, and we may
want to design a network that is able to select which information is important to keep around .
To enable richer interaction between the transformation of the RNN hidden state and the input
symbols, we introduce the gating mechanism. Intuitively, the gating mechanism enables more
fine-grained control over the transformations of the hidden state by “selecting” which aspects of the
hidden state should be retained, which should be modified, and which should be deleted—in general,
5.1. RECURRENT NEURAL LANGUAGE MODELS 153
based on both the previous hidden state as well as the current input symbol. Such transformations
are defined using gates and gating functions.
Definition 5.1.9: Gate
Agate is a real-valued vector gPRD, such that gdPr0,1sfor alldPt1,...,Du. Gates are
computed using gating functions , i.e., functions whose outputs live in r0,1sD.
The fact that every dimension in a gate gttakes a value between 0 and 1 invites a natural
interpretation of the values as soft switches , analogously to how switches are used in the electrical
engineering context. Intuitively, in the context of RNNs, where the information is passed around in
the hidden states ht, a gate of the same dimensionality as the hidden state can control which aspects
(dimensions) of the hidden state should be forgotten (switched off) or and which ones retained (kept
on)—a gate value close to 0 can be interpreted as a signal that the information captured in the
corresponding dimension of the hidden state should be “forgotten”, and a gate value close to 1
as the opposite. Such modifications of the hidden state can be performed using an element-wise
multiplication of the hidden state hand the gate g, which we denote with hdg.
Importantly, the gates can be computed based on the information about the string seen so far as
well as the new input symbol—this means that the decision on what should be remembered and
what should be forgotten can be made for each situation individually. This allows RNN variants
using gating to implement mechanisms to tackle challenges as the one described in Example 5.1.9.
Furthermore this not only enables RNNs to selectively keep information about the string, but
also combat the vanishing and exploding gradient problems (Hochreiter and Schmidhuber, 1997,
Appendix 2). We next consider two of the best-known gated RNNs: Long Short-Term Memory and
Gated Recurrent Unit networks .
Long Short-term Memory Networks
Long Short-term Memory Networks (LSTM, Hochreiter and Schmidhuber, 1997) are perhaps the best-
known type of a gated recurrent network. They were introduced specifically to combat the vanishing
gradient problem in the famous paper with more than 80 000 citations. The somewhat unusual
name comes from connections to human memory, in which short-term memory is characterized by
evanescent neural activations, and long-term memory is based on the growth and structural change
in neuron connections (Hebb, 1949).
The LSTM unit. LSTM RNNs are built from the LSTM units, which implement the RNN
dynamics map and therefore perform the RNN update step. To transform the hidden state htat
each time step, an LSTM network additionally keeps another running summary of the string yăt, on
which the recurrent update depends—this is the so-called memory cell which we will denote by ct.
Informally, one can think of the context as the information needed to decide on how to transform
the hidden state at each individual time step, depending on the input string. The formal definition
of the LSTM cell is the following.
154 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
Definition 5.1.10: Long Short-Term Memory
Along short-term memory unit is a recurrent neural network with the dynamics map
defined through the following sequence of computations:
it“σ`
Uiht´1`Vie1yt`bi˘
(input gate)
ft“σ`
Ufht´1`Vfe1yt`bf˘
(forget gate)
ot“σ`
Uoht´1`Voe1yt`bo˘
(output gate)
gt“tanhpUght´1`Vge1yt`bgq (candidate vector)
ct“ftdct´1`itdgt (memory cell)
ht“otdtanhpctq (hidden state)
it,ft,otare the input ,forget , and output gates, ctis the memory cell vector, and gtis
thecandidate vector . Here,σrefers to the original sigmoid function.
As we can see, the update rule of an LSTM network is considerably more complex than that of an
Elman RNN. It is also computationally more expensive, as it involves more matrix multiplications.
However, LSTMs have consistently shown improved performance compared to vanilla RNNs and
are therefore considered together with GRUs the go-to choice for an RNN architecture (Goodfellow
et al., 2016). The theoretical reason of their success is that their gating mecahnism helps to reduce
the Vanishing/Exploding gradient problem, and thus to learn long-term dependencies (Hochreiter
and Schmidhuber, 1997, Appendix 2).
The names of the different quantities computed in Definition 5.1.10 reflect their intuitive
interpretations. The input, forget, and output vectors are all gates: they control the information
which will be added to the memory cell based on the new input, the information which will be
retained or forgotten from the previous memory cell, and the information which will be transferred
from the memory cell to the hidden state, respectively. Notice the identical nature in which all
three gates are computed: they are non-linear transformations of affine transformations of the
previous hidden state and the input symbol representations. Their parameter matrices define the
way in which the gates will influence the memorization, forgetting, and addition of information. The
additional information added to the memory cell in the form of the candidate vector gtis computed
similarly, with the only difference being the activation function. This is the step that bears the most
resemblance to the update step of the vanilla RNN (Eq. (5.26)). However, compared to the latter,
only parts of this transformation are kept (based on the input and forget vectors itandft). The
memory cell ctthen combines the old memory content ct´1with the newly integrated information
ingtto form the new memory content, which is then transformed using the tanh function and
combined with the output gate to produce the hidden state ht. This is pictorially presented in
Fig. 5.3.
As mentioned, the LSTM update step is noticeably more computationally complex than that of
a vanilla RNN. This has led to a line of work trying to combine the efficiency of vanilla RNNs and
the empirical performance of gated RNNs. In the next subsection, we consider Gated Recurrent
Units, one of the best-known compromises found in this domain.
5.1. RECURRENT NEURAL LANGUAGE MODELS 155
σft
σ Tanhgt
σˆ +
ˆit ˆotTanhct´1Cell
ht´1Hidden
e’yt Inputct
htht
Figure 5.3: A pictorial depiction of the LSTM cell in action. The input it, forget ft, and output
otgates control which information of the input and of the previous hidden state is retained in the
memory cell, and which information is passed to next the hidden state.
Gated Recurrent Units
The Gated Recurrent Unit (GRU, Cho et al., 2014b,a) provides a compromise between the simplicity
of vanilla recurrent neural networks and the empirical success of being able to model long-term
dependencies with LSTMs. It defines a gated recurrent update unit that implements a simpler
dynamics map by removing the memory component ctin the LSTM cell and combining the input
and forget gates it,ftinto one update gate. These changes make GRU more memory efficient and
easier to train than LSTM in practice. The full GRU update step is defined as follows.
Definition 5.1.11: Gated Recurrent Units
Agated recurrent unit defines a dynamics map in which a new hidden state is computed as:
rt“σ`
Urht´1`Vre1yt`br˘
(reset gate)
zt“σ`
Uzht´1`Vze1yt`bz˘
(update gate)
gt“tanh`
Ugprtdht´1q`Vge1yt`bg˘
(candidate vector)
ht“p1´ztqdgt`ztdht´1
rt,ztare known as the reset andupdate gates, and gtas the candidate vector.
Intuitively, the update gate works like a hot/cold water mixing valve: it is trained to find the
optimum blend of information of the candidate vector with that coming from the previous hidden
state. The reset gate instead, can zero the information of the previous hidden state, when computing
the candidate vector. This allows to forget past information that becomes irrelevant, exactly like in
156 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
the LSTM architecture.
Parallelizability: The Achilles’ Heel of Recurrent Neural Networks
It is easy to see from the definition of the RNN hidden state (cf. Definition 5.1.3) that, to compute
ht, we have to compute ht1for allt1ătfirst. Another way to say this is that RNNs are inherently
sequential models, processing the input string one symbol at a time to update their hidden state
ht, and using this hidden state in turn to compute ht`1. This results in perhaps the biggest
shortcoming of the architecture for its applicability to real-world language modeling: The inability
to efficiently parallelize the processing (encoding) of long strings. Let us first consider what we
mean by the parallelizability of a language model architecture.7Due to this sequential nature, the
training procedure of RNNs is difficult to parallelize effectively, leading to slower training times.
This characteristic poses a significant challenge when modeling long strings, as the computation for
each element is dependent on the computation of the previous element, leading to a bottleneck in
the training process.
In short, in our specific use case of language modeling, parallelization refers to the division of the
processing of a specific string across multiple computational nodes, such that any specific node only
performs a subset of operations required to process the entire string—the results of the subsets of
the operations are then combined to build the representation of the entire string. Importantly, the
computations should be performed independently between nodes in the sense that no node has to
wait for any other node to provide it the results of its computation. Being able to parallelize large
models across computational nodes has led to some of the biggest advancements in modern deep
learning. As such, parallelizability is a crucial feature of any successful deep learning architecture.
However, notice that any dependence between the computations performed by the nodes defeats
the purpose of parallelization—if the nodes have to wait for each other to finish computations,
the same operations might as well be performed by a single node. This is where recurrent neural
networks fall short: the computations required to encode a string yinto the hidden state h|y|will
always be sequential, preventing their distribution across different nodes.
Parallelizability in language modeling. When talking about parallelizing language models, it
is important to think about which parts can actually be parallelized. In the case of RNNs, we saw
that no part of the processing can be (besides the matrix multiplication in a single update rule)—the
length of the longest chain of dependent computation will always scale linearly with the length of
the string. In the next section, we introduce transformers, a recent neural network architecture first
introduced for processing text. One of the big contributions of transformers is their parallelizability
during training —it enables their training on extremely large corpora and is thus one of the main
reasons that they are behind the success of many of the most successful modern large language
models. Parallelizability during training is crucial—notice that parallelization is, in fact, not possible
during generation from a locally normalized language model (cf. Definition 2.4.5)—by definition,
such models will generate one symbol at a time . To compute the representation of the new sentence
(or the new prefix), which is required for the generation of the next symbol, the generated (sampled)
symbol has to first be determined, which leaves their generation process inherently sequential. In
that respect, RNNs are as parallelizable as they can be during generation. However, the sequential
7This section provides a very brief and intuitive treatment of parallelization. Our main goal is simply to point out
this shortcoming of RNN LMs and with it motivate the next neural architecture we will introduce: transformers.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 157
computation of the hidden states prevents the parallelization of computations of encRpyďtqeven if
the whole string is already given .
We will see that the big parallelizability improvements of other architectures only come into play
during training when the model is given the whole string in advance (such that no part of the string
has to be sequentially generated) and can compute the (log-)likelihood of the given ground-truth
next symbols given the context. That is, during training and given a string yPΣ˚, the model simply
has to compute pSMpyt|yătqfor allt“1,...,T —in representation based language models (cf.
Definition 3.1.11) this depends on encpyătq. Crucially, computing pSMpyt|yătqis all that we need
for training a language model (in the simplest case that we analyze)—the computed log-likelihood of
the ground-truth next character ytis used for computing the loss function during training and used
for gradient-based updates to the parameters as discussed in §3.2.3. In the next section, we will see
how encpyătqcan be computed without sequential dependencies. However, in the case of RNNs,
encpyătqcan only be computed sequentially—even if the entire string is known in advance. This
results in a crucial bottleneck in training RNNs on large corpora and vastly limits their applicability
to implementing large language models.
5.2 Representational Capacity of Recurrent Neural Networks
Recurrent neural networks are one of the fundamental and most successful neural language model
architectures. In this section, we study some theoretical explanations behind their successes as well
as some of their theoretical limitations. Answering this question is essential whenever we require
formal guarantees of the correctness of the outputs generated by an LM. For example, one might ask
a language model to solve a mathematical problem based on a textual description (Shridhar et al.,
2023) or ask it to find an optimal solution to an everyday optimization problem (Lin et al., 2021b).
If such problems fall outside the theoretical capabilities of the LM, we have no ground to believe
that the result provided by the model is correct. The question also follows a long line of work on
the linguistic capabilities of LMs, as LMs must be able to implement mechanisms of recognizing
specific syntactic structures to generate grammatical sequences (Talmor et al., 2020; Hewitt and
Manning, 2019; Jawahar et al., 2019; Liu et al., 2019; Icard, 2020a; Manning et al., 2020; Rogers
et al., 2021; Belinkov, 2022; Del’etang et al., 2022, inter alia ).
One way of quantifying the expressive power of computational models is with the complexity
of formal languages they can recognize (Del’etang et al., 2022)—we, too, will study the classes of
(weighted) formal languages (such as the regular languages and the Turing computable languages)
they can express. Through this, diverse formal properties of modern LM architectures have been
shown (e.g., Siegelmann and Sontag, 1992; Hao et al., 2018; Korsky and Berwick, 2019; Merrill,
2019; Merrill et al., 2020; Hewitt et al., 2020; Merrill et al., 2022a,b, inter alia). Inspecting complex
models such as recurrent neural networks through the lens of formal language theory allows us to
apply the well-studied theoretical results and understanding from the field to the recently more
successful neural models. While studying neural language models, we will revisit various aspects of
the classical language models introduced in Chapter 4—indeed, this was also our main motivation
for studying those closely.
Specifically, we will focus mainly on the Elman recurrent neural networks due to their simplicity
and their role as the “fundamental” RNN, capturing their recurrent nature. We will also briefly
touch upon the computational power of LSTM networks due to their somewhat different theoretical
properties. However, note that most of the results presented in the section generalize to other
architectures as well. We begin by investigating Elman RNNs in a practical setting, that is, under
158 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
relatively strict and realistic assumptions, such as fixed-point arithmetic. We show that Elman RNNs
under such a regime are in fact equivalent to weighted finite-state automata. Next, in the second
subsection, we show that under more permissive assumptions of infinite precision and unbounded
computation time, Elman RNNs are Turing complete.
5.2.1 RNNs and Weighted Regular Languages
Analyzing complex systems with intricate interactions between inputs and parameters and temporal
dependencies can be tricky. This is a common issue when studying neural networks in general. In
fact, most, if not all, theoretical frameworks for analyzing neural models such as RNNs rely on
various assumptions about their components to make the analysis feasible. For example, theoretical
results on neural networks (for example, optimization guarantees or function/system identifiability
guarantees) often make the assumption that the activation functions are linear or of some other
easy-to-analyze form. Similarly, a fruitful manner to analyze the expressivity of recurrent neural
networks specifically is by making (somewhat different) simplifying assumptions on the non-linear
activation functions, since those are what often make analysis difficult. A common simplification is
the use of the Heaviside activation function.
Definition 5.2.1: Heaviside function
TheHeaviside function is defined as
Hpxq“#
1ifxą0
0otherwise(5.29)
In words, the Heaviside function maps every real value either 0 or 1, depending on whether it is
greater than or less than zero. In the following, we will refer to the set t0,1uasBdef“t0,1u.
Definition 5.2.2: Heaviside Elman Network
AHeaviside Elman network (HRNN) is an Elman network with Heaviside function Has
the non-linearity.
Elman network parameters. Importantly, note that the parameters of the network do not have
to be elements of B—we assume those can take arbitrary real (or rational) values. Indeed, networks
constrained to parameters θPBwould only be able to recognize unweighted languages. Furthermore,
for this section, we expand our definition of a real- or rational-weighted RNN to be able to contain
weights8and´8. While those are not real (or rational) numbers, we will see they become useful
when we want to explicitly exclude specific sequences from the support of the model, i.e., when we
want to assign probability 0 to them.
Before we move to the central result of the subsection, we first introduce a fact that makes it
easier to talk about how an RNN language models can simulate a deterministic PFSAA. We will be
interested in conjoining elements of vectors in BD, which can be performed by an Elman RNN with
appropriately set parameters.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 159
Fact 5.2.1: Performing the ANDoperation with a neural network
LetmP rDs,i1,...,imP rDs, and x,vPBDwithvi“ 1tiPti1,...,imuu. Then,
H`
vJx´pm´1q˘
“1 if and only if xik“1 for allk“1,...,m . In other words,
H`
vJx´pm´1q˘
“xi1^¨¨¨^xim.
The central result. The central result of this section is captured in the following theorem from
Svete and Cotterell (2023b).
Theorem 5.2.1: Equivalence of Heaviside Elman RNNs and WFSAs
Heaviside Elman RNNs are equivalent to deterministic probabilistic finite-state automata.
Notice that we only make the claim for probabilistic WFSA. This is without loss of generality, as,
from Theorem 4.1.1, we know we can assume Ais locally normalized. We will prove Theorem 5.2.1
by showing that an RNN with Heaviside activations is at most regular, and then showing how such
an RNN can in fact simulate any deterministic PFSA. We show each direction as its own lemma.
Lemma 5.2.1
The distribution represented by a recurrent neural network with a Heaviside non-linearity H
is regular.
Proof. LetR“pΣ,D,U,V,E,bh,h0qbe a HRNN defining the conditional probabilities pSM. We
construct a deterministic PFSA A“ pΣ,Q,δ,λ,ρqdefining the same string probabilities. Let
s:BDÑZ2Dbe a bijection. Now, for every state qdef“sphqPQdef“BD, construct a transition
qy{wÝÝÑq1whereq1“σpUh`VJyK`bhqwith the weight w“pSMpy|hq “f∆|Σ|´1pE hqy. We
define the initial function as λpsphqq“ 1th“h0uand final function ρwithρpqqdef“pSMpeos|spqqq.
It is easy to see that Adefined this way is deterministic. We now prove that the weights assigned
to strings by AandRare the same. Let yPΣ˚with|y|“Tand
π“ˆ
sph0qy1{w1Ý ÝÝÝ Ñq1,...,qTyT{wTÝÝÝÝÑqT`1˙
they-labeled path starting in sph0q(such a path exists since we the defined automaton is complete —
all possible transitions are defined for all states).
Apyq“λpsph0qq¨«Tź
t“1wtff
¨ρpqT`1q
“1¨Tź
t“1pSM`
yt|s´1pqtq˘
¨pSM`
eos|s´1pqT`1q˘
“pLNpyq
which is exactly the weight assigned to ybyR. Note that all paths not starting in sph0qhave
weight 0 due to the definition of the initial function. ■
160 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
q00 q01
q10{e
2e`1q11a{1
2b{1
2
a{1
e`1b{e
e`1
a{e
2e`1b{1
2e`1a{1
2
b{1
2
Figure 5.4: The WFSA corresponding to the RNN defined in Eq. (5.30).
Let us look at an example of the construction above.
Example 5.2.1: A PFSA simulating an RNN
LetR“pΣ,D,f,E,h0qbe a Heaviside RNN sequence model with the parameters
Σ“ta,bu (5.30)
D“2 (5.31)
fpht,yq“Hˆˆ
1 0
0 1˙
ht´1`ˆ
1 0
0 1˙
JyK˙
(5.32)
E“¨
˝1 0
0 1
1´8˛
‚ (5.33)
h0“ˆ
0
0˙
(5.34)
andnpaq“1,npbq“2, andnpeosq“3. The automaton corresponding to this RNN contains
the statesqijcorresponding to the hidden states h“ˆ
i
j˙
. It is shown in Fig. 5.4; as we can
see, the automaton indeed has an exponential number of useful states in the dimensionality of
the hidden state, meaning that the RNN is a very compact way of representing it.
To show the other direction of Theorem 5.2.1, we now give a variant of a classic theorem originally
due to Minsky (1986) but with a probabilistic twist, allowing us to model weighted languages with
Elman RNNs.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 161
Lemma 5.2.2: Elman RNNs can encode PFSAs
LetA“pΣ,Q,δ,λ,ρqbe a tight probabilistic deterministic finite-state automaton. Then,
there exists a Heaviside-activated Elman network with a hidden state of size h“|Σ||Q|that
encodes the same distribution as A.
We give proof by construction: Given a deterministic PFSA A“pΣ,Q,δ,λ,ρq, we construct an
Elman RNN R“pΣ,D,U,V,E,bh,h0qaccepting the same weighted language as A:LpAq“LpRq
by defining the elements of the tuple pΣ,D,U,V,E,bh,h0q. In the rest of the section, we will
first intuitively describe the construction, and then formally prove the central results that the
construction relies on. Let n:QˆΣÑZ|Q||Σ|be a bijection, i.e., an ordering of QˆΣ,m:ΣÑZ|Σ|
an ordering of Σ, and m:ΣÑZ|Σ|an ordering of Σ; these mappings assign each element in their
pre-image an integer which can then be used to index into matrices and vectors as we will see below.
We usen,m, andmto define the one-hot encodings J¨Kof state-symbol pairs and of the symbols.
That is, we assume that Jq,yKd“ 1td“npq,yqu, and similar for JyK. Similarly to the proof of
Lemma 5.2.1, we denote with handh´1the mappings from QˆΣto the hidden state space of the
RNN and its inverse. The alphabet of the RNN of course matches the one of the WFSA.
HRNN’s hidden states. The hidden states of the RNN live in B|Q||Σ|. A hidden state htencodes
the stateqtthe simulated Ais in at time tandthe transition symbol ytwith which A“arrived” at
qtas a one-hot encoding of the pairpqt,ytq. Formally,
ht“Jpqt,ytqKPB|Q||Σ|. (5.35)
This also means that D“|Q||Σ|. There is a small caveat: how do we set the incoming symbol
ofA’s (sole) initial state qι(the first time it is entered)? A straightforward solution would be to
augment the alphabet of the RNN with the bossymbol (cf. §2.4), which we define to be the label
of the incoming arc denoting the initial state (this would be the only transition labeled with bos).
However, as we show later, the symbol used to arrive into pdoes not have an effect on the subsequent
transitions—it is only needed to determine the target of the current transition. Therefore, we can
simply represent the initial state h0ofRwith the one-hot encoding of anypairpqι,aq, whereqιis
the initial state of the WFSA and aPΣ.
For example, for the fragment of a WFSA in Fig. 5.5, the hidden state encoding the current
stateqand the incoming arc bis of the form presented in Eq. (5.36).
r qq1
q2b{˝a{˝
b{˝
Figure 5.5: A fragment of a WFSA.ht“¨
˚˚˚˚˚˚˚˚˚˚˚˚˚˝˛
‹‹‹‹‹‹‹‹‹‹‹‹‹‚0
...
0
1Ðnpq,bq
0
...
0(5.36)
162 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
qq1
q2a{w0a{w1
b{w2
h“Jq,aKqq1
q2a{w0a{w1
b{w2Uh“Jq1,aK`Jq2,bK
qq1
q2a{w0a{w1
b{w2
VJaK“Jq,aK`Jq1,aKqq1
q2a{w0a{w1
b{w2
h1“Jq1,aKOut-neighborhood
a-reachablef∆|Σ|´1pEh1q“¨
˝a:w1
b:w2
eos: 0˛
‚
Figure 5.6: A high-level illustration of how the transition function of the FSA is simulated in
Minsky’s construction on a fragment of an FSA starting at q(encoded in h) and reading the symbol
a. The top path disjoins the representations of the states in the out-neighborhood of q, whereas
the bottom path disjoins the representations of states reachable by an a-transition. The Heaviside
activation conjoins these two representations into h1(rightmost fragment). Projecting Eh1results in
the vector defining the same probability distribution as the outcoming arcs of q(green box).
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 163
Encoding the transition function. The idea of defining U,V, and bhis for the Elman update
rule to perform, upon reading yt`1, element-wise conjunction between the representations of the
out-neighborhood of qtand the representation of the states Acan transition into after reading in
yt`1from any state . The former is encoded in the recurrence matrix U, which has access to the
current hidden state that encodes qtwhile the latter is encoded in the input matrix V, which has
access to the one-hot representation of yt`1. Conjugating the entries in those two representations
will, due to the determinism of A, result in a single non-zero entry: one representing the state which
can be reached from qt(1stcomponent) using the symbol yt`1(2ndcomponent); see Fig. 5.6.
The recurrence matrix Ulives in B|Σ||Q|ˆ|Σ||Q|. The main idea of the construction is for each
column U:,npq,yqof the matrix to represent the “out-neighborhood” of the state qin the sense that
the column contains 1’s at the indices corresponding to the state-symbol pairs pq1,y1qsuch that A
transitions from qtoq1after reading in the symbol y1. That is, for q,q1PQandy,y1PΣ, we define
Unpq1,y1q,npq,yqdef“ 1"
qty1{˝ÝÝÑq1Pδ*
. (5.37)
Sinceyis free, each column is repeated |Σ|-times: once for every yPΣ—this is why, after entering
the next state, the symbol used to enter it does not matter anymore and, in the case of the initial
state, any incoming symbol can be chosen to represent h0.
For example, for the fragment of a WFSA in Fig. 5.5, the recurrence matrix would take the form
U“npq,bq
Ó¨
˚˚˚˚˚˚˚˚˚˚˝˛
‹‹‹‹‹‹‹‹‹‹‚0
...
1 Ðnpq1,aq
¨¨¨...¨¨¨
1 Ðnpq2,bq
...
0(5.38)
and the matrix-vector product Uhtwithhtfrom before results in
Uht“¨
˚˚˚˚˚˚˚˚˚˚˚˚˚˚˝˛
‹‹‹‹‹‹‹‹‹‹‹‹‹‹‚0
...
1Ðnpq1,aq
...
1Ðnpq2,bq
...
0(5.39)
164 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
The input matrix Vlives in B|Σ||Q|ˆ|Σ|and encodes the information about which states can be
reached by which symbols (from anystate in A). The non-zero entries in the column corresponding
toy1PΣ correspond to the state-symbol pairs pq1,y1qsuch thatq1is reachable with y1from some
state:
Vnpq1,y1q,mpy1qdef“ 1"
˝y1{˝ÝÝÑq1Pδ*
. (5.40)
For example, for the fragment of a WFSA in Fig. 5.7a, the input matrix would take the form
V“mpbq
Ó¨
˚˚˚˚˚˚˚˚˚˚˝˛
‹‹‹‹‹‹‹‹‹‹‚0
...
1 Ðnpp,bq
¨¨¨...¨¨¨
1 Ðnpq2,bq
...
0(5.41)
and the matrix-vector product VempaqandVempbqwould take the form (see also Fig. 5.7b)
Vempaq“¨
˚˚˚˚˚˚˚˚˚˝˛
‹‹‹‹‹‹‹‹‹‚0
...
1Ðnpq1,aq
...
0Vempbq“¨
˚˚˚˚˚˚˚˚˚˚˚˚˚˚˝˛
‹‹‹‹‹‹‹‹‹‹‹‹‹‹‚0
...
1Ðnpp,bq
...
1Ðnpq2,bq
...
0(5.42)
Lastly, we define the bias as bhdef“´1PR|Q||Σ|, which allows the Heaviside function to perform the
needed conjunction.
To put these components together, consider that, at each step of the computation, Rcomputes
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 165
r pq1
q2b{˝a{˝
b{˝
(a) An example of a fragment of a WFSA.rpq1
q2b{˝a{˝
b{˝
(b) An example of a fragment of a WFSA.
ht`1“HpUht`Vea`bhqwhereyt`1“a. The input to the non-linearity is computed as follows:
Uht`Vempaq`bh“¨
˚˚˚˚˚˚˚˚˚˚˚˚˚˚˝˛
‹‹‹‹‹‹‹‹‹‹‹‹‹‹‚0
...
1Ðnpq1,aq
...
1Ðnpq2,bq
...
0`¨
˚˚˚˚˚˚˚˚˚˝˛
‹‹‹‹‹‹‹‹‹‚0
...
1Ðnpq1,aq
...
0`¨
˚˚˚˚˚˚˚˚˚˝˛
‹‹‹‹‹‹‹‹‹‚´1
...
´1
...
´1(5.43)
The following lemma proves that the construction described correctly implements the transition
function of the PFSA.
Lemma 5.2.3
LetA“pΣ,Q,δ,λ,ρqbe a deterministic PFSA, y“y1...yTPΣ˚, andqtthe state arrived at
byAupon reading the prefix yďt. Let Rbe the HRNN specified by the Minsky construction
forA,nthe ordering defining the one-hot representations of state-symbol pairs by R, and ht
R’s hidden state after reading yďt. Then, it holds that h0“Jpqι,yqKwhereqιis the initial
state of AandyPΣ and hT“JpqT,yTqK.
Proof. Definesph“Jpq,yqKqdef“q. We can then restate the lemma as sphTq“qTfor all yPΣ˚,
|y|“T. Letπbe the y-labeled path in A. We prove the lemma by induction on the string length
T.
Base case: T“0.Holds by the construction of h0.
Inductive step: Tą0.LetyPΣ˚with|y|“Tand assume that sphT´1q“qT´1.
We prove that the specifications of U,V, and bhensure that sphTq “qT. By definition
of the recurrence matrix U(cf. Eq. (5.37)), the vector UhT´1will contain a 1 at the entries
166 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
npq1,y1qforq1PQandy1PΣ such that qTy1{˝ÝÝÑq1Pδ. This can equivalently be written as
UhT´1“Ž
qTy1{˝ÝÝÑq1PδJpq1,y1qK, where the disjunction is applied element-wise.
On the other hand, by definition of the input matrix V(cf. Eq. (5.40)), the vector VJyTKwill
contain a 1 at the entries npq1,yTqforq1PQsuch that˝yT{˝Ý ÝÝ Ñq1Pδ. This can also be written as
VJyTK“Ž
˝yT{˝Ý ÝÝ Ñq1PδJpq1,yTqK.
By Fact 5.2.1, HpUhT´1`VJyTK`bhqnpq1,y1q“HpUhT´1`VJyTK´1qnpq1,y1q“1 holds if
and only ifpUhT´1qnpq1,y1q“1 andpVJyTKqnpq1,y1q“1. This happens if
qTy1{˝ÝÝÑq1Pδand˝yT{˝Ý ÝÝ Ñq1PδðñqTyT{˝Ý ÝÝ Ñq1, (5.44)
i.e., if and only if Atransitions from qTtoqTupon reading yT(it transitions only to qTdue to
determinism).
Since the string ywas arbitrary, this finishes the proof. ■
Encoding the transition probabilities. We now turn to the second part of the construction:
encoding the string acceptance weights given by Ainto the probability distribution defined by R.
We present two ways of doing that: using the more standard softmax formulation, where we make
use of the extended real numbers, and with the sparsemax.
The conditional probabilities assigned by Rare controlled by the |Σ|ˆ|Q||Σ|-dimensional output
matrix E. Since htis a one-hot encoding of the state-symbol pair qt,yt, the matrix-vector product
Ehtsimply looks up the values in the npqt,ytqthcolumn. After being projected to ∆|Σ|´1, the
entry in the projected vector corresponding to some yt`1PΣshould match the probability of that
symbol given that Ais in the state qt. This is easy to achieve by simply encoding the weights of the
outgoing transitions into the npqt,ytqthcolumn, depending on the projection function used. This is
especially simple in the case of the sparsemax formulation. By definition, in a PFSA, the weights of
the outgoing transitions and the final weight of a state qtform a probability distribution over Σfor
everyqtPQ. Projecting those values to the probability simplex, therefore, leaves them intact. We
can therefore define
Empy1qnpq,yqdef“#
ωpqy1{wÝ ÝÝ Ñ˝q | ify1PΣ
ρpqq | otherwise. (5.45)
Projecting the resulting vector Eht, therefore, results in a vector whose entries represent the
transition probabilities of the symbols in Σ.
In the more standard softmax formulation, we proceed similarly but log the non-zero transition
weights. Defining log 0def“´8 ,8we set
Empy1qnpq,yqdef“#
logωpqy1{wÝ ÝÝ Ñ˝q | ify1PΣ
logρpqq | otherwise. (5.46)
It is easy to see that the entries of the vector softmaxpEhtqform the same probability distribution as
the original outgoing transitions out of q. Over the course of an entire input string, these weights are
8Note that the ´8entries are only needed whenever the original WFSA assigns 0 probability to some transitions.
In many implementations using softmax-activated probabilities, this would not be required.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 167
r pq1
q2b{˝a{w1
b{w
2
Figure 5.8: An example of a fragment of a WFSA.
multiplied as the RNN transitions between different hidden states corresponding to the transitions
in the original PFSA A.
For example, for the fragment of a WFSA in Fig. 5.8, the output matrix would take the form
E“npq,bq
Ó¨
˚˚˚˚˚˚˚˚˚˚˝˛
‹‹‹‹‹‹‹‹‹‹‚´8
...
logw1Ðmpaq
¨¨¨...¨¨¨
logw2Ðmpbq
...
´8(5.47)
This means that, if htencodes the state-symbol pair pq,yq, the vector Ehtwill copy the
selected column in Ewhich contains the output weight for all out symbols y; ofq, i.e., the entry
Ehmpy1qcontains the weight on the arc qy1{wÝ ÝÝ Ñ˝ . Over the course of an entire input string y,
these probabilities are simply multiplied as the RNN transitions between different hidden states
corresponding to the transitions in the original WFSA A.
For example, for the fragment of a WFSA in Fig. 5.8, the matrix-vector product Ehtwould take
the form
Eht“¨
˚˚˚˚˚˚˚˚˚˚˝˛
‹‹‹‹‹‹‹‹‹‹‚´8
...
logw1Ðmpaq
...
logw2Ðmpbq
...
´8(5.48)
The equivalence of the produced RNN LM to the PFSA is shown in the following lemma.
168 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
q0 1q1
q2{0.5a{0.1
b{0.9a{0.5
b{0.5b{0.5
Figure 5.9: The WFSA A.
Lemma 5.2.4
LetA“pΣ,Q,δ,λ,ρqbe a deterministic PFSA, y“y1...yTPΣ˚, andqtthe state arrived at
byAupon reading the prefix yďt. Let Rbe the HRNN specified by the Minsky construction
forA,Ethe output matrix specified by the generalized Minsky construction, nthe ordering
defining the one-hot representations of state-symbol pairs by R, and htR’s hidden state after
reading yďt. Then, it holds that pLNpyq“Apyq.
Proof. LetyPΣ˚,|y|“Tand let πbe the y-labeled path in A. Again, let ppyqdef“ś|y|
t“1pSMpyt|yătq.
We proveppyq“śT
t“1wtby induction on T.
Base case: T“0.In this case, y“ε, i.e., the empty string, and Apεq “1.Rcomputes
ppεq“ś0
t“1pSMpyt|yătq“1.
Inductive step: Tą0.Assume that the ppy1...yT´1q“śT´1
t“1wt. By Lemma 5.2.3, we know
thatsphT´1q“qTandsphTq“qT. By the definition of Efor the specific f∆|Σ|´1, it holds that
f∆|Σ|´1pEhT´1qmpyq“ωpsphT´1qy{wTÝÝÝÑsphTqq“wT. This means that ppyďTq“śT
t“1wt, which
is what we wanted to prove.
Clearly,pLNpyq“ppyqpSMpeos|yq. By the definition of E(cf. Eq. (5.45)), pEhTqmpeosq“
ρpsphTqq, meaning that
pLNpyq“ppyqpSMpeos|yq“Tź
t“1wtρpsphTqq“Apyq.
Since yPΣ˚was arbitrary, this finishes the proof. ■
We now walk through an example of the Minsky construction.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 169
Example 5.2.2: Minsky construction
LetA“ pΣ,Q,δ,λ,ρqbe a WFSA as shown in Fig. 5.9. Since Ahas|Q| “ 3 states
and an alphabet of |Σ| “2 symbols, the hidden state of the representing RNN Rwill
be of dimensionality 3 ¨2“6. Assume that the set of state-symbol pairs is ordered as
pq0,aq,pq0,bq,pq1,aq,pq1,bq,pq2,aq,pq2,bq. The initial state can be represented (choosing a
as the arbitrary “incoming symbol”) as
h0“¨
˚˚˚˚˚˚˝1
0
0
0
0
0˛
‹‹‹‹‹‹‚. (5.49)
The recurrent matrix UofRis
U“¨
˚˚˚˚˚˚˝0 0 1 1 0 0
0 0 0 0 0 0
1 1 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
1 1 1 1 1 1˛
‹‹‹‹‹‹‚, (5.50)
the input matrix V
V“¨
˚˚˚˚˚˚˝1 0
0 0
1 0
0 0
0 0
0 1˛
‹‹‹‹‹‹‚, (5.51)
and the output matrix Eis
E“¨
˝logp0.1qlogp0.1qlogp0.5qlogp0.5q ´8 ´8
logp0.9qlogp0.9qlogp0.5qlogp0.5qlogp0.5qlogp0.5q
´8 ´8 ´8 ´8 logp0.5qlogp0.5q˛
‚, (5.52)
where the last row corresponds to the symbol eos. The target of the b-labeled transition from
170 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
q0(q0b{0.9ÝÝÝÑq2) is computed as follows:
h1“HpUh0`VJbK`bhq
“H¨
˚˚˚˚˚˚˝¨
˚˚˚˚˚˚˝0 0 1 1 0 0
0 0 0 0 0 0
1 1 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
1 1 1 1 1 1˛
‹‹‹‹‹‹‚¨
˚˚˚˚˚˚˝1
0
0
0
0
0˛
‹‹‹‹‹‹‚`¨
˚˚˚˚˚˚˝1 0
0 0
1 0
0 0
0 0
0 1˛
‹‹‹‹‹‹‚ˆ
0
1˙
`¨
˚˚˚˚˚˚˝´1
´1
´1
´1
´1
´1˛
‹‹‹‹‹‹‚˛
‹‹‹‹‹‹‚
“H¨
˚˚˚˚˚˚˝¨
˚˚˚˚˚˚˝0
0
1
0
0
1˛
‹‹‹‹‹‹‚`¨
˚˚˚˚˚˚˝0
0
0
0
0
1˛
‹‹‹‹‹‹‚`¨
˚˚˚˚˚˚˝´1
´1
´1
´1
´1
´1˛
‹‹‹‹‹‹‚˛
‹‹‹‹‹‹‚“H¨
˚˚˚˚˚˚˝¨
˚˚˚˚˚˚˝´1
´1
0
´1
´1
1˛
‹‹‹‹‹‹‚˛
‹‹‹‹‹‹‚“¨
˚˚˚˚˚˚˝0
0
0
0
0
1˛
‹‹‹‹‹‹‚,
which corresponds exactly the configuration in which Ais in stateq2which it arrived to by
reading in the symbol b.
The probability of the string y“bunder the locally-normalized model induced by Rcan be
computed as
pLNpyq“pLNpbq“pSMpb|bosqpSMpeos|bq“pSMpb|h0qpSMpeos|h1q
“softmaxpEh0qbsoftmaxpEh1qeos
“softmax¨
˚˚˚˚˚˚˝¨
˝logp0.1q ¨¨¨ ´8
logp0.9q ¨¨¨ logp0.5q
´8 ¨¨¨ logp0.5q˛
‚¨
˚˚˚˚˚˚˝1
0
0
0
0
0˛
‹‹‹‹‹‹‚˛
‹‹‹‹‹‹‚
b¨
softmax¨
˚˚˚˚˚˚˝¨
˝logp0.1q ¨¨¨ ´8
logp0.9q ¨¨¨ logp0.5q
´8 ¨¨¨ logp0.5q˛
‚¨
˚˚˚˚˚˚˝0
0
0
0
0
1˛
‹‹‹‹‹‹‚˛
‹‹‹‹‹‹‚
eos
“softmax¨
˝logp0.1q
logp0.9q
´8˛
‚
b¨softmax¨
˝´8
logp0.5q
logp0.5q˛
‚
eos“0.9¨0.5“0.45.
Implications for recurrent neural language models. Lemmas 5.2.1 and 5.2.2 formalize the
equivalence between HRNNs and deterministic PFSAs. A direct corollary of this result is that
HRNNs are at most as expressive as deterministic PFSAs and, therefore, strictly less expressive as
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 171
q0{1q1
q2q3{1a{0.5
a{0.5b{0.9
b{0.1c{0.1
c{0.9
Figure 5.10: A non-determinizable PFSA. It assigns the string abncthe probability Apabncq“
0.5¨0.9n¨0.1`0.5¨0.1n¨0.9, which can not be expressed as a single term for arbitrary nPNě0.
general, non-deterministic, PFSAs.9An example of a very simple non-deterministic PFSA, i.e., a
PFSA whose distribution cannot be expressed by an HRNN LM, is shown in Fig. 5.10. Furthermore,
even if a non-deterministic PFSA can be determinized, the number of states of the determinized
machine can be exponential in the size of the non-deterministic one (Buchsbaum et al., 2000).
In this sense, non-deterministic PFSAs can be seen as exponentially compressed representations
of finite-state LMs. However, the compactness of this non-deterministic representation must be
“undone” using determinization before it can be encoded by an HRNN.
While Lemma 5.2.1 focuses on HRNN LMs and shows that they are finite-state, a similar
argument could be made for any RNN whose activation functions map onto a finite set. This is
the case with any implementation of an RNN on a computer with finite-precision arithmetic—in
that sense, all deployed RNNLMs are finite-state, albeit very large in the sense of encoding possibly
very large weighted finite-state automata. However, there are a few important caveats with this:
firstly, notice that, although finite, the number of states represented by an RNN is exponential in
the size of the hidden state. Even for moderate hidden state dimensionalities, this can be very large
(hidden states can easily be of size 100–1000). In other words, one can view RNNs as very compact
representations of large deterministic probabilistic finite-state automata whose transition functions
are represented by the RNN’s update function. Furthermore, since the topology of this implicit
WFSA is completely determined by the update function of the RNN, it can be learned very flexibly
yet efficiently based on the training data—this is made possible by the sharing of parameters across
the entire graph of the WFSA instead of explicitly parametrizing every possible transition, as, for
example, in §4.1.3, or hard-coding the allowed transitions as in §4.1.5. This means that the WFSA
is not only represented, but also parametrized very efficiently by an RNN. Nevertheless, there is an
important detail that we have somewhat neglected so far: this is the requirement that the simulated
WFSA be deterministic .
9General PFSAs are, in turn, equivalent to probabilistic regular grammars and discrete Hidden Markov Models
(Icard, 2020b).
172 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
5.2.2 Addendum to Minsky’s Construction: Lower Bounds on the Space
Complexity of Simulating PFSAs with RNNs
Lemma 5.2.2 shows that HRNN LMs are at least as powerful as dPFSAs. More precisely, it shows
that any dPFSA A“pΣ,Q,δ,λ,ρqcan be simulated by an HRNN LM of size Op|Q||Σ|q. In this
section, we address the following question: How large does an HRNN LM have to be such that it
can correctly simulate a dPFSA? We study the asymptotic bounds with respect to the size of the
set of states,|Q|, as well as the number of symbols, |Σ|.10
Asymptotic Bounds in |Q|.Intuitively, the 2Dconfigurations of a D-dimensional HRNN hidden
state could represent 2Dstates of a (P)FSA. One could therefore expect that we could achieve
exponential compression of a dPFSA by representing it as an HRNN LM. Interestingly, this is not
possible in general: extending work by Dewdney (1977), Indyk (1995) shows that, to represent an
unweighted FSA with an HRNN, one requires an HRNN of size Ω´
|Σ|a
|Q|¯
. This lower bound can
be achieved. For completeness, we present constructions by Dewdney (1977); Indyk (1995), which
represent an unweighted FSA with a HRNN of size O´
|Σ||Q|3
4¯
andO´
|Σ|a
|Q|¯
, respectively,
next, before giving a lower bound in for the probabilistic case.
Lemma 5.2.2 gives a relatively simple construction of an RNN recognizing a weighted regular
language. However, the resulting RNN is relatively large, with a hidden state of size linear in
the number of states of the (deterministic) WFSA recognizing the language, with the additional
multiplicative factor in the size of the alphabet. Note that constructions resulting in smaller RNNs
exist, at least for the unweighted case. For example, for an arbitrary WFSA A“pΣ,Q,δ,λ,ρq,
Dewdney (1977); Alon et al. (1991) present a construction of an RNN with a hidden state of size
O´
|Q|3
4¯
simulating A, whereas Indyk (1995) provides a construction of an RNN with a hidden
state of size O´
|Q|1
2¯
. The latter is also provably a lower bound on the number of neurons required
to represent an arbitrary unweighted FSA with a Heaviside-activated recurrent neural network
(Indyk, 1995). It is not yet clear if this can be generalized to the weighted case or if Minsky’s
construction is indeed optimal in this setting. This is quite interesting since one would expect that
an RNN with a hidden state of size Dcan represent up to 2Dindividual states (configurations of the
D-dimensional vector). However, the form of the transition function with the linear transformation
followed by a Heaviside activation limits the number of transition functions that can be represented
usingDdimensions, resulting in the required exponential increase in the size of the hidden state.
Minsky’s construction (Lemma 5.2.2) describes how to represent a dPFSA Awith a HRNN of
size linear in the number of A’s states. Importantly, the encoding of the FSA transition function
(taken from Minsky’s original construction) is decoupled from the parameter defining the probability
distribution, E. This section describes two asymptotically more space-efficient ways of constructing
the component simulating the transition function. They originate in the work by Dewdney (1977),
who showed that an unweighted FSA A“pΣ,Q,I,F,δqcan be represented by an HRNN of size
O´
|Σ||Q|3
4¯
. Using the same ideas, but a specific trick to compress the size of the processing
layer of the RNN further, Indyk (1995) reduced this bound to O´
|Σ|a
|Q|¯
, which, as discussed
in§5.2.3, is asymptotically optimal. Naturally, as shown in §5.2.3, the space-efficiency gain can
not be carried over to the weighted case—that is, the space-efficiency is asymptotically overtaken
10This section is based on the survey by Svete and Cotterell (2023a).
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 173
by the output matrix E. Nevertheless, for a more complete treatment of the subject, we cover the
two compressed constructions of the HRNN simulating an unweighted FSA in this section in our
notation. Importantly, given a dPFSA, we focus only on the underlying FSA , i.e., the unweighted
transition function of the automaton, since by Theorem 5.2.4, the compression can only be achieved
with components representing that part of the automaton.
Dewdney’s Construction
This section describes the construction due to Dewdney (1977) in our notation. Since some of the
parts are very similar to the construction due to Indyk (1995), those parts are reused in §5.2.2 and
introduced more generally.
Representing states of the FSA. LetA“pΣ,Q,I,F,δqbe a deterministic FSA. Recall that
Minsky’s construction encodes the A’s current state as a one-hot encoding of the state-symbol pair.
The construction due to Dewdney (1977), on the other hand, represents the states separately from
the symbols. It encodes the states with two-hot representations by using the coefficients of what we
call a square-root state representation. This results in representations of states of size O´a
|Q|¯
.
The input symbols are incorporated into the hidden state separately .11
Definition 5.2.3: Square-root state representation
LetA“pΣ,Q,I,F,δqbe an FSA and sdef“ ra
|Q|s. We define the square-root state
representation ofA’s statesqPQasa
ϕ2pqqdef“´
tq
su,qmods¯
. (5.53)
We denote the inverse of ϕ2withϕ´1
2and further define for kPZs
ϕ´1
2pk,¨qdef“tqPQ|φ0“kwhere φ“ϕ2pqqu (5.54)
andϕ´1
2p¨,kqanalogously.
aNotice that ϕ2pqqrepresents the coefficients of the expression of qPNin bases.
Specifically, we will denote ϕ´1
2pk,¨qandϕ´1
2p¨,kqwithkin thejthposition (with jPZ2, 0 for
ϕ´1
2pk,¨qand 1 forϕ´1
2p¨,kq) as Φk,j.
We can think of the function ϕ2as representing states of the FSA in a two-dimensional space
ZsˆZs. However, to efficiently simulate Awith an HRNN, it is helpful to think of ϕ2pqqin two
different ways: as a vector vPNě02|Q|, or as a matrix in B|Q|ˆ|Q|in the following sense.
Definition 5.2.4: Vector and matrix state representations
Given a square-root state representation function ϕ2, we define the vector representation
11This again adds a factor |Σ|to the size of the hidden state, as we discuss later.
174 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
of the state qPQas the vector vpqqPB2|Q|with
vpqqφ0“1 (5.55)
vpqqs`φ1“1, (5.56)
where φ“pφ0,φ1q“ϕ2pqq, and all other entries 0. Furthermore, we define the matrix
representation of the state qPQas the matrix BPB|Q|ˆ|Q|with
WQqφ0φ1“1 (5.57)
and all other entries 0.
Dewdney’s construction also heavily relies on the representations of sets of states. We define
those additively.
Definition 5.2.5: Matrix and vector representation of state sets
LetQĎQbe a set of states. We define the vector representation of Qas the vector
vpQqdef“ł
qPQvpqq. (5.58)
Similarly, we define the matrix representation of Qas the matrix
WQQdef“ł
qPQWQq. (5.59)
To help understand the above definitions, we give an example of an FSA and the representations
of its states.
Example 5.2.3: Dewdney’s construction
Consider the FSA in Fig. 5.11, for which s“ra
|Q|s“r?
3s“2, meaning that
ϕ2p0q“p 0,0qϕ2p1q“p 0,1qϕ2p2q“p 1,0q, (5.60)
resulting in the state-to-vector mappinga
vp0q“`
1 0|1 0˘
(5.61)
vp1q“`
1 0|0 1˘
(5.62)
vp2q“`
0 1|1 0˘
, (5.63)
and the state-to-matrix mapping
WQ0“ˆ
1 0
0 0˙
WQ1“ˆ
0 1
0 0˙
WQ2“ˆ
0 0
1 0˙
. (5.64)
The two components of the vector representations separated by “ |” denote the two halves of
the representation vectors, corresponding to the two components of ϕ2pqq.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 175
0
12
ab
ab
b
Figure 5.11: An example of a fragment of an FSA.
aDespite the notation p...|...q, we assume we are working with column vectors.
High-level idea of Dewdney’s construction. Given these definitions, the intuition behind
Dewdney’s construction of an HRNN simulating an FSA Ais the following:
1. Represent A’s states as vectors in B2s, or, equivalently, matrices in Bsˆs.
2.For eachqPQ, construct the matrix representation of the set of y-predecessors WQPredpq;yq
for allyPΣ.
3.To simulate A’s transition function δ, compare the representation of the current state qt
with all constructed predecessor matrices WQPredpq;ytqgiven the current input symbol yt.
Activate the two-hot representation of the (unique) state qt`1for which the representation of
qtwas detected in qt`1’s predecessor matrix for symbol yt,WQPredpqt`1;ytq.
Simulating the transition function of an FSA by detecting preceding states. We elaborate
on the last point above since it is the central part of the construction.12The idea of simulating the
transition function δis reduced to detecting whose predecessor given the current input symbol ytis
currently active—naturally, this should be the state active at t`1. Concretely, consider again the
FSAAin Fig. 5.11. The predecessors of the three states, indexed by the incoming symbols are: for
0tb: 2u, for 1ta: 1,b: 0u, and for 2ta: 1,b: 0u. Suppose that at some time t,Ais in state 0 and
is reading in the symbol b. Then, since the state 0 is the b-predecessor of the state 2, we know that
at timet`1,Awill be in state 2. This principle can be applied more generally: to determine the
state of an FSA at time t`1, we simply have to somehow detect whose predecessor is active at
timetgiven the current input symbol at timet.
The crux of Dewdney’s construction is then the following:13How do we, using only the Elman
update rule, determine whose yt-predecessor is active at time t? This can be done by detecting
which predecessor matrix WQPredpq;ytqthe representation of the current state qtis included in
in the sense that if ϕ2pqtq“φ, it holds that WQPredpq;ytqφ0φ1“1. To be able to formally talk
about the detection of a representation in a set of predecessors, we define several notions of matrix
detection .
Informally, we say that a matrix is easily detectable if the presence of its non-zero elements can
be detected using a single neuron in the hidden layer of a HRNN.
12Later, we will see that Indyk (1995) uses the exact same idea for simulating δ.
13Again, the same applies to Indyk (1995).
176 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
Definition 5.2.6: Easily detectable matrices
LetBPBDˆDbe a binary matrix. We say that Biseasily detectable if there exist wPQ2D
andbPQ(neuron coefficients) such that
σpxeij,wy`bq“1ðñBij“1, (5.65)
where eij“`
ei|ej˘
refers to the 2 D-dimensional vector with 1’s at positions iandD`j.
In words, this means that the neuron defined by w,bfires on the input eijif and only if
Bij“1.
We define detectable matrices as the matrices which can be detected using a conjunction of two
neurons.
Definition 5.2.7: Detectable matrices
LetBPBDˆDbe a binary matrix. We say that Bisdetectable if there exist w1,w2PQ2D
andb1,b2PQsuch that
σpxeij,w1y`b1q“1^σpxeij,w2y`b2q“1ðñBij“1. (5.66)
Furthermore, we say that a matrix is (easily) permutation-detectable if there exist permutation
matrices PandQsuch that PBQ is (easily) detectable.
Intuitively, this means that one can effectively replace an easily detectable matrix Bwith a
single neuron : instead of specifying the matrix explicitly, one can simply detect if an entry BijofB
is 1 by passing eijthrough the neuron and seeing if it fires. This reduces the space complexity from
D2to 2D. Similarly, one can replace a detectable matrix with twoneurons. As shown in Fact 5.2.1,
the required conjunction of the two resulting neurons can then easily be performed by a third (small)
neuron, meaning that a detectable matrix is effectively represented by a two-layer MLP.
An example of easily detectable matrices are the so-called northwestern matrices.
Definition 5.2.8: Northwestern matrix
A matrix BPBDˆDisnorthwestern if there exists a vector αwith|α|“DandDěα1ě
...ěαDě0 such that
Bij“1ðñjďαi. (5.67)
Intuitively, northwestern matrices contain all their ones contiguously in their upper left (northwest)
corner. An example of a northwestern matrix for α“`
2 1 1˘
is
B“¨
˝1 1 0
1 0 0
1 0 0˛
‚. (5.68)
Lemma 5.2.5
Northwestern matrices are easily detectable.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 177
Proof. Define
wdef“`
α|D ... 1˘
andb“´D. It is easy to see that for any eijwhere Bij“1, it holds that
xeij,wy“αi`pD´j`1qěj`D´j`1“D`1
ùñHpxeij,wy`bq“Hpxeij,wy´Dq“1.
On the other hand, for Bij“0, we have
xeij,wy“αi`pD´j`1qăj`D´j`1“D
ùñHpxeij,wy`bq“Hpxeij,wy´Dq“0.
■
A more general useful class of detectable matrices are line matrices (Dewdney, 1977).
Definition 5.2.9: Line matrix
A binary matrix BPBDˆDis aline matrix if any of the following conditions hold:
1.AllB’s ones lie either in the same row ( Bis arow matrix ) or in the same column ( B
is acolumn matrix ).
2.Bis atransversal , i.e., a matrix in which there is at most one 1 in any column and row.
Lemma 5.2.6
Row and column matrices are easily permutation-detectable.
Proof. Leti,NPZDandBbe a row matrix with Bijn“1 fornPZN, i.e., a row matrix with
all its ones in the ithrow. Define PPBDˆDasP1i“1 and 0 elsewhere and QPBDˆDwith
Qjnn“1 and 0 elsewhere. Then, PBQ contains all its 1 in its northwestern corner (contiguously
in the first row) and is thus easily detectable. Let wdef“`
α|D ... 1˘
,b“Dbe the neuron
weights from Lemma 5.2.5. Define w1def“`
PJα|QpD ... 1q˘
,b1“D. It is easy to see that
this “rearranges” the components of the neuron recognizing the northwestern matrix PBQ to make
them recognize the original matrix, meaning that the neuron defined by w1andb1recognizes the
line matrix. The proof for a column matrix is analogous. ■
Lemma 5.2.7
Transversals are permutation-detectable.
Proof. The core idea of this proof is that every transversal can be permuted into a diagonal matrix,
which can be written as a Hadamard product of a lower-triangular and an upper-triangular matrix.
LetBbe a transversal. Pre-multiplying Bwith its transpose Pdef“BJresults in a diagonal matrix.
It is easy to see that PBcan be written as a Hadamard product H1bH2of a lower-triangular matrix
H1and an upper-triangular matrix H2. Both are easily permutation detectable. A conjunction of
the neurons detecting H1andH2(again, performed by another neuron) detects the original matrix
B. In the following, we will refer to H1andH2as the factors of the transversal. ■
178 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
Crucially, any binary matrix BPBDˆDcan be decomposed into a set of line matrices Bwhose
disjunction is B:Ž
MPBM“B. It is easy to see that Bij“1 if and only if there exists MPB
such that Mij“1. This means that non-zero entries of anyBPBDˆDdecomposed into the set of
line matrices Bcan be detected using an MLP in two steps:
1.Detect the non-zero entries of the individual line matrices from the decomposition B(which
are, as shown above, detectable).
2.Take a disjunction of the detections of the individual line matrices to result in the activation
of the original matrix.
The disjunction can again be performed by applying another 2-layer MLP to the activations of the
line matrices. An important consideration in both Dewdney’s as well as Indyk’s construction later
will be how large Bhas to be.
Using matrix decomposition and detection for simulating the transition function. We
now describe how Dewdney’s construction uses matrix detection based on the decomposition of
matrices into line matrices to simulate an FSA using an HRNN. From a high level, the update
steps of the HRNN will, just like in Minsky’s construction, simulate the transition function of the
simulated FSA. However, in contrast to the Minsky construction, in which each transition step in
the FSA was implemented by a single application of the Elman update rule, here, a single transition
in the FSA will be implemented using multiple applications of the Elman update rule, the end result
of which is the activation of the two-hot representation of the appropriate next state. Nonetheless,
there are, abstractly, two sub-steps of the update step, analogous to the Minsky construction (cf.
Fig. 5.6):
1.Detect the activations of all possible next states, considering any possible input symbol
(performed by the term Uhtin Minsky’s construction).
2.Filter the activations of the next states by choosing only the one transitioned into by a
yt-transition (performed by conjoining with the term VJytKin Minsky’s construction).
The novelty of Dewdney’s construction comes in the first sub-step: How can the Elman update
step be used to activate the two-hot representation of qt’s out-neighborhood? As alluded to, this
relies on the pre-computed predecessor matrices Predpq;yq(cf. Definition 5.2.4). The predecessor
matrices of individual states are compressed (disjoined) into component-activating matrices, the
representation matrices of the predecessors of specific setsof states (cf. Definition 5.2.5), defined
through the function ϕ2in the following sense.
Definition 5.2.10: Component-activating matrix
Acomponent-activating matrix is the representation matrix Bj,y,kdef“WQPredpΦk,j;yq
for somekPZrandjPZ2.
Intuitively, the component-activating matrix Bj,y,kis the result of the disjunction of the matrix
representations of all y-predecessors qof all states q1whosejthcomponent of the vector ϕ2pq1q
equalsk. This results in 2 |Σ|smatrices. They can be pre-computed and naturally depend on the
transition function δ. The name component-activating matrix is inspired by the fact that each of the
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 179
matrices “controls” the activation of one of the 2 |Σ|sneurons in a specific sub-vector of the HRNN
hidden state. That is, each component-activating matrix controls a particular dimension, indexed
by the tuplepj,y,kqforjPB,yPΣ,kPZs, in the data sub-vector of the HRNN hidden state.
As we will see shortly, they contain all the information required for simulating Awith a HRNN.
To define the transition function of the HRNN simulating A, all 2|Σ|scomponent-activating
matrices are decomposed into permutation-detectable line matrices (cf. Definition 5.2.9) whose
activations are combined (disjoined) into the activations of individual component-activating matrices.
Analogously to above, we will denote the sets of line matrices decomposing the component-activating
matrices as Bj,y,k, i.e., Bj,y,k“Ž
MPBj,y,kM. The dimensions of the hidden state corresponding to
the activations of the line matrices before they are combined into the activations of the component-
activating matrices form the processing sub-vector of the HRNN hidden state since they are
required in the pre-processing steps of the update step to determine the activation of the actual
hidden state. This is schematically drawn in Fig. 5.12a.
For any component-activating matrix Bdecomposed into the set of line matrices B, we know
by Lemmas 5.2.6 and 5.2.7 that all MPBare detectable by a single-layer MLP. By adding an
additional layer to the MLP, we can disjoin the detections of MPBinto the detection of B. More
abstractly, this MLP, therefore, detects the activation of oneof the 2|Q|scells of the data sub-vector
of the HRNN hidden state—all of them together then form the two-hot encoding of all possible next
states of the FSA (before taking into account the input symbol). Designing 2 |Q|ssuch single-values
MLPs, therefore, results in an MLP activating the two-hot representations of all possible next states
of the simulated FSA. Conjoining these activations with the input symbol, analogously to how this
is done in the Minsky construction, results in the activation of the two-hot representation of only
the actual next state of the simulated FSA. This is illustrated in Fig. 5.12b.
High-level overview of simulating a transition. In summary, after decomposing all the
component-activating matrices into the sets Bj,y,k, the detection of all candidate next states (before
considering the input symbol) in the update step of HRNN is composed of the following sub-steps.
1.Compute the activations of the two factors of all the transversals in Bj,y,k for allj,y,k
(Lemma 5.2.7).
2.Conjoin the activations of the two factors into the activations of the transversals (Lemma 5.2.7).
3.Compute the activations of the column and row matrices in Bj,y,kfor allj,y,k (Lemma 5.2.6).
4.Disjoin of the activations of all the line matrices (transversals, row, and column matrices) in
Bj,y,kfor allx,y,k to compute the activations of all 2 |Σ|scomponent-activatimg matrices.
This results in the activation of the two-hot representations of all possible next states (i.e., the entire
out-neighborhood of qt). In the last sub-step of the HRNN update step, these are conjoined with
the representation of the current input symbol. This step is very similar to the analogous stage in
Minsky’s construction, with the difference that here, the non-zero entries of the vector Vhtmust
cover the two-hot representations of the states with an incoming yt-transition. This conjunction
then ensures that among all the states in the out-neighborhood of qt, only the one reached by taking
theyt-transition will be encoded in ht`1. The construction just described can be summarized by
the following lemma.14
14To formally prove it is correct, we would have to follow a similar set of steps to how the correctness of Minsky’s
construction (Lemma 5.2.3) was proved. We omit this for conciseness.
180 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
… … … Data  sub-vector Pr ocessing  sub-vector … AND 
OR 
… 
(a) High-level overview of Dewdney’s construction. The highlighted orange neuron in the representation
of the state from the data sub-vector corresponds to the activation of one of the components of the red
states (which have in common that their 0thcomponent of ϕ2pqqis the same). The matrix corresponding
to the disjunction of the representations of their y-predecessors (blue states) is decomposed into two line
matrices—a transversal and a column matrix. The non-zero elements of the former can be detected by a
conjunction of two neurons while the non-zero elements of the latter can be detected directly by a single
neuron. Those activations are then disjoined to result in the activation in the orange neuron. The purple
neurons in the processing sub-vector are composed of the neurons in the networks implementing the detection
of line matrices and their conjunctions and disjunctions (also shown in purple).
qq1
q2a
bqq1
q2a
bqq1
q2a
b
ˆ
vpqq
p˙ˆ
vpqq
p1˙ˆ
vptq1,q2uq
p2˙ˆ
vpq1q
p3˙Phase 1 Phase 2 Phase 3 Phase 4
(b) A high-level illustration of how the transition function of the FSA is implemented in Dewdney’s
construction on an example of an FSA fragment, where the simulated automaton is initially in the
stateqand reads the symbol a, transitioning to q1. The components whose changes are relevant
at a given step are highlighted. Starting in the state q, which is stored in the data sub-vector
vpqq, in the first sub-step, the processing bits of the appropriate line matrices are activated ( p1).
Next, the activated line matrices are used to activate the representations of all the states in the
out-neighborhood of qin the data sub-vector ( v`␣
q1,q2(˘
). Lastly, these representations are
conjoined with the states reachable by the symbol a, resulting in the representation of the state q
in the data sub-vector ( vpqq).
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 181
Lemma 5.2.8
LetA“pΣ,Q,I,F,δqbe a deterministic FSA. Then, Dewdney’s construction results in a
HRNN correctly simulating A’s transition function, i.e, sphtq“qtfor allt.
This shows that Dewdeny’s construction correctly encodes the FSA in a HRNN. However, its
space efficiency remains to be determined. As mentioned above, working with two-hot representations
of the states means that the data sub-vector is of size O´
|Σ|a
|Q|¯
. However, the construction
also requires a number of processing dimensions in the processing sub-vector. To understand the
full complexity of the construction, we have to determine the maximal number of processing bits
in the HRNN. The first step to the answer is contained in the following lemma, which describes
the number of line matrices required to cover an arbitrary binary matrix. It lies in the core of the
efficiency of Dewdney’s construction.
Lemma 5.2.9
LetBPBDˆDwithN2elements equalling 1. Then, there exists a decomposition BofBinto
at most 2Nline matrices such thatŽ
MPBM“B.
Proof. Based on Dewdney (1977). Define the sequence of transversals T1,T2,...where Tiis the
transversal containing the maximum number of ones in the matrix Bidef“B´Ži´1
j“1Bj. The
transversal containing the maximal number of ones can be found using the maximum matching
algorithm. Continue this sequence until there are no more ones in Bi. The number of ones in the
matrices Bi,∥Bi∥1, forms a (weakly) decreasing sequence.
If there are at most 2 Ntransversals in the sequence, the lemma holds. Otherwise, we compare
the functions fpiqdef“∥Ti∥1andgpiqdef“2N´i.
•Iffpiqągpiqfor alli“1,...,N , thenřN
i“1fpiq“řN
i“1∥Ti∥1ąřN
i“12N´i“2N2´
1
2NpN`1qěN2. However, the transversals in the decomponsition cannot contain more ones
than the original matrix.
•We conclude that for some iďN,fpiqďgpiq. Leti0be the first such index in 1 ,...,N and
L1def“tT1,...,Tku. Since the maximum number of independent ones (in the sense that at
most one appears in a single row/column) in Bi0´1is∥Ti0∥1ď2N´i0(those are chosen by
the maximum transversal Ti0). By K¨ onig’s theorem (Sz´ arnyas, 2020), there is a set of at most
2N´i0column or row matrices L2def“tL1,...Lkuwithkď2N´i0which cover Bi0´1.15
Therefore, Ldef“L1YL2constitutes a valid cover of BwithďN`2N´i0“OpNqmatrices.
■
We will denote the number of matrices in the line decomposition of a matrix Bconstructed
by the greedy procedure from Lemma 5.2.9 as LpBq. Connecting this lemma to Dewdney’s
construction, this shows that the number of neurons required to detect the activation of a single
setPredpk;yqgrows asymptotically as the square root of the number of ones in the representation
15Intuitively, since all ones are contained within ď2N´i0rows or columns, they can be simply covered by matrices
containing those.
182 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
matrix WQPredpk;yq—this is how many line matrices the matrix will decompose into. The size of
each neuron is 2 |Σ|s.
This allows us to show how many neurons the entire HRNN simulating Ahas. Since we know that
the data sub-vector will always have exactly 2 |Σ|scells, we characterize the number of processing
cells in the following lemma.
Lemma 5.2.10
LetA“pΣ,Q,I,F,δqbe a deterministic FSA. Then, Dewdney’s construction results in a
HRNN with a hidden state of size O´
|Σ||Q|3
4¯
.
Proof. The number of cells in the entire processing sub-vector is simply the sum of the processing
neurons of all the data components. In the worst case, a single component-activating matrix B
requires 2LpBq`1 neurons (2 for each transversal in the decomposition of Band an additional
one for their disjunction). Therefore, enumerating the set of matrices tBj,y,k|jPZ2,yPΣ,kPZsu
withBnforn“1,..., 2|Σ|s, the number of neurons required by all component-activating matrices
is bounded as follows.
2|Σ|sÿ
n“12LpBnq`1ď2|Σ|sÿ
n“12´
2rb
∥Bn∥1s¯
`1def“2|Σ|sÿ
n“14mn`1 (5.69)
Since the matrices Bncontain one non-zero entry for each state-symbol pair, it holds that
2|Σ|sÿ
n“1∥Bn∥1ď2|Σ|sÿ
n“1m2
n“|Σ||Q| (5.70)
Pretending that mncan take real values, the value of Eq. (5.69) is maximized under the constraint
from Eq. (5.70) when all mnare equal with mn“?
2s. This means that
2|Σ|sÿ
n“14mn`1ď2|Σ|sÿ
n“14?
2s`1“8|Σ|s?
2s`1“O´
|Σ||Q|3
4¯
, (5.71)
finishing the proof. ■
All results stated in this section can be summarized in the following theorem.
Theorem 5.2.2: Dewdney (1977)
LetA“pΣ,Q,I,F,δqbe a deterministic FSA. Then, there exists a HRNN of size O´
|Σ||Q|3
4¯
correctly simulating A.
Indyk’s Construction
§5.2.2 describes a construction of an HRNN of size O´
|Σ||Q|3
4¯
simulating an FSA. While this
improves the space efficiency compared to Minsky’s construction, it is not asymptotically optimal.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 183
Indyk (1995) proved that a HRNN simulating an FSA A“pΣ,Q,I,F,δqover a binary alphabet
Σ“Brequires at least Ω´a
|Q|¯
hidden dimensions. He also provided a construction that achieves
this lower bound. This construction is conceptually very similar to Dewdney’s in that it works by
activating neurons corresponding to some form of compressed predecessor matrices (component-
activating matrices) and then selecting the transition which matches the input symbol. Again, it
additively covers these matrices with components that are easy to detect, similar to how Dewdney’s
construction uses line matrices. However, Indyk’s construction defines component-activating matrices
based on different sets of states and covers them with a different decomposition—these are the two
crucial differences allowing the construction to achieve the optimal lower bound.
We first define the component-activating matrices and their role in updating the hidden state of
the HRNN. In Indyk’s construction, the component-activating matrices are based on four-hot rather
than two-hot encodings of states.
Definition 5.2.11: Four-hot representation of a state
LetA“pΣ,Q,I,F,δqbe an FSA, rdef“r|Q|1
4s, andπa permutation of Q“r|Q|s.aWe define
thefour-hot representation ofqPQas
ϕ4pqq“pℓ1,ℓ2,ℓ3,ℓ4q (5.72)
where
ℓj“πpqq
rj´1modr. (5.73)
We denote the inverse of ϕ4withϕ´1
4and further define for kPZr
ϕ´1
4pk,¨,¨,¨qdef“tqPQ|ϕ4pqq1“ku (5.74)
andϕ´1
4p¨,k,¨,¨q,ϕ´1
4p¨,¨,k,¨q, andϕ´1
4p¨,¨,¨,kqanalogously.
aThe exact form of πwill be important later. For now, one can think of πas the identity function.
We will denote ϕ´1
4p...,k,...qwithkinjthposition (with jPZ4) asΦk,j. Despite using the
four-hot representations, Indyk’s construction still requires the two-hot representations based on ϕ2
as before. In this case, however, they again depend on the chosen permutation π. This allows us to
define the component-activating matrices as follows.
Definition 5.2.12: Component-activating matrix
Acomponent-activating matrix in Indyk’s construction is the representation matrix
WQPredpΦk,j;yqfor somekPZr,jPZ4, andyPΣ.
For efficient detection, the component-activating matrices are covered by so-called non-decreasing
matrices.
184 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
Definition 5.2.13: Non-decreasing matrix
We say that BPBDˆDisnon-decreasing if there exists a non-decreasing (partial) function
f:ZDÑZD(from columns to rows) such that
Bij“1ðñfpjq“i (5.75)
and, iffis defined for some jPZD, it is also defined for all j1ěj.
Example 5.2.4: Non-decreasing matrices
An example of a non-decreasing matrix is
B“¨
˚˚˝0 1 0 0
0 0 0 0
0 0 1 1
0 0 0 0˛
‹‹‚. (5.76)
The function fdefining the non-decreasing matrix Bisf“ˆ
0 1 2 3
H0 1 1˙
, whereHdenotes
that the function is not defined.
Again, clearly, any matrix BPBDˆDcan be (non-uniquely) decomposed into at most D
non-decreasing matrices. Moreover, non-decreasing matrices are detectable.
Lemma 5.2.11
Non-decreasing matrices are detectable.
Proof. LetBPBDˆDbe a non-decreasing matrix defined by the partial function f. Divide the
domain offinto the set of intervals in which the function is constant, with Ipjqdenoting the interval
ofjPZr2forjsuch thatfpjqis defined. Then, it is easy to see that Bij“1ðñi“fpjq,
meaning that by defining the parameters wandbas
wfpjqdef“r2´Ipjq (5.77)
wr2`jdef“Ipjq (5.78)
bdef“´r2(5.79)
and other elements as 0, we get that
Bij“1ðñi“fpjq ðñwi`wj`b“0. (5.80)
Compared to earlier, where component-activating matrices were detected by testing an inequality ,
detecting a non-decreasing matrix requires testing an equality . Since all terms in the equality are
integers, testing the equality can be performed with the Heaviside activation function by conjoining
two neurons; one testing the inequality wi`wj`b´1ă0 and another one testing the inequality
wi`wj`b`1ą0. Both can individually be performed by a single neuron and then conjoined by
an additional one. ■
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 185
With this, the high-level idea of Indyk’s construction is outlined in Fig. 5.13. After constructing
the component-activating matrices based on ϕ4and decomposing them into non-decreasing matrices,
the rest of Indyk’s construction is very similar to Dewdney’s construction, although the full update
step of the HRNN requires some additional processing. To test the equality needed to detect
non-decreasing matrices in the decomposition, Eq. (5.80), the four-hot representations are first
converted into two-hot ones. This can be done by a simple conjunction of the first two and the last
two components of the four-hot representation. Then, the activations of the non-decreasing matrices
can be computed and disjoined into the representations of the component-activating matrices. These
form the 4|Σ|rcomponents of the data sub-vector of the HRNN hidden state. They contain the
activations of all possible next states, i.e., the out-neighborhood of the current state of A. These are
then conjoined with the representation of the current input symbol in the same way as in Dewdney’s
construction but adapted to the four-hot representations of the states. The process is thus very
similar to the phases of Dewdeney’s construction illustrated in Fig. 5.12b.
Indyk’s construction can be summarized by the following lemma.16
Lemma 5.2.12
LetA“pΣ,Q,I,F,δqbe a deterministic FSA. Then, Indyk’s construction results in a HRNN
correctly simulating A’s transition function, i.e, sphtq“qtfor allt.
The only remaining thing to show is that Indyk’s construction achieves the theoretically optimal
lower bound on the size of the HRNN simulating a deterministic FSA. All previous steps of the
construction were valid no matter the chosen permutation π. The permutation, however, matters
for space efficiency: intuitively, it determines how efficiently one can decompose the resulting
component-activating matrices (which depend on the permutation) into non-decreasing matrices in
the sense of how many non-decreasing matrices are required to cover it. Indyk, therefore, proved
that there always exists, with non-zero probability, a permutation in which the decomposition across
all states is efficient enough to achieve the minimum number of neurons required. This is formalized
by the following lemma, whose proof can be found in Indyk (1995, Lemma 6).
Lemma 5.2.13
LetA“pΣ,Q,I,F,δqbe a deterministic FSA. There exists a permutation of Qsuch that
Indyk’s construction results in a HRNN of size O´
|Σ|a
|Q|¯
.
This concludes our presentation of Indyk’s construction. All results stated in this section can be
summarized by the following theorem.
Theorem 5.2.3: Indyk (1995)
LetA“pΣ,Q,I,F,δqbe a deterministic FSA. There exists a HRNN of size O´
|Σ|a
|Q|¯
correctly simulating A.
16Again, to formally prove it is correct, we would have to follow a similar set of steps to how the correctness of
Minsky’s construction (Lemma 5.2.3) was proved. We omit this for conciseness.
186 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
… … Data  sub-vector Pr ocessing  sub-vector … AND 
OR 
… 
 … … … 
… … 
AND 
Figure 5.13: High-level overview of Indyk’s construction. The highlighted orange neuron in the
representation of the state from the data sub-vector corresponds to the activation of one of the
components of the red states (which have in common that their 0thcomponent of ϕ4pqqis the same).
The matrix corresponding to the disjunction of the representations of their y-predecessors (blue states)
is decomposed into two non-decreasing matrices. The non-zero elements of both can be detected
by a conjunction of two neurons; here, f1“ˆ
0 1 2 3
H0 0 0˙
andf2“ˆ
0 1 2 3
H H 1 2˙
, meaning
thatw1“`
3 0 0 0 |0 1 1 1˘
,w2“`
0 3 2 0 |0 0 1 2˘
, andb1“b2“4.
Those activations are then disjoined to result in the activation in the orange neuron. The purple
neurons in the processing sub-vector are composed of the neurons in the networks implementing the
detection of line matrices and their conjunctions and disjunctions (also shown in purple). Note that
even if the second matrix were not non-decreasing in itself (i.e., the columns of the two ones would
be flipped), one could still transform it into a non-decreasing matrix by permuting the columns and
permuting the corresponding neurons.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 187
5.2.3 Lower Bound in the Probabilistic Setting
We now ask whether the same lower bound can also be achieved when simulating dPFSAs. We find
that the answer is negative: dPFSAs may require an HRNN LMs of size Ω p|Σ||Q|qto faithfully
represent their probability distribution. Since the transition function of the underlying FSA can be
simulated with more efficient constructions, the bottleneck comes from defining the same probability
distribution. Indeed, as the proof of the following theorem shows, the issue intuitively arises in the
fact that, unlike in an HRNN LM, the local probability distributions of the different states in a
PFSA are completely arbitrary, whereas they are defined by shared parameters (the output matrix
E) in an HRNN LM.
Theorem 5.2.4: A lower bound on the size of the RNN simulating a PFSA
LetA“pΣ,Q,δ,λ,ρqbe a minimal dPFSA and pΣ,D,U,V,E,bh,h0qan HRNN LM defining
the same LM. Then, Dmust scale linearly with |Q|.
Proof. Without loss of generality, we work with R-valued hidden states. Let Abe a minimal
deterministic PFSA and R“ pΣ,D,U,V,E,bh,h0qa HRNN with pLNpyq “Apyqfor every
yPΣ˚. Let yăTPΣ˚andyďTdef“yăTyfor someyPΣ. Defineppyqdef“ś|y|
t“1pSMpyt|yătq. It
is easy to see that ppyăTyTq“ppyăTqpSMpyt|yăTq. The conditional distribution pSMp¨|yăTq
are proportional to the values in EhT´1. By definition of the deterministic PFSA, there are |Q|
such conditional distributions. Moreover, these distributions (represented by vectors P∆|Σ|´1) can
generally be linearly independent . This means that for any q, the probability distribution of the
outgoing transitions can not be expressed as a linear combination of the probability distributions of
other states. To express the probability vectors for all states, the columns of the output matrix E,
therefore, have to span R|Q|, implying that Emust have at least |Q|columns. This means that the
total space complexity (and thus the size of the HRNN representing the same distribution as A) is
Ωp|Q|q. ■
Asymptotic Bounds in |Σ|Since each of the input symbols can be encoded in log|Σ|bits, one
could expect that the linear factor in the size of the alphabet from the constructions above could be
reduced to Oplog|Σ|q. However, we again find that such reduction is in general not possible—the set
of FSAs presented next is an example of a family that requires an HRNN whose size scales linearly
with|Σ|to be simulated correctly. We also provide a sketch of the proof of why a compression in
|Σ|is not possible.
LetAN“pΣN,t0,1u,t0u,t1u,δNqbe an FSA over the alphabet Σ N“ty1,...,yNusuch that
δN“!
0y1ÝÑ1)
Y!
0ynÝÑ2|n“2,...N)
(see Fig. 5.14).
Clearly, to be able to correctly represent all local distributions of the dPFSA, the HRNN LM
must contain a representation of each possible state of the dPFSA in a unique hidden state. On the
other hand, the only way that the HRNN can take into account the information about the current
stateqtof the simulated FSA Ais through the hidden state ht. The hidden state, in turn, only
interacts with the recurrence matrix U, which does not have access to the current input symbol
yt`1. The only interaction between the current state and the input symbol is thus through the
addition in Uht`VJyt`1K. This means that, no matter how the information about qtis encoded in
ht, in order to be able to take into account all possible transitions stemming in qt(before taking
188 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
01
2y2,...,y
Ny1
Figure 5.14: The FSA AN.
into account yt`1),Uhtmust activate allpossible next states, i.e., the entire out-neighborhood of
qt. On the other hand, since VJyt`1Kdoes not have precise information about qt, it must activate
all states which can be entered with an yt`1-transition, just like in Minsky’s construction.
In Minsky’s construction, the recognition of the correct next state was done by keeping a separate
entry (one-dimensional sub-vector) for each possible pair qt`1,yt`1. However, when working with
compressed representations of states (e.g., in logarithmic space), a single common sub-vector of
sizeă|Σ|(e.g., log|Σ|) has to be used for all possible symbols yPΣ. Nonetheless, the interaction
between UhtandVJyt`1Kmust then ensure that only the correct state qt`1is activated. For
example, in Minsky’s construction, this was done by simply taking the conjunction between the
entries corresponding to q,yinUhtand the entries corresponding to q1,y1inVJy1K, which were
all represented in individual entries of the vectors. On the other hand, in the case of the log
encoding, this could intuitively be done by trying to match the log|Σ|ones in the representation
pppyq|1´ppyqq, where ppyqrepresent the binary encoding of y. If the log|Σ|ones match (which
is checked simply as it would result in a large enough sum in the corresponding entry of the
matrix-vector product), the correct transition could be chosen (to perform the conjunction from
Fact 5.2.1 correctly, the bias would simply be set to log|Σ|´1). However, an issue arises as soon as
multiple dense representations of symbols in VJyKhave to be activated against the same sub-vector
inUht—the only way this can be achieved is if the sub-vector in Uhtcontains the disjunction of the
representations of all the symbols which should be activated with it. If this sets too many entries in
Uhtto one, this can result in “false positives” . This is explained in more detail for the dPFSAs in
Fig. 5.14 next.
Letrnrepresent any dense encoding of ynin the alphabet of AN(e.g., in the logarithmic case,
that would be pppnq|1´ppnqq). Going from the intuition outlined above, any HRNN simulating
AN, the vector Uh0must, among other things, contain a sub-vector corresponding to the states
1 and 2. The sub-vector corresponding to the state 2 must activate (through the interaction in
the Heaviside function) against any ynforn“2,...,N inAN. This means it has to match
all representations rnfor alln“2,...,N . The only way this can be done is if the pattern for
recognizing state 2 being entered with any ynforn“2,...,N is of the form r“ŽN
n“2rn. However,
for sufficiently large N,r“ŽN
n“2rnwill be a vector of all ones—including all entries active in r1.
This means that anyencoding of a symbol will be activated against it—among others, y1. Upon
readingy1in state 1, the network will therefore not be able to deterministically activate only the
sub-vector corresponding to the correct state 1. This means that the linear-size encoding of the
symbols is, in general, optimal for representing dPFSAs with HRNN LMs. This discussion implies
the following theorem.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 189
Theorem 5.2.5: A lower bound on the size of the RNN simulating a PFSA
LetA“pΣ,Q,δ,λ,ρqbe a minimal dPFSA and pΣ,D,U,V,E,bh,h0qan HRNN LM defining
the same LM. Then, Dmust scale linearly with |Σ|.
Based on the challenges encountered in the example above, we can devise a simple sufficient
condition for a logarithmic compression w.r.t. |Σ|to be possible: namely, that for any pair of states
q,q1PQ, there is at most a single transition leading from qtoq1. This intuitive characterization
can be formalized by a property we call log |Σ|-separability.
Definition 5.2.14: log|Σ|-separable finite-state automaton
An FSA A“pΣ,Q,I,F,δqislog|Σ|-separable if it is deterministic and, for any pair q,q1PQ,
there is at most one symbol yPΣ such that qyÝ Ñq1Pδ.
log|Σ|-separability is a relatively restrictive condition. To amend that, we introduce a simple
procedure which, at the expense of enlarging the state space by a factor of Σ, transforms a general
deterministic (unweighted) FSA into a log|Σ|-separable one. We call this log|Σ|-separation .
Intuitively, it augments the state space by introducing a new state pq,yqfor every outgoing transition
qyÝ Ñq1of every state qPQ, such thatpq,yqsimulates the only state the original state qwould
transition to upon reading y. Due to the determinism of the original FSA, this results in a
log|Σ|-separable FSA with at most |Q||Σ|states.
While the increase of the state space might seem like a step backward, recall that using Indyk’s
construction, we can construct an HRNN simulating an FSA whose size scales with the square
root of the number of states. And, since the resulting FSA is log|Σ|-separable, we can reduce
the space complexity with respect to Σ to log|Σ|. This is summarized in the following theorem,
which characterizes how compactly general deterministic FSAs can be encoded by HRNNs. To our
knowledge, this is the tightest bound on simulating general unweighted deterministic FSAs with
HRNNs.
Theorem 5.2.6: Efficiently simulating general FSAs
LetA“pΣ,Q,I,F,δqbe a minimal FSA recognizing the language L. Then, there exists an
HRNN R“pΣ,D,U,V,E,bh,h0qacceptingLwithD“O´
log|Σ|a
|Σ||Q|¯
.
The full log|Σ|-separation procedure is presented in Algorithm 2. It follows the intuition of
creating a separate “target” for each transition qyÝ Ñq1for every state qPQ. To keep the resulting
FSA deterministic, a new, artificial, initial state with no incoming transitions is added and is
connected with the augmented with the out-neighborhood of the original initial state.
The following simple lemmata show the formal correctness of the procedure and show that it
results in a log |Σ|-separable FSA, which we need for compression in the size of the alphabet.
Lemma 5.2.14
For anyyPΣ,pq,yqy1
Ý Ñpq1,y1qPδ1if and only if qy1
Ý Ñq1Pδ.
190 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
Algorithm 2
1.def Separate (A“pΣ,Q,I,F,δq):
2.A1ÐpΣ,Q1“QˆΣYtqι1u,δ1“∅,I1“tqι1u,F1“∅q
3.ŹConnect the out-neighborhood of the original initial state qιwith the new, aritificial, initial state.
4.foryPΣ :
5. forqιy1
Ý Ñq1Pδ:
6. addqι1yÝ Ñpq1,y1qtoδ1
7.forqPQ,yPΣ :
8. forqy1
Ý Ñq1Pδ:
9. addpq,yqy1
Ý Ñpq1,y1qtoδ1
10.ŹAdd all state-symbol pairs with a state from the original set of final states to the new set of final states.
11.forqφPF,yPΣ :
12. addpqφ,yqtoF1
13.ifqιPI:ŹCorner case: if the original initial state qιis an initial state, make the artificial initial state
qι1final.
14. addqι1toF1
15.return A1
Proof. Ensured by the loop on Line 3. ■
Lemma 5.2.15
log|Σ|-separation results in an equivalent FSA.
Proof. We have to show that, for any yPΣ˚,yleads to a final state in Aif and only if yleads to a
final state in A1. For the string of length 0, this is clear by Lines 13 and 14. For strings of length
ě1, it follows from Lemma 5.2.14 that yleads to a state qinAif and only if DyPΣ such that y
leads topq,yqinA1. From Lines 11 and 12, pq,yqPF1if and only if qPF, finishing the proof. ■
Lemma 5.2.16
log|Σ|-separation results in a log |Σ|-separable FSA.
Proof. Since the state pq1,y1qis the only state in Q1transitioned to from pq,yqafter reading y1(for
anyyPΣ), it is easy to see that A1is indeed log|Σ|-separable. ■
Discussion and the practical applicability of these result. This section showed that Heaviside-
activated RNNs are equivalent to WFSAs. This might come as a bit of a surprise considering that
we introduced RNNs with the goal of overcoming some limitations of exactly those models, e.g., the
finite context length. However, note that to arrive at this result, we considerately restricted the
form of a recurrent neural network. While on the one hand restriction to the Heaviside activation
function means that all the RNNs we considered in this section can be implemented and represented
in a computer, the RNN sequence models that we usually deal with are much more complex than
this analysis allowed for. Furthermore, note that the RNNs in practice do not learn sparse hidden
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 191
states of the form considered in the construction in the proof of Lemma 5.2.2—indeed, networks
with Heaviside activation functions are not trainable with methods discussed in §3.2 as the gradient
on the entire parameter space would be either 0or undefined and in this sense, the trained networks
would never have such hidden state dynamics. The dynamics of RNNs in practice result in dense
hidden states, i.e., states in which many dimensions are non-zero. Nonetheless, keep in mind that
theoretically, due to the finite-precision nature of our computers, all models we ever consider will
be at most finite-state—the differentiating factor between them will be how appropriately to the
task they are able to learn the topology (transitions) of the finite-state automaton they represent
and how efficiently they are able to learn it. Lastly, note that, by considering special classes of
(sub-)regular languages, one can arrive at neural representations far smaller than those described by
the constructions above (Hewitt et al., 2020; Svete et al., 2024).
192 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
5.2.4 Turing Completeness of Recurrent Neural Networks
We now turn to the (purely theoretical) treatment of the expressive capacity of recurrent neural
networks in which we take the liberty of making somewhat unrealistic assumptions. Specifically, in
practice, RNNs usually have the following properties:
•The weights and intermediate computations are done with finite floating point precision;
•An RNN always operates in real-time , meaning that it performs a constant number of operations
before consuming/outputting a symbol.
Under these assumptions, we saw that RNNs with Heaviside activations in a practical setting lie at the
bottom of the weighted Chomsky hierarchy, being able to only recognize regular languages. However,
if we relax these two assumptions, allowing for arbitrary precision and unbounded computation time
between symbols, RNNs jump directly to the top of the hierarchy: they become Turing complete.
We start by introducing the saturated sigmoid, one of the building blocks we will use to show
this.
Definition 5.2.15: Saturated Sigmoid function
Thesaturated sigmoid is defined as
σpxq“$
’&
’%0ifxď0
xif0ăxď1
1ifxą1. (5.81)
Intuitively, the saturated sigmoid clips all negative values to 0, all values larger than 1 to 1, and
leaves the elements of r0,1sintact. The graph of this function is shown in Fig. 5.15.
σpxq
x 0 11
Figure 5.15: The saturated sigmoid.
The central result of this subsection is then summarized in the following theorem, summarized
from Nowak et al. (2023).
Theorem 5.2.7: Saturated Sigmoid Elman RNNs are Turing complete
Elman recurrent neural network sequence models with the saturated sigmoid activation
functions are Turing complete.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 193
By the end of this subsection, we will have proven this result by showing that Saturated Sigmoid
Elman RNNs can encode two-stack pushdown automata which are computationally equivalent to
Turing machines (cf. Definitions 4.2.50 and 4.2.51). We start with a simpler construction: building
on our placement of RNNs on at least the regular rung of the ladder of formal language complexity
(cf. Lemma 5.2.2), we take one step up the ladder and show that RNNs can simulate a single-stack
pushdown automaton (cf. Definition 4.2.31). This will help us gain the intuition behind how
RNNs can use infinite precision arithmetic to simulate a stack. We will then simply generalize this
construction to the two-stack case.
Let us begin by considering the problem of representing a stack—an arbitrarily long sequence
of symbols, accessible in a last-in-first-out (LIFO) fashion—in a vector of constants size, e.g., the
hidden state of a recurrent neural network. For simplicity, but without the loss of generality, assume
that we are working with a simple two-letter stack alphabet Γ “t0,1u. Any stack sequence γwill
be a member of Γ˚, i.e., a string of 0’s and 1’s. If we think of the stack symbols asnumbers for a
moment, there is a natural correspondence between the possible stack configurations and numbers
expressed in base 2. By convention, we will represent a string of stack symbols γwith numbers
after the decimal point, rather than as integers. Assuming infinite precision, we can therefore
simply represent each stack configuration as a single number (of course, the stack alphabet does not
have to be exactly Γ “t0,1u—we can always map symbols from any alphabet into their numeric
representations in some base large enough to allow for the entire alphabet). Notice that in this
case, pushing or popping from the stack can be performed by division and multiplication of the
value representing the stack—if we want to push a value xP t0,1u, we can divide the current
representation (by 2) and append xto the right side of the new representation and if we want to
pop any value, we simply have to multiply the current representation by 2. This also gives us an
idea of how to represent a stack in the hidden state of an RNN: the entire stack sequence will simply
be represented in a single dimension of the hidden state, and the value stored in the cell will be
updated according to the transitions defined by the simulated automaton. Note, however, that the
RNN will not only have a single dimension in the hidden state: other dimensions will contain values
that will be required to control the RNN updates correctly.
In our proofs, we consider a special type of pushdown automata, as defined in Definition 4.2.31:
we will use pushdown automata which only consider the topmost element of the stack when
defining the possible transitions from a configuration and can only push one stack symbol at a
time. More formally, this means that in the tuple P“pΣ,Q,Γ,δ,pqι,γιq,pqφ,γφqq, we have that
δĎQˆΓˆpΣYtεuqˆQˆΓ rather than the more general δĎQˆΓ˚ˆpΣYtεuqˆQˆΓ˚.
Furthermore, we assume that γι“εandγφ“ε, that is, the PDA starts off with an empty stack
and has to empty it again to arrive at a final configuration. Note that these restrictions can be done
without loss of generality—that is, such pushdown automata are as powerful as the unrestricted
versions (Sipser, 2013). With this in mind, we can show that arbitrary precision RNNs are capable
of recognizing at least deterministic context-free languages:
Theorem 5.2.8: RNNs can recognize deterministic context-free languages
Elman recurrent neural networks can recognize deterministic context-free languages.
Before we continue to the proof of Theorem 5.2.8, let us remark on three simple but important
intuitions which will be crucial for understanding the construction of the Elman RNN, both in the
single- as well as the two-stack variants of PDAs. Multiple times in the construction, we will be
194 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
faced with the task of moving or copying the value from some dimension ito the dimension jin the
vector. The following fact shows how this can be done using simple matrix multiplication with a
specific matrix.
Fact 5.2.2: Copying elements of a vector
LetxPRDandMPRDˆDsuch that Mi,:“ei, where Mi,:denotes the ithrow of Mand
eidenotes the ithbasis vector. Then, it holds that pMxqi“xi.
Also, note that setting the row Mi,:to the zero vector 0PRDsets the entry xito 0, i.e., it erases
the entry.
Furthermore, we will use the saturated sigmoid function multiple times to detect whether a
number of dimensions of a vector are set to one at the same time. Given the recurrent dynamics of
the Elman RNN (cf. Eq. (5.26)), we can perform this check as follows.
Fact 5.2.3: Detecting the activation of multiple values in the hidden state
Letσbe the saturated sigmoid from Definition 5.2.15, mPt1,...,Du,i1,...,im,jPt1,...,Du,
xPRD,bPRD, and MPRDˆDsuch that
Mj,i“#
1ifiPti1,...,imu
0otherwise
andbj“ ´pm´1q. Then, it holds that pσpMx`bqqj“1 if and only if xik“1 for all
k“1,...,m .a
aNote that this is simply a restatement of Fact 5.2.1, which we include here for clarity and to make the
connection with the construction that follows clearer.
Lastly, we will sometimes have to turn off certain dimensions of the hidden state if any of the
other dimensions are active. Using the dynamics of Elman RNNs and the saturated sigmoid, this
can be done as follows.
Fact 5.2.4: Turning off dimensions in the hidden state
Letσbe the saturated sigmoid from Definition 5.2.15, mPt1,...,Du,i1,...,im,jPt1,...,Du,
xPRD,bPRD, and MPRDˆDsuch that
Mj,i“#
´1ifiPti1,...,imu
0otherwise
andbj“1. Then, it holds that pσpMx`bqqj“0 if and only if xik“1 for somek“1,...,m .
With these intuitions in mind, we now prove Theorem 5.2.8. Due to the relatively elaborate
construction, we limit ourselves to pushdown automata with a two-symbol input alphabet Σ “ta,bu
as well as a two-symbol stack alphabet Γ “t0,1u. Note, however, that this restriction can be done
without the loss of generality, meaning that this is enough to prove the Turing completeness of
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 195
RNNs in general.17
Proof. We show this by constructing, for a given deterministic pushdown automaton Precognizing
a deterministic context-free language, an Elman RNN simulating the steps performed by P.
LetP“pΣ,Q,Γ,δ,pqι,γιq,pqφ,γφqqbe such a deterministic pushdown automaton. We now
define the parameters of the RNN R“pΣ,D,U,V,E,bh,h0qsuch that the updates to R’s hidden
state will correspond to the configuration changes in P.
The construction is more involved than the one in Minsky’s theorem (cf. Lemma 5.2.2). We,
therefore, first intuitively describe the semantics of the different components of the hidden state
of the RNN. Then, we describe the submatrices of the parameters U,V, and bthat control these
components of the vector. The hidden state hof the RNN will altogether have fivecomponents.
•Component 1: Data component : This component, consisting of three cells, will contain
the actual numerical representation of the stack, STACK , as well as two additional “buffer” cells,
BUFF 1and BUFF 2, which will be used for intermediate copies of the stack values during the
computation of the new state.
•Component 2: Top of stack component : This component contains three cells, each
corresponding to a flag denoting that (a)the stack is empty ( STACKε),(b)the top element of
the stack is a 0 ( STACK 0), or (c)the top element of the stack is a 1 ( STACK 1).
•Component 3: Configuration component : This component encodes the current configu-
ration of the stack (Component 2) together with the current input symbol. Note that, while
we assume that the input PDA works with the two-symbol alphabet Σ “ta,bu, the sequence
model defined by the RNN requires an eossymbol to be able to terminate generation (cf.
Eq. (2.44)): R, therefore, defines the conditional probabilities over the set Σ“ta,b,eosu.
With this, there are nine possible configurations py,γqforγPtε,0,1uandyP ta,b,eosu,
meaning that there are nine cells in this configuration, CONFγ,y, each corresponding to one of
these configurations.
•Component 4: Computation component : This component contains four cells in which the
computation of the next value of the stack is computed. There are five cells OPaction,γbecause
allpossible actions ( PUSH 0,PUSH 1,POP0,POP1, and NO-OP ) are performed simultaneously,
and only the correct one is copied into the data component (Component 1) in the end.
•Component 5: Acceptance component : This component contains a single cell, ACCEPT ,
signaling whether the RNN accepts the string yafter reading in the input yeos.
Altogether, the hidden state of Rcontains 3`3`9`5`1“21 dimensions. The initial hidden
state h0is a vector with a single non-zero component, whose value is 1: the cell STACKεsince we
assume that the stack of the simulated automaton is empty at the beginning of the execution. We
now intuitively describe the dynamics that these components define.
17To simulate an arbitrary Turing machine with a machine with the binary alphabet, we simply have to encode
each of the finitely-many symbols of the simulated machine using binary encoding.
196 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
The full update step of the network. The RNN will compute the next hidden state corre-
sponding to the new stack configuration by applying the Elman update rule (cf. Eq. (5.26)) four
times to complete four discrete sub-steps of the computation. We first define
hp0q
t`1def“ht (5.82)
and
hpnq
t`1def“σ´
Uhpn´1q
t`1`Vepytq`bh¯
(5.83)
forn“1,2,3,4. Then
ht`1def“hp4q
t`1. (5.84)
Intuitively, each of the four stages of computation of the actual next hidden state “detects” some
parts of the pattern contributing to the transition in the pushdown automaton. We describe those
patterns next intuitively before talking about the submatrices (or subvectors) of the RNN parameters
corresponding to the specific parts that update the individual components of the hidden state.
Data component. The cells of the data component form a queue of three components: the STACK
cell forms the head of the queue, followed by BUFF 1andBUFF 2. The values in the cells are updated
at each execution of Eq. (5.83) by moving the currently stored values into the next cell in the queue.
By doing so, the entry in BUFF 2gets discarded. The value of STACK is copied from the cells of the
computation component by summing them. We will see later that at any point of the computation
(when it matters), only one of the computation components will be non-zero, which means that
the summation simply corresponds to copying the non-zero computation component. All these
operations can be performed by matrix multiplication outlined in Fact 5.2.2.
Encoding the stack sequence. While we outlined a possible encoding of a stack sequence above,
the encoding we use in this construction is a bit different. Remember that for a stack sequence
γPΓ˚of lengthN, the right-most symbolγNdenotes the top of the stack. We encode the stack
sequence γPΓ˚as follows:
reppγ1...γNqdef“Nÿ
n“1digitpγnq10N´n´1(5.85)
where digitpγqdef“#
1ifγ“0
3otherwise.
Example 5.2.5: Scalar stack representations
For example, the stack sequence γ“00110111 would be represented with repp00110111q“
0.33313311. Notice the “opposite orientation” of the two strings: the top of the stack in γis
the right-most symbol, while it is the left-most digit in the numerical representation.
Note that the digits 1 and 3 in the definition of digitp¨qare chosen somewhat arbitrarily—the
encoding could also have been chosen differently. Similarly, a different (non-decimal) base could
have been chosen.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 197
Top of stack component. As mentioned, the STACKεcell of this component is one of the two
cells set to 1 in the initial state of the RNN. The individual cells of this component then get updated
according to the top symbol on the stack encoded in the STACK cell by taking into account how
the stack is represented by STACK . Specifically, the parameters of the RNN are defined such that
hSTACKε“1 if previously hSTACK“0,hSTACK 0“1 if previously hSTACK“0.1..., and hSTACK 1“1 if
previously hSTACK“0.3....
Configuration component. The cells of the configuration component combine the pattern
captured by the top of the stack component with the input symbol at the current time step to
activate only the appropriate cell CONFγ,y. This can be done by incorporating the information from
the top of the stack component with the information about the current input symbol from VJytK.
More precisely, the parameters of Rare set such that hCONFγ,y“1 if at the previous one of the four
sub-steps of the computation of the next hidden state, hSTACKγ“1andthe input symbol is y.
Computation component. The computation component contains the cells in which the results
of all the possible actions on the stack are executed. The parameters of the computation component
are set such that, given that the previous stack configuration is hSTACK“x1x2...xNand the input
symbol isy, cells of the computation component are set as
hOPPOP,γ“x2...xN
hOPPUSH,γ“digitpyqx1...xN.
Acceptance component. The cell in the acceptance component is activated if and only if the
current input symbol is eos(denoting the end of the string whose recognition should be determined)
and the stack is empty, i.e., the STACKεcell is activated.
More precisely, the dynamics described here are implemented by the four steps of the hidden
state update as follows (where hp1q
t“ht).
•Inphase 1 , the configuration of the stack is determined by setting the top of the stack
component in hp2q
t.
•Inphase 2 , the configuration of the stack and the input symbol are combined by setting the
configuration component in hp3q
t.
•Inphase 3 all possible operations on the stack are performed in the computation component,
and, at the same time, the results of all invalid operations (only one operation is valid at each
time step due to the deterministic nature of P) are zeroed-out in hp4q
t. This is done by setting
the entries of the recurrence matrix Usuch that only the valid action is not zeroed out.
•Inphase 4 the result of the executed operations (only one of which is non-zero) is copied over
to the STACK cell in the hidden state in ht`1.
Having defined the intuition behind the dynamics of the hidden state updates, we now formally
define how the parameters of the RNN are set to enable them. Whenever an entry of a matrix or
vector is not set explicitly, it is assumed that it is 0 (that is, we only explicitly set the non-zero
values). Again, we define them for each component in turn.
198 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
The data component. The values of the parameters in the data component are set as follows.
UBUFF1,STACK“1 (5.86)
UBUFF2,BUFF1“1 (5.87)
USTACK,OPPUSH,0“USTACK,OPPUSH,1“USTACK,OPPOP,0“USTACK,OPPOP,1“1 (5.88)
The first two elements correspond to moving the values to the next element in the data component
queue, while the entries in the last row correspond to summing up the values from the computation
component to move them into the stack cell after the computation has been completed. Note that,
of course, the elements of the computation component are always summed up and written in the
STACK cell, no matter what the values there are. However, the division of the computation of the
next hidden state into phases ensures that when it matters , i.e., after the third phase, there is only
a single computation component that is non-zero, and that one is copied into the STACK component
in the fourth computation sub-step. All other parameters (in Vandbh) are 0.
The top of the stack component. The parameters setting the top of the stack component are
set as follows:
USTACKε,STACK“´10 (5.89)
USTACK 0,STACK“´10 (5.90)
USTACK 1,STACK“10 (5.91)
bSTACKε“1 (5.92)
bSTACK 0“3 (5.93)
bSTACK 1“´2. (5.94)
Other parameters ( V) are 0. The reasoning behind these parameters is the following. The cell
STACK contains the numeric encoding of the stack content. We distinguish three cases.
•If the stack is empty ,hSTACK“0. Therefore, using the parameters above, the value of the cell
STACK 1after the sub-step update will be 0, while the cells STACKεandSTACK 0will be 1 due to
the positive bias term. This might not be what you would expect—it might seem like, in this
case, this step erroneously signals both an empty stack and a stack whose top component is 0.
This, however, is corrected for in the configuration component, as we discuss below.
•If the top of the stack is the symbol 0, hSTACK“0.1.... This means that 10 ¨hSTACKď1 and,
therefore, after the update rule application, hSTACK 1“0. It is easy to see that the setting
of the parameters also implies hSTACKε“0. However, since ´10¨hSTACKě´2, we have that
hSTACK 0“1.
•Lastly, if the top of the stack is the symbol 1, hSTACK“0.3.... Therefore, 10 ¨hSTACKě3,
meaning that after the update rule application, hSTACK 1“1. Again, it is easy to see that the
setting of the parameters also implies hSTACKε“0. On the other hand, since ´10¨hSTACKď´3,
it also holds that hSTACK 0“0.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 199
The configuration component. The configuration component is composed of the most cells of
any component:
UCONFγ,y,STACKγ“1forγPtε,0,1u,yPteos,a,bu (5.95)
UCONFγ,0,STACKε“´1foryPteos,a,bu (5.96)
VCONFγ,y,mpyq“1forγPtε,0,1u,yPteos,a,bu (5.97)
bCONFγ,y“´1forγPtε,0,1u,yPteos,a,bu (5.98)
Here, the first, third, and fourth terms together ensure that the cell CONFγ,yis activated if the current
input symbol is a(VCONFγ,y,mpyq) and the top of the stack is γ(UCONFγ,y,STACKγ).bCONFγ,yensures that
both conditions have to be met. The second term, UCONF0,y,STACKε, on the other hand, takes care of
an edge case: as shown above, bSTACK 0“0, which means that STACK 0is, by default, set to 1. The
negative weight UCONF0,y,STACKε“´1 ensures that, if the stack is indeed empty, the effect of this
default value is “canceled out”, i.e., the configuration cell is not activated by mistake.
The computation component. This is the most complicated component. The computation
components are manipulated with the following parameters:
UPUSH0,BUFF2“UPUSH1,BUFF2“1
10(5.99)
UPOP0,BUFF2“UPOP1,BUFF2“10 (5.100)
UNO-OP,BUFF2“1 (5.101)
UA,CONFγ,y“´10forAPtOPPUSH,0,OPPUSH,1,OPPOP,0,OPPOP,1,OPNO-OPu (5.102)
γPt0,1u,yPta,bu
UOPA,γ1,CONFγ,y“0forqy,γÑγ1
Ý ÝÝÝÝ ÑqPδ, (5.103)
APtOPPUSH,0,OPPUSH,1,OPPOP,0,OPPOP,1,OPNO-OPu
UOPNO-OP,CONFγ,eos“0forγPtε,0,1u (5.104)
bOPPUSH,0“1
10(5.105)
bOPPUSH,1“3
10(5.106)
bOPPOP,0“´1 (5.107)
bOPPOP,1“´3 (5.108)
bOPNO-OP“0 (5.109)
The first three parameters above concern copying the value of the stack encoded by the previous
hidden state into the computation component and preparing it for modification. They work together
with the corresponding entries in the bias vector bh. For example, a value can be pushed onto the
stack by dividing the value of the stack encoding by 10 and adding either 0 .1 or 0.3, depending on
whether 0 or 1 is being pushed. This is encoded by the first setting above and bOPPUSH,0andbOPPUSH,1.
Similarly, a value can be popped from the stack by multiplying the stack encoding with 10 and then
subtracting the appropriate value according to the bias entry. The NO-OP action is implemented
simply by copying the values of the stack into its cell. The remaining three parameter settings above
200 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
ensure that, after executing all possible stack actions, only the appropriate computation is kept and
all others are zeroed out. The fourth row above ensures that “by default”, all computation cells are
reset to 0 after every update. However, the next row “removes” the negative weights (sets them to 0)
for the changes in the configuration which correspond to the valid transitions , or valid actions, in the
pushdown automaton. That is, setting those values of the matrix Uto 0 disables “erasing” the entry
OPA,γ1in the hidden state by the configuration CONFγ,yifthe transition from the configuration with
the top of the stack γtoγ1with the action Aupon reading yis encoded by the original automaton.
The last remaining row simply ensures that reading in the eossymbol results in the NO-OP action
being executed ( eosactions are not encoded by the original pushdown automaton).
The acceptance component. Lastly, the acceptance component is controlled by the following
parameters:
UACCEPT,A“´10forAPtOPPUSH,0,OPPUSH,1,OPPOP,0,OPPOP,1,OPNO-OPu (5.110)
UACCEPT,CONFγ,y“´10forγPtε,0,1u,yPteos,a,bu (5.111)
bACCEPT“1 (5.112)
The entry bACCEPT ensures that, by default, the value of ACCEPT is set to 1. However, the other
parameters ensure that, as soon as any part of the configuration is not compatible with the acceptance
state (the read symbol is not eosor the stack is not empty), the acceptance bit is turned off.
A full proof of the theorem would now require us to show formally that the update rule
Eq. (5.84) results in the correct transitions in the PDA. We, however, leave the proof with the
intuitive reasoning behind the setting of the parameters and leave this as an exercise for the
reader. The proof is also demonstrated in the python implementation of the constructions here:
https://github.com/rycolab/rnn-turing-completeness . ■
The construction described in the proof of Theorem 5.2.8 is demonstrated in the following
example.
Example 5.2.6: Siegelmann’s construction
LetPbe a single-stack PDA presented in Fig. 5.16. We now simulate the recognition of the
string y“ab, which is accepted by P. The initial state h0has a single non-zero cell, STACKε.
The four phases of the processing of the first input symbol aare shown in Tab. 5.1. The four
phases of the processing of the second input symbol bare shown in Tab. 5.2.
Theorem 5.2.8 shows that Elman RNNs are theoretically at least as expressive as deterministic
CFGs. We now return to the main result of this subsection: the Turing completeness of RNNs.
Luckily, Theorem 5.2.8 gets us most of the way there! Recall that by §4.2.9, two-stack PDA
are Turing complete. We make use of this fact by generalizing the construction in the proof of
Theorem 5.2.8 to the two-stack case. This will prove that RNNs can in fact simulate any Turing
machine, and are, therefore, Turing complete.
Lemma 5.2.17
LetP“pQ,Σ,Γ,δ,pqι,γι,σιq,pqφ,γφ,σφqqbe a two-stack pushdown automaton. Then, there
exists an Elman RNN Rsimulating P, i.e.,LpRq“LpPq.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 201
Initial state Phase 1 Phase 2 Phase 3 Phase 4
STACK 0 02
52
51
10
BUFF 1 0 0 02
52
5
BUFF 2 0 0 0 02
5
STACKε 1 1 1 0 0
STACK 0 0 1 1 0 0
STACK 1 0 0 0 1 1
CONF eos,a 0 0 1 1 0
CONF eos,b 0 0 0 0 0
CONF 0,a 0 0 0 0 0
CONF 0,b 0 0 0 0 0
CONF 1,a 0 0 0 0 1
CONF 1,b 0 0 0 0 0
CONFε,eos 0 0 0 0 0
CONF 0,eos 0 0 0 0 0
CONF 1,eos 0 0 0 0 0
OPPUSH,0 01
101
101
101
10
OPPUSH,1 03
103
100 0
OPPOP,0 0 0 0 0 0
OPPOP,1 0 0 0 0 0
OPNO-OP 0 0 0 0 0
ACCEPT 0 1 0 0 0
Table 5.1: The simulation of the processing of the first symbol aby the RNN simulating the PDA
in Fig. 5.16. After the fourth phase, the stack cell contains the encoding of the stack as 0 .1.
202 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
Initial state Phase 1 Phase 2 Phase 3 Phase 4
STACK 1{10 1{10 1 2 {5 0
BUFF 1 2{5 1{10 1{10 1 2 {5
BUFF 2 2{5 2{5 1{10 1{10 1
STACKε 0 0 0 0 0
STACK 0 0 1 1 0 0
STACK 1 1 0 0 1 1
CONF eos,a 0 0 0 0 0
CONF eos,b 0 0 0 0 0
CONF 0,a 0 0 0 0 0
CONF 0,b 0 0 1 1 0
CONF 1,a 1 0 0 0 0
CONF 1,b 0 1 0 0 1
CONFε,eos 0 0 0 0 0
CONF 0,eos 0 0 0 0 0
CONF 1,eos 0 0 0 0 0
OPPUSH,0 1{10 0 0 0 0
OPPUSH,1 0 0 0 0 0
OPPOP,0 0 0 0 0 0
OPPOP,1 0 1 0 0 0
OPNO-OP 0 0 2 {5 0 0
ACCEPT 0 0 0 0 0
Table 5.2: The simulation of the processing of the second symbol bby the RNN simulating the PDA
in Fig. 5.16. After the fourth phase, the stack cell contains the encoding of the empty stack.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 203
qa,εÑ0
a,0Ñ1
a,1Ñε
b,εÑ1
b,0Ñε
b,1Ñ1
Figure 5.16: The single-stack pushdown automaton P.
Proof. Again, given a two-stack PDA P“ pQ,Σ,Γ,δ,pqι,γι,σιq,pqφ,γφ,σφqqwith Σ“ ta,bu,
Γ1“ t0,1u, and Γ 2“ t0,1u, we construct an Elman RNN R“ pΣ,D,U,V,E,bh,h0qwhich
recognizes the same language as P.
The hidden state of Rwill contain the same components as in the proof of Theorem 5.2.8.
Moreover, their dynamics will be exactly the same—they will simply be larger to account for more
possible configurations of the two stacks together. For example, the top of the stack component will
now consist of cells STACKγ1γ2forγ1PΓ1andγ2PΓ2flagging that the top symbol on stack 1 is γ1
and the top symbol of stack 2 is γ2. Furthermore, the configuration component will contain cells
of the form OPaction,γ1γ2with an analogous interpretation. Lastly, all the computation and data
component cells would be duplicated (with one sub-component for each of the stacks), whereas the
acceptance component stays the same. We now again describe these components and their dynamics
intuitively, whenever there is any difference to the single-stack version.
Data component. Instead of having a single queue of data cells, Rnow has twoqueues, one
for each of the stacks. The first queue will be formed by STACK 1,BUFF 11, and STACK 21, and the
second one by STACK 2,BUFF 12, and STACK 22. Each of the queues acts exactly the same as in the
single-stack version, and they act independently based on the computations done in the computation
cells of the respective stacks. Each of the stacks is also encoded as a numeric sequence in the same
way as in the single-stack version.
Top of stack component. Again, Rstarts in an initial state in which the cell STACKεεis 1 and
all others are 0. The individual cells of this component then get updated according to the top
symbols on the stacks encoded in the STACK 1 and STACK 2 cells.
Configuration component. The cells of the configuration component combine the pattern
captured by the top of both stack components with the input symbol at the current time step to
activate only the appropriate cell CONFγ1γ2,y.
Computation component. Again, the computation component contains the cells in which the
results of all the possible actions on both stacks are executed. They execute the actions on both
stacks independently.
Acceptance component. The acceptance component functions identically to the single-stack
case.
204 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
Using these components, the RNN then transitions between the phases exactly like in the
single-stack case. We leave the specifications of the matrix parameter values to the reader. They
again follow those presented in the single-stack case but treat the transitions and configurations of
both stacks.
■
5.2.5 The Computational Power of RNN Variants
Most of the current section was devoted to understanding the computational power of simple Elman
RNN language models due to their simplicity which allows an easy connection to formal models
of computation. However, as mentioned in §5.1.5, gated RNN variants such as LSTMs and GRUs
have become the standard in modern natural language processing tasks, showing more better and
more reliable performance on a variety of tasks. Interestingly, besides their better resilience to the
vanishing gradient problem, LSTM-based language models are also provably more expressive than
simple RNN language models. On the other hand, the simpler GRU-based language models are in
some ways only as powerful as simple Elman RNN language models. To give an intuition behind
this, this subsection provides a short overview the results considering the computational power of
RNN variants.
Weiss et al. (2018) compare the practical computational power of different RNN variants under
the constraint of bounded computation time and limited precision. Interestingly they empirically find
that LSTMs can learn to recognize languages that require some form of counting , liketanbn|nPNu
ortanbncn|nPNu, while GRUs struggle to do so. This invites the comparison of LSTMs with
counter machines (Hopcroft et al., 2006; Fischer et al., 1968), a class of formal computational
models with the ability to count. Simply put, counter machines are finite-state automata with an
additional (unbounded) counter cell, which they can manipulate by incriminating and decrementing
the value stored in it.18Counter machines present an interesting addition to the traditional hierarchy
of computational models, since they in some ways cut across it, being able to recognize some, but
not all, context-free languages, while also being able to recognize some context-sensitive languages.
Among others, they can for example recognize the context-free language tanbn|nPNuand the
context-sensitive language tanbncn|nPNu, while not being able to recognize the Dyck context-free
languages Dpkqwithkdifferent parenthesis types (intuitively, this is because recognizing Dyck
languages requires keeping track of the order in which the parantheses appeared, not only their
counts).
Counter machines can recognize languages like tanbncn|nPNu, by counting the number of as
and making sure that it matches the number of bs. Further, analyzing the activation of the memory
cell and of the hidden state, they find that LSTMs that one can recognize the use of a counting
mechanism implemented by the network by using one or more dimensions of the memory cell as
counters. This result is particularly interesting as GRUs are often considered an equivalent variant
to the LSTM one, with the same computational power, but smaller computation overhead. However,
GRUs seem to lack this counting behavior, which can be backed by theoretical analysis.
Merrill et al. (2020) take a look at the computational power of the different RRN variant from
another perspective. They consider space complexity and whether the networks are rationally
recurrent, i.e. whether their hidden state update function can be expressed in terms of finite state
machine computations. Making the assumption of saturated networks19, they find that while GRUs
18We only consider counter machines with a single counter cell.
19Informally, a saturated network is a neural network in which the norms of the parameters are taken to 8. This
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 205
Figure 5.17: Picture from Weiss et al. (2018): Plot of the activation value of the memory cell (LSTM)
and of the hidden state (GRU), versus the indices of the input. The Networks have been trained
to recognize either the languages tanbn|nPNuortanbncn|nPNu. As you can notice, in both
cases the LSTM has learned to use one or more dimension of the memory cell as a counter , which
allows it to count how many aandbhave been have been consumed so far. Conversely, the GRU
hasn’t developed this counter mecahnism, and in fact empirical evidence shows that it struggles to
to recognize the described languages.
and Elman networks are rationally recurrent and therefore at most regular, LSTMs are not, meaning
that their hidden state update function cannot be expressed by means of finite-state machines.
5.2.6 Consequences of the Turing completeness of recurrent neural net-
works
The section above outlines Siegelmann and Sontag’s (1992) construction of encoding a Turing
machine in an RNN. While Turing completeness means that RNNs are in many ways computational
very powerful (as powerful as they can be), it also brings with it many computational challenges faced
when working with Turing machines. Computability theory, for example, defines many problems
related to Turing machines and their properties which are not computable , orundecidable , meaning
that no algorithm (or, equivalently, a Turing machine) can solve them.20The most classical and
has the effect of pushing all the squashing functions of the network to one of their extreme values, in the case of the
sigmoidt0,1uandt´1,1uin the case of the hyperbolic tangent
20The notion of solving a problem computationally is quite nuanced and we only provide very broad intuitions
for now. Readers who want to delve deeper into the topic of computability and with it the implications of Turing
completeness are encouraged to look at some classical texbooks on the material, for example Sipser (2013, Part Two).
206 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
fundamental of such problems is the halting problem (Turing, 1937).
Definition 5.2.16: Halting problem
LetMbe a Turing machine over the input alphabet Σ and yPΣ˚. The halting problem is
the problem of deciding whether M(halts and) accepts y.a
aA formal definition of the halting problem would require us to define the formal language Lof Turing
machine and input tuples pM,yqand asking if there exists a Turing machine M1that accepts L. However, to
keep the discussion brief, we define the problem more intuitively.
The halting problem is the foundation of the results presented by Chen et al. (2018), which
building on Siegelmann and Sontag (1992) considers its implications on the theory of RNN language
models. For example, they show that determining many practically useful properties of general
rationally weighted RNNs is undecidable . Such properties include the tightness of a general RNN,21
the equivalence of two RNN language models, the minimal size (in the size of the hidden state) of
an RNN defining the same language model as some given RNN, and the highest probability string
in an RNN, i.e., argmaxyPΣ˚pLNpyq. By simulating a Turing machine, we can encode the halting
problem as solving any of those tasks.22. This means that if we could solve these problems for RNNs,
we could also solve the halting problem, which is provably impossible, meaning that these problems
are not computable either. We briefly outline the individual findings of Chen et al. (2018) below,
sketching the rough idea behind the proofs.23
Theorem 5.2.9: Tightness
Determining tightness of an RNN language model is undecidable.
Proof. Note first that not all RNNs are tight language models. As mentioned, this is not a
contradiction to the results from §5.1.3, which considered softmax-projected RNN language models.
Indeed, the RNN we constructed in §5.2.4 is not such an RNN. That construction shows that, given
an arbitrary Turing machine Mand input y, we can construct an RNN that simulates Mrunning
ony. Using this construction, we can reduce the problem of deciding the tightness of a general
(rational weighted Elman) RNN to the halting problem, i.e., the problem of answering “Does M
halt on y?”. We do that by constructing an RNN that simulates the given Turing machine Mon
the input y, ending generation if Mhalts, and, at the same time, produces strings according to a
distribution that is tight for finite length strings, on the condition that no infinite length strings can
be produced. Now if and only if Mhalts on y, the language model is tight. Therefore, deciding is
at least as hard as solving the halting problem, and since that is undecidable, so is tightness. ■
Theorem 5.2.10: Highest weighted string
Finding the highest weighted string of an RNN LM is undecidable.
Proof. Once more, we reduce the halting problem to the given task on the RNN. The idea behind
this is to again simulate an arbitrary Turing machine by constructing an RNN LM which is not
21Note that the results from §5.1.3 consider only RNN LM with the softmax projection function.
22In computability theory, this is known as reducing the halting problem to the given one
23The interested reader is encouraged to see the original paper for the details.
5.2. REPRESENTATIONAL CAPACITY OF RECURRENT NEURAL NETWORKS 207
tight unless the simulated Turing machine halts. One can create such an RNN with a one-symbol
output alphabet such that its highest-weighted output is an infinite string. Now, if we again enforce
that the RNN ends producing outputs once the simulated Turing machine halts, it can be shown
that there exists a language in which each string has a probability of less than 0 .12 if and only if the
Turing machine does not halt. On the other hand, if the Turing machine does halt after Tsteps,
producing a string which has length 3 T´5 has a probability of ě0.25. Therefore, the weight of the
highest probability string depends on the question of whether the simulated Turing machine halts,
which is undecidable. ■
Theorem 5.2.11: Equivalence
Equivalence between two RNN LMs in the sense of defining the same language model is
undecidable.
Proof. The proof of this claim is again a reduction from the halting problem. We construct an RNN
which simulates a given arbitrary Turing machine until it halts and has the same outputs as some
other RNN. As soon as the Turing machine halts, the outputs differ, so the RNNs are equivalent if
and only if the Turing machine does not halt. Hence, equivalence is undecidable. ■
Theorem 5.2.12: Minimization
Finding the RNN with the minimum number of hidden layer neurons defining the same
language model as a given RNN LM is undecidable.
Proof. We can reduce the halting problem to the following problem: For some RNN LM and an
integerD, return yes if there is another RNN LM with ďDhidden units that generates the same
weighted language. Assume that there is a Turing machine Mthat can decide this problem. Now,
for another Turing machine M1and input y, construct a one symbol RNN LM, R, that simulates
M1running on yand stops generating if M1halts. We assume without loss of generality that
M1runs for more than one computation step. Now we run Mon the input pR,0q, which checks
whether there is another RNN LM generating the same weighted language as Rand has no hidden
units. Having no hidden units means that the output probabilities of each symbol would have to
be constant for each time step. If Mdecides minimization returns true, that means the output
probabilities of Rcannot change over time, which means that M1has to run forever. Conversely, if
Mreturns false, the output probabilities change when M1halts. Therefore, Mdeciding the minimal
hidden states problem on pR,0qis equivalent to it deciding the Halting problem for pM1,yq.■
This concludes our investigation of the formal properties of recurrent neural language models.
The sequential nature of the architecture and the relatively simple transition functions in the vanilla
RNN architectures made the link to automata from formal language theory relatively straightforward,
which allowed relatively strong theoretical insights. However, it was exactly this sequential nature
and the issues associated with it of RNNs that eventually led to them being overtaken by another
neural architecture, which is now at the core of most if not all, modern state-of-the-art language
models: the transformer.24We introduce them and discuss their theoretical underpinnings in the
next section.
24We will not discuss the issues with the training speed and parallelization of RNNs in detail. Some of these issues
will be highlighted in the latter parts of the course.
208 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
5.3 Transformer-based Language Models
In the previous section ( §5.1.2), we introduced and studied RNN language models as of language
models capable of storing an arbitrarily long context in its encoding encpyătqby updating their
hidden state htan arbitrary number of times. As we saw in their theoretical analysis, this mechanism
gives them, under some assumptions, a lot of expressive power. Nevertheless, as we also discussed,
RNN LMs come with their distinct set of drawbacks. Some of them, e.g., the exploding and vanishing
gradient problems, can be amended using specific mechanisms resulting in more complex recurrent
neural networks, such as LSTMs and GRUs (cf. §5.1.5). As discussed in §5.1.5, a more fundamental
issue that cannot be avoided is the difficulty of parallel training, which is particularly noticeable on
the vast internet-scale corpora used nowadays to train language models. Can we do anything about
that? As discussed, the inherently sequential nature of RNNs suggests strict limits on this front.
Motivated by this limitation, in this section, we present a newer architecture that took over the field
of language modeling (and NLP in general)—transformers (Vaswani et al., 2017). It was originally
introduced for machine translation, but it can easily be applied to language modeling and has led to
the success of models such as GPT-n.
The structure of this section will be a bit different from that of the other sections in the notes.
We will first give a formal definition of a transformer model in §5.3.2 and, based on this definition,
derive a number of results analogous to those for RNN LMs from §5.2. However, due to the current
practical relevance of transformers in language modeling, we then devote a significant portion of the
section to more practical aspects of transformer models and introduce a number of modifications
used in modern language modeling systems.
5.3.1 Informal Motivation of the Transformer Architecture
Before we introduce transformers, let us consider another practical drawback of RNN LMs, which
will give us more clues on how to improve them and motivate the architectural decisions behind
transformers. Luckily, the simplest patch-up of this issue will also lend itself naturally to paral-
lelization, as we will see shortly. The main characteristic of RNNs is the use of a single hidden
state htto represent an arbitrary prefix of any string yătup to the current time step t. While this
allows RNNs to model strings of any length, it also means that arbitrarily long strings must be
compressed into this hidden vector of fixed size. Intuitively, this becomes increasingly difficult as
the length of the context grows: As the amount of information to be compressed into the hidden
state increases with the prefix length, the hidden state may struggle to model the entirety of the
preceding context. How can we amend that? The simplest na¨ ıve way to go about this is to retain
the contextual encodings of allprefixes of the string. In this case, it is actually more natural to
talk about contextual encodings not of full prefixes, but simply of the individual symbols in the
string.25Here, contextual means that the symbol encodings are augmented with the information
from the rest of the string (in most cases, about the preceding context, as with the hidden states of
an RNN). With this, we avoid the need to summarize the entire context into a single state. Note
that our infinite-precision RNNs from the previous section implicitly did that as well—for example,
by storing the information in the “stack” neurons, they could, in principle, store the entire history of
the string. However, storing all the encodings explicitly makes their utilization more direct and thus
easier. This of course leaves the model with the issue of remembering increasingly large amounts of
25For example, looking back to RNNs, we could consider htto simply be an encoding of the symbol ytaugmented
with the information from yăt.
5.3. TRANSFORMER-BASED LANGUAGE MODELS 209
y0y1y2y3y4h0 h1 h2 h3 h4 State ht
Inputyt ¨¨¨¨¨¨
Figure 5.18: An abstract depiction of how a transformer language model produces the contextual
embeddings of all symbols in a string. The hidden state htcan “attend to” (the precise meaning of
this term will be introduced soon in §5.3.2) all preceding symbols yătand the current symbol yt.
information as the length of the context increases, but we will, for the moment, assume that we can
always store enough information to process any string in this way.
Having decided to keep around the encodings of all symbols in the string, let us think about
parallelizing the process of encoding a string, i.e., computing encpyq. Remember the very general
way in which RNNs build a representation of the string yătby incrementally modifying ht, which
is illustrated in Fig. 5.1a—this incrementality brings with it all the challenges of impossible par-
allelization. The workaround for the issues with the sequential processing of RNNs is to process
the context for each ytindependently , without relying on the encodings of the previous symbols,
thus avoiding the sequential bottleneck. Nevertheless, we still want the contextual encoding of ytto
contain information about the rest of the string, i.e., the preceding context. How can we achieve that
without relying on recurrence? Again, we grab onto the simplest solution: to compute the symbol
encodings for each symbol yt“from the ground up” based only on the static symbol encodings e1pytq,
which do not require any recurrence. This is abstractly illustrated in Fig. 5.18, whereas Fig. 5.19
shows how this translates into the generative framework from Definition 3.1.11, where individual
symbolsytare both sequentially generated based on the encodings of the preceding context encpyătq
as well as used to build the representation of the context in the next time step encpytq. Notice that
instead of containing arcs denoting dependencies between the symbol encodings (“hidden states”)
ht, Fig. 5.18 and Fig. 5.19 contain arcs connecting each htto all symbols yjfor alljďt. Compare
this to Fig. 5.1a, where the arcs between ht´1andhtinduce the temporal dependence, and carry
the information about the symbols yjfor alljďt.
Clearly, this avoids the issues faced by RNNs due to their sequentiality. However, it also
introduces more work required to compute the individual contextual encodings from the static
symbol representations. The operations performed to do this by transformers, which are together
known as the attention mechanism , are introduced in the next subsection. They represent
possibly the most important component of the entire structure of the transformer—by “attending”
to relevant preceding symbols when computing the symbol encodings (i.e., using them to compute
encpyătq), the transformer can model long-range dependencies very effectively, and use them for
appropriately modeling the distribution over the next word.
To recap, in this section, we informally motivated the new architecture, transformers, with the
goals of 1. remembering the contextual encodings of all symbols explicitly and 2. parallelizing the
computation of the contextual symbol encodings. The next subsection formally introduces the
architecture, before we dive into their theoretical properties.
210 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
y0 y1 y2 y3h0 h1 h2 State ht
Input/Output yt ¨¨¨¨¨¨
y1„
pSM
p¨|
h
0qy2„
pSM
p¨|
h
1qy3„
pSM
p¨|
h
2q
Figure 5.19: An abstract depiction of how a transformer language model generates a string one
symbol at a time. The hidden state htcan attend to all previously generated symbols yătto sample
the next symbol yt. The dotted lines denote the sampling steps.
5.3.2 A Formal Definition of Transformers
Having informally introduced the main two ideas behind the transformer architecture in the previous
subsection, we now provide a formal definition of a transformer model, which we will then augment
with more practical considerations in .
Definition 5.3.1: Transformer network
Atransformer network Tis a tuplepΣ,D,encTqwhere
•Σ is the alphabet of input symbols,
•Dis the dimension of T, and
•encTis the transformer encoding function (cf. Definition 3.1.7), which we define in more
detail below (Definition 5.3.2).
From afar, the definition of a transformer network is therefore relatively simple; it is stated to
make the transformer models fit well into our representation-based locally normalized language
modeling framework (cf. Definition 3.1.11). The complexity of the models of course comes from the
definition of the transformer encoding function enc T, to which we devote the rest of the section.
Continuing in the framework of representation-based locally normalized language models, the
hidden states of the transformer play an analogous role to those of RNNs, with the only difference
being how they are computed.
Definition 5.3.2: Transformer hidden state
LetT“pΣ,D,encTqbe a transformer network. The hidden state htPRDdescribes the state
ofTafter reading yďt. It is defined with respect to the transformer encoding function encT
as follows:
htdef“encTpyďtq (5.113)
Crucially, as we will see shortly, the hidden state htof the transformer does not have any
dependence on the preceding hidden states themselves (although, as we will see, it is partially a
5.3. TRANSFORMER-BASED LANGUAGE MODELS 211
function of the same inputs ).
As hinted above, with this, we can easily fit transformers into the representation-based locally
normalized language modeling framework and define a sequence model based on the model.
Definition 5.3.3: Transformer sequence model
LetTbe a transformer network and EPR|Σ|ˆDa symbol representation matrix. A D-
dimensional transformer sequence model over the alphabet Σ is a tuple pΣ,D,encT,Eq
defining the sequence model of the form
pSMpyt|yătqdef“softmaxpE ht´1qyt“softmaxpEencTpyătqqyt(5.114)
Now that we have unified the transformer Tto the theoretical framework introduced so far in
the course, we can jump in and look at the internal structure of the transformer encoder function,
which is where the novelty of the transformer architecture comes from.
The Attention Mechanism
As we mentioned in the informal motivation, to avoid over-compressing information about sentences
into a single vector, a transformer model retains the encodings (captured in the hidden states ht) of
allpossible prefixes of the string, which we can equivalently simply regard as encodings of individual
symbols augmented with the information from the preceding string (see Fig. 5.18).26However,
rather than computing the encodings sequentially like an RNN, the encodings of the individual
symbols are computed with the so-called attention mechanism.
Definition 5.3.4: Attention
Letf:RDˆRDÑRbe ascoring function andf∆D´1a projection function. Furthermore,
letqPRD,Kt“`
kJ
1,...,kJ
t˘
PRtˆDandVt“`
vJ
1,...,vJ
t˘
PRtˆD.
Attention overKt,Vt, also denoted by Attpqt,Kt,Vtq:RDˆRtˆDˆRtˆDÑRDis a
function computing the vector ain the following two-step process:
st“ps1,...,stqdef“f∆D´1pfpq,k1q,fpq,k2q,...,fpq,ktqq (5.115)
at“Attpq,Kt,Vtqdef“s1v1`s2v2`¨¨¨`stvt (5.116)
q,K, and Vare commonly referred to as the query ,keys , and values of the attention
mechanism, respectively. We talk about the parameters q,K, and Vcompletely abstractly for now.
However, to help you connect this to the representation-based language modeling framework, note
thatqwill later correspond to a query representing an individual symbol yt, whereas KandVwill
contain the information from yătused to compute ht.
What the attention function computes. The scoring function fis, abstractly, simply a
parameter of the model which we can choose freely. Intuitively, it should express the relevance of a
particular key kto the query q—the more the key is relevant to the query, the more “attention” the
26From now on, we will talk about contextual symbol encodings , which simply refers to the hidden states
corresponding to individual symbols.
212 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
model will put to the value associated to that key. The projection function f∆D´1then transforms
the computed scores ensuring that the transformed scores sum to 1.27The vector of transformed
scores s(Eq. (5.115)) is then used to compute the result of the attention function—the vector a.a
is a convex combination of the values vpassed to the attention function. Abstractly, therefore, the
keys contain the information used for “indexing” the values with the specific query .
The scoring function. As mentioned, the scoring function is supposed to measure the “relevance”
of a particular value for a query qthrough the values’ key. The most common choice for fis the dot
product between query and key, which is often scaled by the square root of the vector dimensionality:
fpq,kq“1?
Dxq,ky (5.117)
The projection function and soft and hard attention. The projection function used to
transform the un-normalized attention scores is a crucial component of the transformer model. By
far the most commonly used projection function is again the softmax. In this case, the attention
function is referred to as soft attention.
Definition 5.3.5: Soft attention
Thesoft attention is computed with the projection function f∆D´1“softmax.
However, the softmax again makes the models difficult to analyze. In our voyage to theoretically
understand transformer-based language models, we will, therefore, again make specific (less frequently
used) modeling choices, particularly in the case of the projection function.
Indeed, to be able to derive any interesting expressivity results (see §5.4), we jump to the other
side of the spectrum and define hard attention. Simply put, instead of spreading the attention across
all values like softmax, hard attention puts all the mass on the element whose key maximizes the
scoring function f. One way to arrive at it from the definition of soft attention is by sending the
temperature τin the definition of the softmax function (cf. Definition 3.1.10) to 0. Recall that that
results in the output vector representing a uniform distribution over the elements that maximize the
input vector. This is known as averaging hard attention.
Definition 5.3.6: Averaging hard attention
Theaveraging hard attention is an attention mechanism with the projection function
f∆D´1“hardmax avg, where hardmax avgis defined as:
hardmax avgpxqddef“#
1
rifdPargmaxpxq
0 otherwise, ford“1,...,D (5.118)
where xPRDandr“|argmaxpxq|is the cardinality of the argmax set over x.
Interestingly, there is another form of hard attention that results in a model with a different
expressive capacity: the unique hard attention. The difference lies exactly in how it handles ties
27While the fact that the transformed scores sum to one invites their interpretation as probabilities, this is not
their central role. Rather, the weights are simply used to define a convex combination of the values.
5.3. TRANSFORMER-BASED LANGUAGE MODELS 213
in the elements which maximize the scoring function. Unique hard attention chooses only a single
element of those that maximize the score: it can be chosen randomly or deterministically (e.g.,
always the first one).
Definition 5.3.7: Unique hard attention
Theunique hard attention is an attention mechanism with the projection function f∆D´1“
hardmax uni, where hardmax uniis defined as follows. For xPRD, sample ˆd„Unifpargmaxpxqq
or choose some ˆdPargmaxpxqdeterministically. Then
hardmax unipxqddef“#
1 ifd“ˆd
0 otherwise, ford“1,...,D. (5.119)
While the difference between unique and averaging hard attention might seem subtle and marginal,
it actually results in a large difference in the expressivity of transformer-based language models as
we discuss in §5.4. While we will investigate this in a lot more detail there, we just mention that the
intuition behind the expressive difference is relatively straightforward: while the keys maximizing
the un-normalized scores might be the same (even though they necessarily don’t have to be if fis
not injective), the values (whose content is decoupled from the keys) that those keys index might
not be—and in some cases, all those different values might be relevant for the task at hand. Unique
hard attention always allows us to only “lookup” a single value associated with those keys, no matter
how “different” and relevant all of those are. It also does not allow the attention mechanism to
sum over (“summarize”) across all the elements that maximize attention. This is a very limiting
characteristic, as many of the expressivity results that we will see later rely on summing over all the
elements that maximize the attention scores.
Transformer Blocks
We have, so far, described the “low-level” details of how the attention function is computed. We now
combine the computations performed into larger blocks, showing how they are used to compute the
string-augmented encodings of the individual symbols. In particular, we have to connect the concepts
of queries, keys, and values to the symbols and their (initial, static) encodings. Intuitively, this is
done by transforming the static encodings of those symbols using specific functions implemented
through the attention mechanism and using the transformed encodings as queries, keys, and values,
as we describe below.
We first abstract the attention mechanism from Definition 5.3.4 a bit. With this, we will, in a
few steps, arrive at exactly how the hidden states htor the contextual encodings are computed in a
transformer. At the core of this computation lies a repeated application of the same sequence of
operations, which, as we will see, augment the “current version” of the contextual encodings with
the current information from the preceding information. We call a single sequence of operations a
transformer layer.
214 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
Definition 5.3.8: Transformer layer
LetQ,K,V, andObe parametrized functions from RDtoRD.
Atransformer layer is a function T:RTˆDÑRTˆDthat takes as input sequence of vectors
X“pxJ
1,xJ
2,...,xJ
Tqand returns Z“pzJ
1,zJ
2,...,zJ
TqPRTˆDaccording to the following
steps:
at“AttpQpxtq,KpXtq,VpXtqqloooooooooooooooomoooooooooooooooon
Definition 5.3.4`xt (5.120)
zt“Opatq`at (5.121)
fort“1,...,T , so that TpXqdef“Z“pzJ
1,zJ
2,...,zJ
TqPRTˆD.
While we defined the transformer layer on a general matrix (with Tcolumns), note that these
Tvectors will refer to the (current) symbol encodings of the symbols in the string up to the Tth
symbol, i.e., yďT.
What do these quantities correspond to? Eqs. (5.120) and (5.121) outline a two-step process
of computing the outputs of a single transformer layer: X“pxJ
1,xJ
2,...,xJ
Tqrepresents the input
to the layer, which Ttransforms into the output sequence Z“pzJ
1,zJ
2,...,zJ
Tq. Before being fed
into the attention mechanism, the inputs Xare first transformed into the quantities required by the
attention mechanism: the query qt(a single one for each xt), the matrix of keys Kt, and the matrix
of values Vt—all of these are, therefore, transformations of the input sequence of vectors. The
transformations Q,K, andVdetermine how these inputs are transformed into the (interpretable)
quantities required by the attention mechanism.
The individual atrepresent the “intermediate” results of the computation—the results of applying
the actual attention mechanism (with a slight modification) from Definition 5.3.4 onto the produced
values of the query, the keys, and the values.
The modification mentioned refers to the addition of the inputs xtto the output of the attention
mechanism in Eq. (5.120). This mechanism is known as adding residual connections to the model.
First introduced by He et al. (2016) in the context of deep convolutional neural network-based
architectures, residual connections are now a common feature in many state-of-the-art deep learning
architectures. Note that their use is mostly motivated by empirically better performance—this is
often attributed to the fact that, intuitively, residual connections allow gradients (i.e., learning signals)
to flow through the network through a more direct route rather than all layers that can “squish” the
signal similarly to the Elman RNN case (in that sense, they help mitigate the vanishing gradient
issue). However, as we will see later in our analysis, residual connections will also play an important
role in determining the theoretical properties of transformers, particularly their computational
expressive power. The same mechanism is applied in the second step of the transformer layer, where
the intermediate results atare transformed by the output transformation Ointo the final outputs of
the layer.
In the simplest case, you can imagine the inputs Xto be the initial static embeddings of the
symbols. The application of the transformer layer, in this case, therefore, “selectively” (determined by
the attention mechanism) augments the static embeddings with the information from the preceding
context. However, as we will see shortly, a transformer model will apply multiple transformer
blocks to the input sequence and thus transform it in multiple steps, analogously to how layers are
composed in a regular feed-forward neural network. In that sense, the inputs to the transformer
blocks will refer to general intermediate representations produced from the initial static embeddings
5.3. TRANSFORMER-BASED LANGUAGE MODELS 215
after some number of applications of transformer layers.
Lastly, let us consider how the current symbol representations Xare transformed into the queries,
keys, and values using Q,K, andV? The original formulation (Vaswani et al., 2017) and all standard
implementations of the transformer architecture use one of the simplest possible mappings: a linear
transformation implemented by matrix multiplication. On the other hand, the final output of the
transformer, computed with output mapping O, is usually implemented by a multi-layer perceptron.
The transformer layer puts the attention mechanism into a functional block that describes how
a sequence of current symbol representations is transformed in a single step into a sequence of
representations augmented with the information in the current set of values. However, we are not
done abstracting yet! As mentioned, this process can be applied arbitrarily many times, resulting
in “deep” encodings of individual symbols, which contain information from the preceding symbols
in the string computed in a composite way. This also answers the question of how the augmented
symbol representations used in the language modeling formulation are computed from the initial
symbol representations: they are the result of multiple applications of the transformer layer to the
matrix of initial symbol representations. That is: multiple transformer layer layers are stacked on
top of one another so that the output of one layer becomes the input of the next.
We now have all the building blocks to define the full transformer architecture, which computes
the encodings of the string prefixes (and thus the hidden states) in Definition 5.3.3.
Definition 5.3.9: Transformer
ForLPN, we define a L-layer transformer model as a D-dimensional transformer sequence
model over an alphabet Σ where the hidden state htdef“encTpy1...ytq“encTpyqis computed
as follows.
X1def“`
e1py0q,e1py1q,...,e1pytq˘
(5.122)
Zℓ“TℓpXℓqfor 1ďℓăL (5.123)
Xℓ`1“Zℓfor 1ďℓăL (5.124)
ht“FpzL
tq (5.125)
Tℓforℓ“1,...,L representLdifferent transformer layers with decoupled parameter (cf.
Definition 5.3.8). F:RDÑRDis a transformation function applied to the contextual
encoding of the last symbol in the last ( Lth) layer and e1:ΣÑRDis a symbol representation
function computing the initial representations of the symbols passed to the first layer of the
transformer.a
aThe symbol representation function e1is often also implemented as a linear transformation of the one-hot
representations (cf. Definition 5.1.5) of symbols, i.e., it is simply a table-lookup.
With this, the transformer model now fully specifies how to compute the representations
required for the representation-based locally normalized sequence models from Definition 5.3.3—the
representation function encTis the composition of Ltransformer layers applied to the sequence of
static encodings, followed by a final transformation F.
216 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
y0y1y2y3pSMpy4|h3q pSM`
yt`1|ht˘
h3 ht
yt´1ytLocally-normalized
distribution
Encoding enc T
Lapplications of
the transformer
layer
Input y ¨¨¨¨¨¨
Figure 5.20: encTpyďtqis a function of the symbols y1,...,ytcomputed with multiple applications
of the transformer block. Here, the dashed lines illustrate the dependencies of the outputs hton
the initial static encoding of the symbols ydenoted by green nodes. Na¨ ıvely, h3andhtcould be
computed by independently applying the attention mechanism from Definition 5.3.4. However, as we
describe in the text, while the applications of the attention mechanism do not share computations,
they can be written concisely together.
Making Attention Work Fast Over Entire Strings
Notice that, in the formulations so far, we always presented computations of the attention mechanism
for individual queries qt. This corresponds to the computation of the new version of the representation
of asingle symbol in the string, with the keys and values representing the preceding symbols (including
the symbol itself). This is illustrated in Fig. 5.20. This could of course be applied |y|-times to
compute the representations of all |y|symbols in a single transformer block. However, this would
unnecessarily re-compute the keys and values of the symbols multiple times—and, as we motivated
at the beginning, speedups were one of the main reasons to talk about transformers in the first
place. We can now show how the attention mechanism can be conveniently applied to entire strings
at once. Specifically, we focus on the case where the attention scoring function fis implemented as
a dot-product.28
What does the attention mechanism from Definition 5.3.4 do in this case? Given a query
qtand a matrix of key values K“`
kJ
1,...,kJ
t˘
PRtˆD, the scoring function simply computes29
uj“fpqt,kjq“qJ
tkj.
Notice that, in this case, the vector u“pu1,...,utqofunnormalized attention weights can simply
be computed as a single matrix-vector product
u“qJ
tKJ.
28For conciseness, we will ignore the scaling factor, which could easily be added.
29We switch notation from xqt,kjytoqJ
tkjto make the connection to matrix multiplication later clearer.
5.3. TRANSFORMER-BASED LANGUAGE MODELS 217
Furthermore, with this, attention can be easily extended to consider many queries in parallel by
stacking multiple queries into a matrix Qdef“`
qJ
1,qJ
2,...,qJ
t˘
, as we detail now.30Consider now
the product
U“QKJ.
Each entry of the resulting matrix Uijis exactly the dot-product between the query qiand the
keykj! The rows ofUthen contain the unnormalized score vectors uifrom the definition of the
attention mechanism. This means that if we now apply the normalization function f∆D´1row-wise
(such that the sums of the elements in each row equal 1), we end up with exactly the required
normalized scores required for combining the values from the value matrix. With some abuse of
notation, we will simply write that as
Sdef“`
sJ
1,...,sJ
t˘def“f∆D´1pUq“f∆D´1`
QKJ˘
. (5.126)
The rows of f∆D´1pAq, therefore, represent the normalized attention weights. This brings us to the
final step of the matrix-multiplication-based attention mechanism: Combining the values based on the
computed attention weights. Again, this can be performed by a single matrix multiplication. Notice
that the value vectors are the same for all queries—they are simply combined with different (attention)
weights based on the query. Right-multiplying the transposed values matrix V“`
vJ
1,...,vJ
t˘
with
S, therefore, perform the convex combination of the value vector vJ
1,...,vJ
tsuch that
ai“siVJ“Si,:VJ(5.127)
and thus
Adef“pa1,...,atq“SVJ. (5.128)
Altogether, this means that, given a sequence of (contextual) symbol encodings X, we can compute
the attention values (i.e., the output of the attention mechanism) of allqueries (i.e., for all string
in the string) with a single matrix multiplication, as long as the scoring function is the (scaled)
dot-product. We refer to this version of attention as an attention block, which, intuitively, simply
replaces the element-wise definition of the attention mechanism from Definition 5.3.4 with a more
efficient (and concise) definition through matrix multiplications.31
Definition 5.3.10: Attention Block
LetQ,K, andVbe parametrized functions from RTˆDtoRTˆDandXPRTˆDthe matrix
of input encodings. An attention block is the function A:RTˆDÑRTˆDdefined as
ApXq“f∆D´1´
QpXqKpXqJ¯
VpXq (5.129)
Further, we define the attention matrix as the square matrix Udef“QpXqKpXqJPRTˆT.
30Note that, for easier presentation, we make a slight departure from the original definition of the attention
mechanism, where the result of the attention mechanism for query tonly depended on the keys and values jďt. For
the rest of the paragraph, we assume that a query qiwithiătcan consider keys and values kjandvjwithjąi,
which, in the interpretation of attention applied to strings, would mean that the symbols can “look ahead” in the
string to their right. This will be addressed shortly with masking .
31Again, with the caveat that the attention weights are not confined to the preceding symbols but to all symbols in
the string.
218 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
ht
y1y2¨¨¨ yt¨¨¨ yT´1yT
k1,v1 k2,v2 kt,vt kT´1,vT´1kT,vTEncoding enc T
Lapplications of
the transformer
layer
Input y
Figure 5.21: In the context of language modeling, the attention mechanism is allowed to consider the
symbolsyjand their keys/values for jďt(the green dashed lines) when computing the contextual
encoding of the symbol yt. In Definition 5.3.4 this is enforced by the definition of the matrices Ktand
Vt. However, in the attention block formulation of the attention mechanism from Definition 5.3.10,
since the matrices KandVcontain the values corresponding to the entire string y, the query qt
could, in principle, index into the values corresponding to the symbols yt`1,...,yT(the red dotted
lines). Masking prevents that by enforcing the attention weights at`1,...,aTto be 0. In this sense,
it removes the red dotted lines.
As mentioned, the functions Qp¨q,Kp¨q, andVp¨qare usually implemented as a linear trans-
formation via matrix multiplication using weight matrices WQ,WK, and WV. This means that
the query matrix Qcan be computed as Q“XWQ, where WQPRDˆDis a matrix of learnable
parameters. Since the attention block uses the same input matrix Xto encode queries, keys, and
values, it is usually called self-attention .
Confining attention to preceding symbols in the string. We now address the departure
from the original definition of the attention mechanism in which the query qtwas only allowed
to consider the keys and values kjandvjwithjďt. Notice that, in general, the version of
attention of Eq. (5.129) allows each symbol to attend to anysymbol in the string, even those in
later positions in the string. Note that there is nothing inherently wrong with that—the contextual
symbol encodings could, in principle, depend on the information from the entire string. In fact, this
is very common in the so-called masked language modeling , which, importantly, despite its name,
does not define language models in our sense of the word. A very commonly used family of masked
models is the BERT family of models.32However, in the case of locally normalized language models,
“looking ahead” obviously violates the autoregressive structure of language modeling, i.e., violates
our assumption that the context at time tforms yăt. This is illustrated in Fig. 5.21. To recover the
autoregressive nature of the language model, we, therefore, posthoc modify Eq. (5.129) to allow
each symbol to attend only to itself and to preceding symbols, while still being able to implement it
using matrix multiplication. We do that by adding a mask to zero out the unwanted elements of U.
32In a very unfortunate, but also understandable, turn of events, we mention two completely different notions of
masking in a single paragraph. Importantly, the masking in masked language models (which, again, are not language
models in our strict definition) has nothing to do with the “causal” masking relevant for autoregressive language
modeling which we introduce in this section.
5.3. TRANSFORMER-BASED LANGUAGE MODELS 219
Definition 5.3.11: Masked Attention Block
LetQp¨q,Kp¨q, andVp¨qbe parametrized functions from RTˆDtoRTˆD. Amasked attention
block is a function ApX,Mq:RTˆDˆRTˆDÑRTˆDdefined as
ApX,Mq“softmaxpQpXqKpXqJdMqVpXq (5.130)
wheredis the element-wise product between matrices, and MPRℓˆℓ, themasking matrix ,
is constructed as follows.
Mi,j“#
1 ifiďj
´8otherwisefor 0ďi,jăT (5.131)
This implements a very easy “fix” to the looking-ahead problem by simply putting the normalized
attention scores of the “illegal” elements to 0. In general, the exact value of the elements Mi,jwith
iąjof course depends on the projection function f∆D´1—for simplicity, we only define Mfor the
case where f∆D´1“softmax.
Bits and Bobs of the Transformer Architecture: Positional Encodings, Multiple Heads,
and Layer Normalization
Let us now take a step back and consider what the transformer model introduced so far does abstractly.
A transformer takes as input as string yPΣ˚, computes the initial symbol embeddings X1“X, and
transforms those through a sequence of Lapplications of the transformer layer (cf. Definition 5.3.9).
This results in the final augmented (contextual) symbol representations ht“encTpyďtq, which are
then used to compute the conditional probabilities in the representation-based locally normalized
language model defined by the transformer (cf. Definition 5.3.3), as illustrated on top of Fig. 5.20.
In this subsection, which will finish off our formal definition of the architecture, we introduce the last
three components often connected closely to the transformer model: Symbol positional encodings,
multi-head attention, and layer normalization.
Adding positional information into the transformer architecture. There is an important
omission we still have not addressed when talking about transformers: How does the model
incorporate any notion of word order into the contextual representations of symbols or the encodings
of the context ht? The motivation is very clear: The meaning of a sentence depends on the word
order. The meaning of “A dog bit a man.” is not the same as “A man bit a dog.” . This is one of
the reasons why simple “sentence encoding” functions such as bag-of-words, which simply represent
sentences with the number of individual words they contain, do not work well. A careful reader might
have noticed that at no point in our discussion about transformers and the attention mechanism
did we say anything about the positions of the words. Importantly, we did not talk about word
positions in the case of RNNs either. However, the sequential and incremental processing nature of
RNNs makes it easy to “manually” keep track of the position of the current symbol of the string
yt, to the extent that the RNN variant is capable of “counting” (cf. §5.2). However, all operations
composing the transformer model are position-agnostic : The convex combination of the value vectors
Vwill be the same, no matter the permutation of the vectors (if we, of course, accordingly permute
the keys). The keys also cannot contain any positional information, since they are computed from
220 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
position-agnostic static embeddings and a transformation function Kwhich does not depend on the
position.
All that is to say that, to be able to take into account word order in a transformer, we have to
explicitly provide the positional information to the model. The simplest way to do this is to augment
the static symbol encodings in the first transformer layer with positional encodings in the form of
vectors which can be added to or concatenated to the static encodings of symbols (Vaswani et al.,
2017).33
Definition 5.3.12: Positional encoding
Apositional encoding is a function fpos:NÑRD.
This is a very simple definition: A positional encoding simply assigns a position in a string
a vector representation. A trivial example would be fposptq “ ptq. This allows us to define a
position-augmented symbol representation function.
Definition 5.3.13: Position-augmented representation function
Lete1:ΣÑRDbe a symbol representation function and fpos:NÑRDa positional encod-
ing. A position-augmented representation function of a symbol ytin a string yis the
representation function e1
pos:ΣÑRDdefined as
e1
pospytqdef“e1pytq`fposptq. (5.132)
To make the positional information available to the transformer model, we now simply pass the
position-augmented “static” symbol encodings Xposto the model instead of the original ones X.
Apart from that, the transformer model can remain unaltered, and function simply as defined above,
taking into account the positional information now included in its inputs. Importantly, the intuitive
notion of the importance of positional encodings for understanding natural language also transfers to
the computational power of the model: Transformers as introduced in this section without positional
information are strictly less powerful than those with positional information (P´ erez et al., 2021).
Again, this intuitively makes sense: Without positional information, a transformer model could not
even recognize the simple (unweighted) regular language
L“tabn|nPNu
since it would have no way of knowing, provided that ais in a given string y, whether it appears in
the first position or in any other position in the string.
Multiple heads. Importantly, the transformer introduced so far computes a single set of contextual
representations—one for every input symbol (at every layer of the transformer). However, we can
easily extend the model to compute multiple contextual representations for each symbol. This
is done using the so-called multi-head attention , where a single attention block is called an
33For simplicity, we assume the positional encodings are added to the static ones; notice that by dividing the
D-dimensional vectors into two components, one responsible for the static encodings and one for the positional
ones (where the positional encoding component is zeroed out in the static encoding and vice-versa), one can easily
implement “concatenation” of the two representations using only addition.
5.3. TRANSFORMER-BASED LANGUAGE MODELS 221
attention head . This increases the representation space of the individual symbols and thus enables
the model to capture more information about the symbols and the sentence. The interpretation of
computing multiple representations (one for each head) independently also invites the interpretations
that each of the heads “focuses” on a separate aspect of the text. To be able to use the outputs of
multi-head attention as inputs to the next block again, the outputs of the different attention heads
are then concatenated and then projected down to the output size of a single attention block using
an additional transformation.
Definition 5.3.14: Multi-Head Attention Block
LetHPNbe the number of attention heads, Qhp¨q,Khp¨q, andVhp¨qbe parametrized functions
fromRTˆDtoRTˆDfor 0ďhďH, and fH:RT¨HˆDÑRTˆDbe a parametrized function.
Amulti-head attention block is a function MH-ApXq:RTˆDÑRTˆDdefined as
MH-ApXq“fHpconcat 0ďhăH´
softmax´
QhpXqKhpXqJqVhpXq¯¯
(5.133)
While multi-head attention is mostly motivated by empirically better performance (and the
intuitive motivation of being able to separately focus on different notions of similarity), it will have
some implications on the computational power of the model as well. As we will see shortly, having
multiple heads makes it very easy to reason about how a transformer model can simulate an n-gram
model.34
Layer normalization. As a final component of a transformer, we mention layer normalization.
Layer normalization, similar to the use of residual connections, represents a common “trick” in the
deep learning space for ensuring more stable and reliable gradient-based learning—as such, it is not
limited to transformers. Formally, we can define layer normalization as follows (Ba et al., 2016).
Definition 5.3.15: Layer normalization
Letx,γ,βPRD, andϵą0. The layer normalization function LN:RDÑRDis defined as
LNpx;γ,βqdef“x´xa
σ2pxq`ϵdγ`β, (5.134)
where xrefers to the mean of the vector x(and is subtracted from all elements of xin the
formulation above) and σ2pxqrefers to the variance of elements of x.ϵis added in the
denominator to ensure stability if σ2pxq!1.
Intuitively, the application of the layer normalization function ensures that the mean of the vector
xis (approximately) βand its variance is controlled by γ(after being standardized by dividing by
the standard deviation of x). Most commonly, we simply set γ“1PRDandβ“0PRD.
Layer normalization is most commonly applied to the output of the transformer layer (on every
layer), i.e., to ziin Eq. (5.121). The full output of the transformer layer is therefore computed as
zidef“LNpOpaiq`ai;γ,βq. (5.135)
34This does not, however, mean that multiple heads are required for recognizing n-gram models. As we will see,
under some caveats, single-head transformers are Turing complete.
222 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
Interestingly, although we mentioned that layer normalization is mostly motivated by the stability it
brings to training and with it better performance, it does, just like multi-headed attention, seem to
contribute to the computational expressivity of transformer models. As we will see in §5.4, layer
normalization allows for a simple fix that solves one of the best known formal limitations of the
transformer architecture (again, under some assumptions) (Hahn, 2020; Chiang and Cholak, 2022).
Connecting the Formal Definition Back to our Desiderata
This brings us to the end of the formal definition of the transformer architecture. As we saw, a
transformer has many more moving parts than an RNN. Is the additional complexity warranted?
Let us now return back to the informal motivations or desiderata that we laid out in §5.3.1 and see
how the components we defined in this section come through and ensure transformers fit them. First
of all, the transformer layers clearly store the representations of all symbols at all times—they are
all needed to produce the query, key, and value matrices required by the attention mechanism. As
mentioned above, this allows us to easily store information about the entire string in a convenient
and “accessible” format without having to compress it into a single hidden state. Furthermore, the
fact that the contextual representations Zpℓ`1qtare computed from the representations xℓ
1,...,xℓ
t
directly at every time step, with no direct dependence between the different zpℓ`1qi, means that
these computations can easily be parallelized. More concretely, in the most common implementations
of the transformer components, most operations take the form of matrix multiplications, which
makes the computation and parallelization that much more efficient.35Again, note that here, we
are only interested in parallelizing the processing of entire strings, as for example given in a training
corpus. As discussed in §5.1.5, there is an aspect of language modeling that is inherently sequential
and even heavily parallelizable architectures such as the transformer cannot overcome: Generating
stings one symbol at time. While generating strings, even a transformer model will have to generate
symbols one at a time and, therefore, recompute (parts of) the encoding enc Tpyďtqanew at every
time step to generate the next symbol. The advantages of parallelizability, therefore, come only
at training time—however, given the vast corpora used for training today’s models, this makes a
crucial difference in the applicability of the architecture over recurrent ones.
Altogether, this means that transformers do, indeed, achieve the desiderata from our informal
motivation! This concludes our formal definition of transformers. We move to analyze their
theoretical properties.
5.3.3 Tightness of Transformer-based Language Models
Having introduced transformers formally, we can start investigating their formal properties. As we
did for RNN LMs, we first consider their tightness. Specifically, in this subsection, we show that
allsoft attention-based transformer language models are tight. Key to our proof of the tightness of
transformer language models, as well as the tightness of various other neural architectures, is the
following basic fact in topology.
35Note that, there is, of course, some sense of recurrence in the transformer—the composition of the transformer
layers, which are stacked on top of each other, and of course require sequential computation. However, crucially, the
number of layers does not depend on the length of the string —the number of sequential steps required to process a
string, therefore, does not depend on its length, which is what we wanted to achieve.
5.3. TRANSFORMER-BASED LANGUAGE MODELS 223
Theorem 5.3.1: Compactness
LetXbe a compact topological space and Ybe any topological space. If f:XÑYis
continuous, then fpXqĎYis also compact.
Proof. LettUsusPAbe any open cover of fpXq. By continuity, f´1pUαqĂXis open for any αPA,
and hencetf´1pUsquαPAis also an open cover of X. By the compactness of X, there is a finite
sub-covertf´1pUαnquN
n“1, in which case tUαnuN
n“1forms a finite sub-cover for fpXq. ■
We now further mathematically abstract transformers as a function on vector tuples,36fAtt:`
RD˘`Ñ`
RD˘`, that is length-preserving in the sense that fAtt`
RtˆD˘
Ď`
RtˆD˘
for alltą0. Intuitively,
this definition is saying that fAttis a function that maps a nonempty vector tuple tvjut
j“1to another
vector tuplethjut
j“1of the same length,
fAttpv1,...,vtq“ph1,...,htqPRtˆD, (5.136)
where vj“e1pyjqPRDare the initial representations of the input symbols yj. In particular, we
can take the function fAtt:`
RD˘`Ñ`
RD˘`to be the function defined by a stack of transformer
layers, i.e., an attention block. This setup will help us state the following.
Lemma 5.3.1
LetfAtt:`
RD˘`Ñ`
RD˘`be the function defined by a Ltransformer layers with continuous
functionaQ,K,V, andO. Given a compact set KĂRD. Then, there exists a compact set
K1ĂRDsuch that for every tPZą0,
fAtt`
Kt˘
Ď`
K1˘t. (5.137)
Note. We make use of the following notations in the proof below: Brpzq“tvPRD:distpz,vqăru
denotes the open ball centered at zwith radius r;Adenotes the closure of set A.
Proof. LetK0“K. In an autoregressive transformer, each of the Llayers consists of two blocks: a
self-attention block and a feedforward block. We will use induction on the 2 Lblocks to build up
compact sets K1,K2,...,K2Lthat contain the output vectors of these respective blocks, and then
takeK1“K2L.
The self-attention block is a function on pRDq`ÑpRDq`. So, lettPZą0be arbitrary and
consider any sequence of input vectors pv1,...,vtqsuch that for all i,viPK0. Denote the output
vectors of the attention block with pv1
1,...,v1
tq. By definition of attention, each output vector
v1
j“řt
i“1spjq
iviwhere spjqP∆t´1are the attention weight vectors obtained through the softmax
function. Compact sets in RDare bounded (by the Heine–Borel theorem), and hence there exists
36Here`
RD˘`is the set of nonempty tuples of vectors in RD. This is formally the disjoint union (coproduct)š
tPZą0RtˆD.
224 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
Mą0 such that K0ĎBMp0q. Noting that the norm function }¨}onRDis convex, we have the
following
}v1
j}“›››››tÿ
i“1spjq
ivi›››››(5.138a)
ďtÿ
i“1spjq
i}vj} (˚)
ďtÿ
i“1spjq
iM“M (5.138b)
where (˚) results from Jensen’s inequality. Eq. (5.138b) shows that each of the output vectors v1
j
lies inBMp0qwhich is compact. Hence, setting K1“BMp0q, we have shown that, for any tPZą0,
the attention block maps Kt
0intoKt
1.
Note that we cannot use Theorem 5.3.1 here because the attention block defines a different
function on RtˆDÑRtˆDfor eacht, and Theorem 5.3.1 only implies that there exists a separate
length-dependent output compact set KtĂRtˆDfor eacht, which is different from this lemma’s
statement.
The feedforward function is a continuous function on RDÑRD, and therefore, by Theorem 5.3.1,
maps its input compact set K1to an output compact set, which we call K2.
Finally, residual connections and layer norms are also continuous functions acting on each of the
input vectors, and hence by the same reasoning would also preserve compactness.
Now we can use induction and show that there exist compact sets K3,K4,...,K2L´1,K2Lwhere
K2Lcontains the output set of the final layer. Set K1“K2Land we have proven the statement. ■
Now recall that a transformer language model with the softmax projection function (Defini-
tion 5.3.3) defines the conditional probabilities using the softmax transformation
pSMpyt|yătq“exppepytqJhtq
ř
y1PΣexppepy1qJhtq(5.139)
where epyq PRDis the output symbol embedding of yPΣandhtis defined from the input
embeddings of yătvia Eq. (5.136). Using Lemma 5.3.1, together with the finiteness of the vocabulary
Σ and the continuity of the softmax transformation (5.139), readily yields our main result on
transformer language models.
Theorem 5.3.2: Transformer language models are tight
The representation-based locally normalized language model (cf. Definition 5.3.3) defined by
any (fixed-depth) transformer with soft attention is tight.
Proof. Given the Transformer, there exists a fixed compact set Kthat will contain all inputs viPRD
to the first layer. This is true because each viis the sum of a word embedding, which falls in a
finite set since Σis finite, and a position embedding, which lies in the compact set r´1,1sD. Hence,
by Lemma 5.3.1, there exists a fixed compact set K1that contains all output embedding vectors
(regardless of how long the sequence is).
5.4. REPRESENTATIONAL CAPACITY OF TRANSFORMER LANGUAGE MODELS 225
1 210,1
Figure 5.22: An FSA recognizing the language First“tyPΣ˚|Σ“t0,1u, y1“1u.
The final output probability is given by a multiplication with the word embedding matrix followed
by the softmax function as in Eq. (5.139). This process amounts to composing two continuous
functions. In particular, we can extract the eosprobability as a continuous R-valued function
geos:K1Ñp0,1q(neither 0 nor 1 is in the range of the softmax function). By continuity of geos
and Theorem 5.3.1, K2def“geospK1qĎp 0,1qis compact. Since K2is compact, and hence closed,
infK2PK2. Thus infK2Pp0,1qand in particular infK2ą0. Therefore, taking ϵ“infK2, we have
shown that the eosprobability of a Transformer is bounded below by some ϵą0 (regardless of the
length of the sequence). Hence, by Proposition 2.5.6, any transformer-based sequence model is tight
and thus defines a language model. ■
5.4 Representational Capacity of Transformer Language Mod-
els
So far, we have introduced our formal definition of the transformer architecture and examined its
tightness. We now move on to the computational power of the architecture. This section mirrors
§5.2 and examines the expressivity of the transformer language model as defined in Definition 5.3.3.
Transformers are a much more recent architecture than recurrent neural language models, and
our theoretical understanding of them is thus much more limited. However, over the last few years,
a series of results showing various properties of the transformer model have been established. At
first glance, one might find a number of contradictions among them: One of the first results shows
that transformers are not even able to recognize the very simple First language recognized by
the (unweighted) finite-state automaton shown in Fig. 5.22 nor the Dyck language. On the other
hand, there is work showing that transformers canrecognize the majority language (determining
whether a string contains more symbols aorb) and can even count : Both of these languages are
instances of non-regular languages. Moreover, a fundamental result by P´ erez et al. (2021) even
shows that transformers are Turing complete . Upon closer inspection, the results can be explained
by the different theoretical abstractions of the original transformer model that the different works
make and even different notions of equivalence. Even very subtle differences in the model can lead
to substantial differences in the expressivity of the model, as we will see below. In this section,
we present some original results which show that transformers can, with infinite precision, in fact,
reach up to the top of the hierarchy of formal languages that we consider in these notes: They are
Turing complete. We also comment on the differences in our approach to the work so far (the main
difference and novelty is that we embed the analysis into our language modeling framework) and try
to unify it. We will show that infinite-precision transformers can simulate the recognizers across
the entire hierarchy: (weighted) finite-state automata, pushdown automata, and Turing machines.
Luckily, apart from the first construction, the proofs and constructions in our ascent up the hierarchy
will be based on a unified approach in which we build on P´ erez et al. (2021) and sequentially add
226 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
components to be able to recognize more and more complex languages.
Besides being more novel and thus less researched, transformers are also less intuitive to think
about as sequential machines transitioning between states as with finite-state or pushdown automata.
All classical computational models we introduced (finite-state automata, pushdown automata, and
Turing machines) rely on some notion of an internal state which is sequentially updated, where
the next state is determined based on the current configuration. We also said in §5.1.5 that this
sequentiality is the Achilles’ heel of the ability to parallelize and thus speed up the computations
in a language model. One of the main motivations for defining the transformer model is to avoid
these sequential dependencies and to make sure the contextual representations of the individual
symbols can be computed independently. However, the lack of sequentiality in transformers makes it
more difficult to compare to classical and well-understood models of computation—they simply do
not define any notion of a configuration that would be passed over by reading a symbol at a time,
and relating the configurations at different time points to the configuration of some classical model
of computation was the main idea of most of the analyses in §5.2. This will not be possible with
transformers, and we will have to be more clever about it to draw parallels to better-understood
formalisms. What is more, it seems like their parallelizable nature is one of the reasons for the lower
(or, at least, ambiguous) computational power under some formalisms , as covered in Merrill et al.
(2022a); Merrill and Sabharwal (2023).
A word on model equivalence. As mentioned above, the nature of the transformer architecture
does not lend itself well to a straightforward comparison to classical models of computation. To
make the connection, we will have to be somewhat clever about the analysis. As we will see shortly,
we will mainly deal with this in two ways: (i)By foregoing any notion of a state of a machine in
case of n-gram language models37and (ii)byembedding the state of a computational model into
the alphabet itself —the model will then use the augmented output alphabet to keep track of its
state in the string itself without relying on any notion of its own internal state which would have
to be updated sequentially.38How can this help us? As will become clear in our analysis of the
Turing completeness of a transformer model, the model can use the generated string as a sort of a
sequential memory structure . Because the transformer model can look back at the entirety of the
string when computing encTpyďtq(where yďtis the augmented string generated so far), it is able
to “read off” its internal state from the string. Importantly, the generated string will still contain
the information about the generated string, besides including the state of the computational model.
As the transformer will then compute the new embeddings encTpyďtq, it will be able to account for
the state it should be in. This is illustrated in Fig. 5.23.
While this might seem like a convenient trick to achieve Turing completeness—and in many ways,
it is—it is also, in a way, cheating. This “cheating” can be described formally as the difference between
model equivalence and homomorphism equivalence . When we discussed the Turing completeness of
RNN LMs, we showed they can model a Turing machine by directly recognizing the same strings
(for the time being, we ignored the string weights). This means that, for every Turing machine
M, there exists an RNN Rwhich recognizes the same language: LpRq“LpMq. However, we
will not be able to make statements like this in the case of transformer models. The augmented
37Reminder that n-gram language models are in fact subregular (cf. §4.1.5) and we make use of that in our analysis.
Because their recognition relies purely on local patterns in the strings, and a transformer model has the ability to
consider large enough substrings, we will see that we can model an n-gram language model without keeping any
notion of a state in a transformer
38Recall that, as discussed in §5.1.5, generation is inherently sequential. One can thus imagine augmenting the
alphabet as a sort of exploitation of this sequential nature.
5.4. REPRESENTATIONAL CAPACITY OF TRANSFORMER LANGUAGE MODELS 227
Original tape of the
Turing machine
Augmented tape of
the stateless modely1y2y3y4¨¨¨yt¨¨¨
y1q1y2q2y3q3y4q4¨¨¨ytqt¨¨¨
Figure 5.23: An abstract illustration of how a model can keep track of its internal state by “outputting”
it into the generated string. By reading the augmented symbol generated at time t, the model can
then determine its internal state.
alphabet will instead bring us to a statement of the sort “For every Turing machine M, there
exists a transformer Twhich recognizes the same language augmented with the state set of the
Turing machine: L∆pTq“LpMq,” whereL∆refers to the language of strings where each symbol
is additionally augmented with the state of the Turing machine. This might seem like a small
difference, but, in formal language theory, homomorphism equivalence refers to a different problem
to that of normal model equivalence (Culik and Salomaa, 1978) and thus has to be considered
differently. Intuitively, it additionally allows additional information to be stored in the strings (in
our case, that will be the state of the Turing machine) while still considering some models to be
“equivalent”. Formally, model equivalence asks the following question.
Definition 5.4.1: Model equivalence
Two computational models C1andC2areequivalent if
LpC1q“LpC2q. (5.140)
On the other hand, homomorphic equivalence considers the following.
Definition 5.4.2: Homomorphism
LetC1andC2be two computational models. C1ishomomorphically equivalent toC2if
there exists a homomorphisms h:LpC1qÑLpC2qsuch that
hpLpC1qqdef“thpyq|yPLpC1qu“LpC2q. (5.141)
Definition 5.4.3: Homomorphic equivalence
LetC1andC2be two computational models. C1andC2arehomomorphically equivalent if
C1is homomorphically equivalent to C2andC2is homomorphically equivalent to C1as per
Definition 5.4.2.
228 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
Transformers and the Inability to Recognize Simple Languages
We start our exploration of the computational power of transformer models with some negative
results, which we will later “correct” by using our formalization of a transformer model and different
components of the transformer architecture (for example, a different form of attention). Given
their success at modeling human language which is assumed to be at least mildly context-sensitive
(Huybregts et al., 1984; Shieber, 1985), it seems surprising that transformers cannot, in fact, recognize
some very simple regular languages, such as Parity orFirst (the FSA shown in Fig. 5.22), as well
as simple non-regular context-free languages such as Dyck languages:
First“tyPΣ˚|Σ“t0,1u, y1“1u
Parity“tyPΣ˚|Σ“t0,1u,yhas odd number of 1s u
Dyck“tyPΣ˚|Σ“tp,qu,yis correctly parenthesized u
This has been formally shown by Hahn (2020), and experimentally verified by Chiang and Cholak
(2022). Bhattamishra et al. (2020) found that transformers especially struggle to learn any languages
that require counting occurrences in some way, such as the number 0s and 1s in Parity or the
number of previous open and closed parentheses in Dyck . Hahn (2020) finds that with unique
hard attention , these languages cannot be recognized: Recognizing them by a transformer in their
formulation would require the number of parameters to increase with the length of the input. Chiang
and Cholak (2022) consider the setting with soft attention, where the issue is more subtle: In theory,
it is possible for a transformer to recognize languages such as First andParity , however with
less and less confidence as the length increases. This is reflected by the cross-entropy of deciding
language membership approaching the worst possible value of 1 bit per symbol. The reason behind
this is quite intuitive: The membership of any of the languages defined above changes if asingle
symbol changes. However, by examining the information flow in a transformer, one can show that
the corresponding information gets less and less weight relative to the length of the string due to
the attention mechanism averaging over all positions.
Transformers Can Simulate n-gram Models
§5.4 showed that transformer models can struggle to recognize some of the simplest formal languages.
While we did not discuss those results in detail, intuitively, they stem from the use of unique
hard attention and the resulting inability to take into account all values whose keys maximize the
attention scoring function. By relaxing that restriction to averaging hard attention, the model
becomes more expressive. To show that, we begin by looking at the very simple n-gram language
models, as defined in §4.1.5. By constructing, for any n-gram model, a transformer representing it,
we will show the following theorem.
Theorem 5.4.1: Transformer language models can simulate n-gramlanguage models
LetpLNbe an n-gram language model. Then, there exists a transformer TwithLppLNq“LpTq.
Alternatively, we could say that transformers can recognize strictly local languages (cf.§4.1.5).
Proof. We prove the theorem by constructing, for pLN, a transformer TwithLppLNq“LpTq. Note
that we will mostly restrict the proof to the construction of the transformer, i.e., the formal definition
5.4. REPRESENTATIONAL CAPACITY OF TRANSFORMER LANGUAGE MODELS 229
y1y2¨¨¨yt´3yt´2yt´1yt¨¨¨Head 3 Head 2 Head 1fHpSMpyt|yătq
E
Figure 5.24: An abstract depiction of how a transformer can simulate an n-gram model using n´1
heads (here, n“4). The stronger arrows from the heads to the symbols in the string show where
the heads concentrate their attention. The lighter green arrow is meant to represent that the heads
still can consider the entire history of the input so far but are then configured such that they only
look at the appropriate position.
of its parameters. The (mostly trivial) mathematical details and derivations are left as an exercise
to the reader.
Recall that, by definition, an n-gram language model considers a fixed number of previous
symbols to define pSMpyt|yătq—exactly n´1 of them. The constructed transformer Twill capture
this idea with n´1heads , each of them attending to exactly one of those positions in the previous
n´1 positions.39We can then use the symbols the heads attended to (and thus identified) to
identify the current n-gram and with it define the relevant conditional distribution over the next
symbol. To be able to attend to the positions of interest—the ones containing the previous n´1
symbols—we have to make use of appropriate positional encodings (cf. Definition 5.3.12), which
will allow the model to attend to them. The idea of the construction is abstractly illustrated in
Fig. 5.24.
For hopefully a better pedagogical effect, we will present this proof from the “last” part of the
construction to the “first”. We, therefore, start with the final step: Assuming we have identified
the appropriate n-gram yt´n:t´1yt, how can we encode the conditional probability distribution
pSMpyt|yt´n:t´1q? The construction we use here directly mirrors the one in Minksy’s construction
(cf. Lemma 5.2.2): Knowing what the individual pSMpyt|yt´n:t´1qforytPΣare (those are, as
described in §4.1.5, “hard-coded”, or specified, for each n-gram separately in a look-up table), we
can simply put their logits ( logprobabilities; in case we are using the softmax projection function)
or the probabilities directly (if we are using the sparsemax projection function) into a vector and
39Note that, given an n-gram model, the number nis fixed. This means that, for a given n-gram we can always fix
the number of heads and therefore construct such a transformer.
230 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
concatenate all the constructed vectors (for all possible n-grams) into a large matrix E.
If we one-hot encode the identified n-gram by defining
encTpyătqdef“Jyt´n:t´1K (5.142)
we can then, using the formulation of the transformer sequence model from Definition 5.3.3, use the
one-hot encoded n-gram to lookup the appropriate column containing the conditional probabilities
given the identified n-gram for all possible ytPΣ.40. The formal proof of correctness given that we
have identified the correct n-gram is therefore analogous to the final part of the Minsky construction.
We now consider the preceding step of the simulation: How can we identify the complete n-gram
given that the n´1 heads of the transformer identified the symbols in the positions they attended
to? This, it turns out, is a simple instance of the “ AND” problem investigated in Fact 5.2.3: After
concatenating the values of the n´1 heads into a common vector v(each of which is a |Σ|-dimensional
vector), this vector of size |Σ|pn´1qwill contain the multi-hot representation of the n-gram of
interest. Let y1,...,yn´1be the symbols represented by v. This means that vis of the form
v“¨
˚˝Jy1K
...
Jyn´1K˛
‹‚ (5.143)
andvk|Σ|`j“1 if and only if mpykq “jfor an ordering mofΣdetermining the one-hot
representations of the individual symbols. We would then like to transform this vector into a vector
uPR|Σ|n´1such that
ui“1 if and only if i“s`
y1,...,yn´1˘
(5.144)
for some ordering sofΣˆ¨¨¨ˆ Σ looooomooooon
n´1 times. This can be equivalently written as
ui“1 if and only if vk|Σ|`mpykq`1 for allk“1,..., n´1 (5.145)
wherei“s`
y1,...,yn´1˘
. Clearly, this is the same problem as described in Fact 5.2.3 and can
therefore be solved by a linear transformation followed by the application of the thresholded sigmoid
nonlinearity, which will together form the transformation fHcombining the information obtained
from all the heads of the transformer model. Note that, to make this more similar to the practice of
how transformers are actually implemented, we could also use the ReLU activation function instead
of the saturated sigmoid.
This brings us to the final part of the proof, which considers the first part of determining the
conditional probability of the n-gram model by the transformer: Identifying the symbols at the
previous n´1 positions by the n´1 heads of the transformer. To show how this can be done, let
us consider and define the “degrees of freedom” we have left when specifying a transformer model in
our framework.
•The symbol representations r. We will use simple one-hot encodings of the tokens: rpyqdef“JyK.
40Note that we are again working over the set of extended realsR“RYt´8,8uin case of the softmax activation
function.
5.4. REPRESENTATIONAL CAPACITY OF TRANSFORMER LANGUAGE MODELS 231
•The positional encodings fpos. We will use the following simple positional encoding: fposptq“ˆ
t
1˙
. The utility of the constant 1 will be made clear shortly. We will combine positional
encodings with symbol representations by concatenating them into a vector of size |Σ|`2.
•The number of transformer layers. We will use a single transformer layer.
•The number of heads H: As we mentioned, we will use H“n´1 heads to attend to the
previous n´1 symbols.
•The form of the attention scoring function f. While not the most typical, we will use the
following scoring function:
fpq,kqdef“´|x q,ky|. (5.146)
It will, together with the positional encodings, allow us to easily single out the positions in
the string that we care about.
•The form of attention. We will use hard attention (in this case, it can be either unique or
averaging).
•The parameters of each of the attention heads, that is the transformations Q,K, andV. Each
of those will take the form of a linear transformation of the symbol embedding. We describe
them and their roles in more detail below.
As mentioned above, the input symbol ytis presented to the transformer model together with its
positional encoding in the form
rpytq“¨
˝JytK
t
1˛
‚PR|Σ|`2. (5.147)
The parameters of all the heads are defined in the same way, with the only difference being a simple
parameter that depends on the “index” of the head we are considering, h. Therefore, in the following,
we describe the construction of a single head Headh. At any time step t(i.e., when modeling the
conditional distribution pSMpyt|yătq), the head hwill attend to or be “responsible for” recognizing
the symbol at position t´h,yt´h. This can be seen in Fig. 5.24, where, for example, Head 3 is
responsible for the position t´3, which is denoted by the stronger arrow to that position. All we
still have to do is describe the individual transformations Qh,Kh,Vhof the head h. All of them
will be linear transformations, i.e., matrix multiplication:
qdef“Qpxqdef“Qhx (5.148)
kdef“Kpxqdef“Khx (5.149)
vdef“Vpxqdef“Vhx (5.150)
We now define the matrices Qh,Kh, and Vh, specifically in the first (in this case, the only) layer
of a transformer language model. Importantly, since we are talking about only the first layer, we
can simply consider as inputs to the layer the original static symbol representations together with
their position encodings rather than any contextual representations. First, let us consider again
what roles the matrices play in computing encTpyătq. In the context of language modeling, the
matrix Qhtakes in the representation of the “latest” generated symbol yt´1and produces from it
232 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
the query vector of yt´1. It is, therefore, only applied once per generation step—only for symbol
yt´1. The matrices KhandVh, on the other hand, transform allnon-masked input symbols to
the key and value vectors. That is, they take the representations of the input symbols and their
positions encodings encTpyjqfor everyj“1,...,t´1 and transform them into the key and value
vectors. The keys will then be compared with the query constructed for yt´1with the Qhmatrix,
while the constructed values will be used to compute the new hidden state ht.41
So, what kind of query, key, and value vectors do we want? As mentioned, the head hwill be
responsible for identifying the symbol at position t´h. Therefore, we want it to put all its attention
to this position. In other words, given the query qt´1, we want the attention function in Eq. (5.146)
to be maximized by the key of the symbol at position t´h. Notice that, therefore, the key does
not have to depend on the identity of the symbol at position t´h—only the position information
matters. Let us then consider the following query and key transformations for head h:
Q:¨
˝JytK
t
1˛
‚ÞÑˆ
t´h
1˙
(5.151)
K:¨
˝JyjK
j
1˛
‚ÞÑˆ
´1
j˙
. (5.152)
Given such a query and such keys, the attention scoring function computes
fpqt,kjq“´|xˆ
t´h
1˙
,ˆ
´1
j˙
y|“´|t´h´j|, (5.153)
which is maximized exactly when j“t´h, that is, at the position that we want the head hto
attend to! This means that the hard attention we use will put all its probability mass to exactly the
position we intended it to. Intuitively, both transformations keep only the positional information.
The query transformation “injects” the knowledge of which position should maximize the attention
score, while the key transformation (which is, again, applied to all the non-masked positions) simply
“exposes” the positional information about the symbol. The alternating constant 1 (or ´1) and the
index of the position ensure that the inner product simply computes the difference between the
position of the symbol and the position of interest—we will use this trick multiple times in later
constructions as well. It is easy to see that the two transformations are indeed linear.
This leaves us with the question of how to use this position of the symbol of interest ( t´h)
to extract the one-hot encoding of the symbol at that position. Luckily, due to the information
contained in the symbol representations rpyjq, this is trivial. All that the transformation Vhas to
do is the following:
V:¨
˝JyjK
j
1˛
‚ÞÑJyjK. (5.154)
With this, the identity of the symbol is carried forward through the attention mechanism. Again,
is easy to see that this is a linear transformation of the symbol representation. Notice that the
only head-depend transformation is the query transformation—it depends on the index of the head,
41Importantly, in a multi-layer transformer, the queries would be constructed for every non-masked symbol and its
representation (hidden state) would be updated. However, since the updated representations would not be used in
the single layer case, we only have to compute the representation of the newest symbol in this case.
5.4. REPRESENTATIONAL CAPACITY OF TRANSFORMER LANGUAGE MODELS 233
determining the position of interest, meaning that every head defines a different query transformation,
while the keys and values transformations are the same among all heads.
This concludes the proof. Fig. 5.25 again shows an illustration of the described model with all
the defined components.
■
This proof establishes the only “concrete” result on the (lower bound of the) expressivity for
transformers in the form of model equivalence (cf. Definition 5.4.1) that we know of. In the next
subsections, we discuss how transformer-based language models can simulate more complex formal
models. However, the simulation will not be as “direct” as the n-gram one, in the sense that
we will have to work with modified alphabets which, as we noted above, results in a different
notion of equivalence of models than what we have considered so far. We will thus not model the
conditional probabilities pSMpy|yătq, but rather the probabilities over some more complex (but
still finitely-many) objects, which will carry in them more information than just the generated
symbol. As we will discuss, this will be required due to the limited abilities of transformers to
execute sequential operations compared to RNNs and classical language models, as hinted at in
§5.1.5.
234 CHAPTER 5. NEURAL NETWORK LANGUAGE MODELS
Values Jy1K Jy2K¨¨¨ Jyt´3KJyt´2KJyt´1KKeys´1
1´1
2¨¨¨´1
t´3´1
t´2´1
t´1Queryt´3
1
Attention scores 2´t1´t¨¨¨ 0´1´2
Hard attention weights 0 0¨¨¨ 1 0 0y1y2¨¨¨yt´3yt´2yt´1yt¨¨¨Head 3 Head 2 Head 1fHpSMpyt|yătq
E
Figure 5.25: A more complete illustration of the construction described in the proof for the case of
the third head, Head 3, based on Fig. 5.24. Note that, among the three heads, only the query vector
(transformation) differs, while the key and value transformations are identical among the heads.
Index
Symbols
bossymbol 18
eossymbol 21
σ-algebra 10
softmax RNN sequence models 144
σ-algebra 10
(full) path weight 82
(weighted) language 83, 130
beginning o f sequence 18
end of sequence 21
n-gram assumption 97
A
accepting 82, 127
accepts 78
accessible 84
accessible state 84
activation function 146
algebra 11
allsum 83, 117, 131
alphabet 14
ambiguous 112
applicable 109
Attention 211
attention block 217
attention head 221
attention matrix 217
attention mechanism 209
averaging hard attention 212
B
backward values 83
basis vectors 48
bias 58
bias vector 151
bigram 98
bigram model 98C
candidate 155
candidate vector 154
categorical distribution 53
center embeddings 108
co-accessible 84
co-accessible state 84
column matrix 177
complete 49
component-activating matrix 178, 183
concatenation 14
configuration 126
conjugate 48
consistent 26
context 21, 97
context encoding function 52
context-free 118
context-free grammar 108
applicable production 109
derivation 110
derivation tree 110
derive 110
language 110
non-terminal 109
parse tree 110
production 109
start-symbol 109
terminal 109
context-free language 107
context-free languages 107
contextual symbol encodings 211
coordinate vector 48
corpus 61
counter machines 204
cross-serial dependencies 138
cross-serial dependency 138
curriculum learning 70
235
236 INDEX
cylinder set 30
D
data leakage 68
data sub-vector 179
derivation 110
derivation set 111
derivation set of a grammar 112
derivation set of a non-terminal 112
derivation tree 110
derived from 110
derives 110
detectable 176
deterministic 77, 128
deterministic FSA 77
deterministic PDA 128
dimensionality 48
distributed word representations 104
divergence measure 63
dynamics map 140
E
early stopping 71
easily detectable 176
Elman sequence model 149
embedding function 51, 149
embedding tying 150
embeddings 51
empty string 14
encoding 52
energy function 18
energy-based 18
entropy regularizer 72
entropy regularizers 72
equivalent 227
event 10
events 10
exposure bias 66
F
finite-state 76, 85
finite-state automaton 76
recognized language 78
string acceptance 78
first-moment matrix 122
forget 154
formal language theory 14
four-hot representation 183
Frobenius normal form 95G
gate 153
gated recurrent unit 155
gating functions 153
generating 115
generating function 121
globally normalized model 19
good representation principle 47
H
halting problem 135, 206
Heaviside 158
Heaviside Elman network 158
hidden state 140
hidden states 140
Hilbert space 47, 49
history 21, 97
homomorphically equivalent 227
I
infinite sequence 15
infinite sequences 15
information projection 65
initial state 142
inner path weight 82
inner product 48
inner product space 48
input 154
input matrix 151
input string 77
irreducible normal form 95
J
Jordan sequence model 150
K
keys 211
Kleene closure 15
Kleene star 14
L
language 15, 78, 110
language model 16, 29
context-free 118
energy-based 18
finite-state 85
globally normalized 19
locally normalized 21
pushdown 132
weighted language 17
INDEX 237
language model induced by G 120
language model induced by P 132
language model induced by A 87
language modeling task 61
language recognized by P 127
layer normalization 221
length 81
level of a generation sequence 120
likelihood 64
log 64
pseudo 66
line matrix 177
locally normalized language model 21
log-likelihood 64
logits 52
long short-term memory unit 154
LSTM 154
M
masked attention block 219
masking matrix 219
matrix detection 175
matrix representation 174
measurable space 10
measurable subset 10
measurable subsets 10
memory cell 154
monoid 14
multi-head attention 220
multi-head attention block 221
multi-hot 230
N
non-decreasing 184
non-deterministic 78, 128
non-deterministic FSA 78
non-deterministic PDA 128
non-scanning 127
non-terminating 29
non-tight 26, 38
norm 49
normalizable 19, 84, 118, 131
normalizable energy function 19
normalization constant 19
northwestern 176
O
one-hot encoding 146
one-hot encodings 51
optimization algorithms 69outcome space 10
output 154
output matrix 151
overfitting 71
P
pad 98
padding 98
parameter estimation 68
path 81
accepting 82
inner weight 82
length 81
successful 82
weight 82
yield 81
permutation-detectable 176
position-augmented 220
positional encoding 220
prefix 16
prefix probability 22
Prefixes 16
probabilistic 84, 116, 129, 134
probabilistic context-free grammar 116
probabilistic finite-state automaton 84
probabilistic pushdown automaton 129
probabilistic two-stack pushdown
automaton 134
probability 86, 119
probability measure 11
probability pre-measure 12
probability simplex 53
probability space 11
processing sub-vector 179
production
yield 109
production generating function 121
projection 53
projection function 53
proper 26
pruned 115
Pruning 115
pseudo(log)likelihood 66
pushdown automata 126
pushdown automaton 126
accepting run 127
configuration 126
non-scanning transition 127
recognized language 127
recognized string 127
238 INDEX
run 127
scan 127
scanning transition 127
pushdown language model 132
pushforward measure 12
Q
query 211
R
random variable 12
rational-valued recurrent neural network
142
rational-valued recurrent neural networks
142
reachable 115
real-valued recurrent neural network 142
real-weighted context-free grammar 115
real-weighted finite-state automaton 79
real-weighted pushdown automaton 129
recognizes 127, 130
rectified linear unit 147
recurrence matrix 150
recurrent dynamics map 140
recurrent neural encoding function 143
recurrent neural network 140
recurrent neural networks 140
recurrent neural sequence model 144
regular 78
regular language 78
regular languages 78
ReLU 147
representation function 50
representation space 47
representation-based language modeling 46
representation-based locally normalized
model 58
reset 155
residual connections 214
result of applying 109
RNN 140
row matrix 177
run 127
S
saturated sigmoid 192
scalars 47
scanning 127
scans 127
score function 92scores 52
scoring function 211
self-attention 218
self-supervised 62
sentence 15
sentences 15
sequence 15
sequence model 21, 29
sequences 15
soft attention 212
softmax 54
sparsemax 57
spectral radius 95
square-root state representation 173
static symbol embeddings 149
stochastic gradient descent 70
strictly n-local 101
strictly n-local 101
strictly local 101
string 14
stringsum 82, 117, 130
subregular 101
subregular language 101
subsequence 16
substochastic 94
substring 16
successful 82
suffix 16
suffixes 16
symbol-specific transition matrix 80
symbols 14
T
teacher forcing 66
temperature 54
thin cylinder 30
tied 150
tight 21, 26, 38
token 15
token to type switch 103, 136
tokens 15
training 68
transformer 215
transformer layer 214
transformer network 210
transformer sequence model 211
transition 76
transition matrix 80
transitions 76
Trimming 84
INDEX 239
two-stack pushdown automaton 133
two-stack real-weighted pushdown
automaton 133
two-stack weighted pushdown automaton
133
U
unambiguous 111, 112
unique hard attention 213
unit 14
update 155
useful 84
useful state 84
V
values 211
vector representation 46, 173
vector space 47
vectors 47
vocabulary 15
W
weight 130
weight of a derivation tree 116
weight pushing 90
weighted context-free grammar 115allsum 117
derivation tree weight 116
induced language model 120
non-terminal allsum 117
normalizable 118
stringsum 117
weighted finite-state automaton 79
allsum 83
induced language model 87
normalizable 84
state-specific allsum 83
stringsum 82
substochastic 94
transition matrix 80
trim 84
weighted language 17
weighted pushdown automaton 129
allsum 131
induced language model 132
normalizable 131
run weight 130
stringsum 130
word 14, 15
words 15
Y
yield 81, 109
240 INDEX
Bibliography
Steven Abney, David McAllester, and Fernando Pereira. 1999. Relating probabilistic grammars
and automata. In Proceedings of the 37th Annual Meeting of the Association for Computa-
tional Linguistics , pages 542–549, College Park, Maryland, USA. Association for Computational
Linguistics.
Noga Alon, A. K. Dewdney, and Teunis J. Ott. 1991. Efficient simulation of finite automata by
neural nets. J. ACM , 38(2):495–514.
St´ ephane Aroca-Ouellette and Frank Rudzicz. 2020. On Losses for Modern Language Models.
InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP) , pages 4970–4981, Online. Association for Computational Linguistics.
Enes Avcu, Chihiro Shibata, and Jeffrey Heinz. 2017. Subregular complexity and deep learning.
ArXiv , abs/1705.05940.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization.
Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. 1983. A maximum likelihood approach to
continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence ,
PAMI-5(2):179–190.
J. Baker. 1975a. The DRAGON system–An overview. IEEE Transactions on Acoustics, Speech, and
Signal Processing , 23(1):24–29.
James K. Baker. 1975b. Stochastic Modeling as a Means of Automatic Speech Recognition. Ph.D.
thesis, Carnegie Mellon University, USA. AAI7519843.
Anton Bakhtin, Yuntian Deng, Sam Gross, Myle Ott, Marc’Aurelio Ranzato, and Arthur Szlam.
2021. Residual energy-based models for text. Journal of Machine Learning Research , 22(40):1–41.
Yonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational
Linguistics , 48(1):207–219.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for
sequence prediction with recurrent neural networks. In Advances in Neural Information Processing
Systems , volume 28.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and
new perspectives. IEEE transactions on pattern analysis and machine intelligence , 35(8):1798–1828.
Cite arxiv:1206.5538.
241
242 BIBLIOGRAPHY
Yoshua Bengio, R´ ejean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model.
InAdvances in Neural Information Processing Systems , volume 13. MIT Press.
Yoshua Bengio, R´ ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003a. A neural proba-
bilistic language model. J. Mach. Learn. Res. , 3:1137–1155.
Yoshua Bengio, R´ ejean Ducharme, Pascal Vincent, Christian Jauvin, Jauvinciro Umontreal Ca,
Jaz Kandola, Thomas Hofmann, Tomaso Poggio, and John Shawe-Taylor. 2003b. A neural
probabilistic language model. Journal of Machine Learning Research , 3:1137–1155.
Yoshua Bengio, Holger Schwenk, Jean-S´ ebastien Sen´ ecal, Fr´ ederic Morin, and Jean-Luc Gauvain.
2006. Neural Probabilistic Language Models , pages 137–186. Springer Berlin Heidelberg, Berlin,
Heidelberg.
Julian Besag. 1975. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society.
Series D (The Statistician) , 24(3):179–195.
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. 2020. On the Ability and Limitations of
Transformers to Recognize Formal Languages. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) , pages 7096–7116, Online. Association for
Computational Linguistics.
Patrick Billingsley. 1995. Probability and Measure , 3rdedition. Wiley.
Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning . Springer-Verlag, Berlin,
Heidelberg.
Mathieu Blondel, Andre Martins, and Vlad Niculae. 2019. Learning classifiers with fenchel-young
losses: Generalized entropies, margins, and algorithms. In Proceedings of the Twenty-Second
International Conference on Artificial Intelligence and Statistics , volume 89 of Proceedings of
Machine Learning Research , pages 606–615. PMLR.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word
vectors with subword information. Transactions of the Association for Computational Linguistics ,
5:135–146.
L. Boltzmann. 1868. Studien ¨ uber das Gleichgewicht der lebendigen Kraft zwischen bewegten
materiellen Punkten: vorgelegt in der Sitzung am 8. October 1868 . k. und k. Hof- und Staatsdr.
T.L. Booth and R.A. Thompson. 1973. Applying probability measures to abstract languages. IEEE
Transactions on Computers , C-22(5):442–450.
Adam L. Buchsbaum, Raffaele Giancarlo, and Jeffery R. Westbrook. 2000. On the determinization
of weighted finite automata. SIAM Journal on Computing , 30(5):1502–1531.
Alexandra Butoi, Brian DuSell, Tim Vieira, Ryan Cotterell, and David Chiang. 2022. Algorithms
for weighted pushdown automata.
Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for
language modeling. In 34th Annual Meeting of the Association for Computational Linguistics ,
pages 310–318, Santa Cruz, California, USA. Association for Computational Linguistics.
BIBLIOGRAPHY 243
Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. 2018. Recurrent
neural networks as weighted language recognizers. NAACL HLT 2018 - 2018 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies - Proceedings of the Conference , 1:2261–2271.
Zhiyi Chi. 1999. Statistical properties of probabilistic context-free grammars. Computational
Linguistics , 25(1):131–160.
Zhiyi Chi and Stuart Geman. 1998. Estimation of probabilistic context-free grammars. Computational
Linguistics , 24(2):299–305.
David Chiang and Peter Cholak. 2022. Overcoming a theoretical limitation of self-attention. In
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) , pages 7654–7664, Dublin, Ireland. Association for Computational Linguistics.
Kyunghyun Cho, Bart van Merri¨ enboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014a. On the
properties of neural machine translation: Encoder–decoder approaches. In Proceedings of SSST-8,
Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation , pages 103–111,
Doha, Qatar. Association for Computational Linguistics.
Kyunghyun Cho, Bart van Merri¨ enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014b. Learning phrase representations using RNN encoder–
decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP) , pages 1724–1734, Doha, Qatar. Association
for Computational Linguistics.
N. Chomsky and M.P. Sch¨ utzenberger. 1963. The algebraic theory of context-free languages. In
P. Braffort and D. Hirschberg, editors, Computer Programming and Formal Systems , volume 35
ofStudies in Logic and the Foundations of Mathematics , pages 118–161. Elsevier.
Noam Chomsky. 1959. On certain formal properties of grammars. Information and Control ,
2(2):137–167.
Noam Chomsky. 1965. Aspects of the Theory of Syntax , 50 edition. The MIT Press.
K. Culik and Arto Salomaa. 1978. On the decidability of homomorphism equivalence for languages.
Journal of Computer and System Sciences , 17(2):163–175.
Gr’egoire Del’etang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot
Catt, Marcus Hutter, Shane Legg, and Pedro A. Ortega. 2022. Neural networks and the chomsky
hierarchy. ArXiv , abs/2207.02098.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training
of deep bidirectional transformers for language understanding. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis,
Minnesota. Association for Computational Linguistics.
A. K. Dewdney. 1977. Threshold matrices and the state assignment problem for neural nets. In
Proceedings of the 8th SouthEastern Conference on Combinatorics, Graph Theory and Computing ,
pages 227–245, Baton Rouge, La, USA.
244 BIBLIOGRAPHY
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah A. Smith.
2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early
stopping. CoRR , abs/2002.06305.
Li Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara Meister, Jason Eisner, and Ryan Cotterell.
2022. A measure-theoretic characterization of tight language models.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res. , 12(null):2121–2159.
Rick Durrett. 2019. Probability: Theory and Examples , 5thedition. Cambridge Series in Statistical
and Probabilistic Mathematics. Cambridge university press.
Jason Eisner. 2016. Inside-outside and forward-backward algorithms are just backprop (tutorial
paper). In Proceedings of the Workshop on Structured Prediction for NLP , pages 1–17, Austin,
TX. Association for Computational Linguistics.
Jeffrey L. Elman. 1990. Finding structure in time. Cognitive Science , 14(2):179–211.
Patrick C. Fischer, Albert R. Meyer, and Arnold L. Rosenberg. 1968. Counter machines and counter
languages. Mathematical systems theory , 2:265–283.
William A. Gale and Geoffrey Sampson. 1995. Good-turing frequency estimation without tears. J.
Quant. Linguistics , 2:217–237.
W. G. Gibbs. 1902. Elementary Principles in Statistical Mechanics . Charles Scribner’s Sons.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectifier neural networks. In
Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics ,
volume 15 of Proceedings of Machine Learning Research , pages 315–323, Fort Lauderdale, FL,
USA. PMLR.
Glorot, Xavier and Bengio, Yoshua. 2010. Understanding the difficulty of training deep feedforward
neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence
and Statistics , volume 9 of Proceedings of Machine Learning Research , pages 249–256, Chia Laguna
Resort, Sardinia, Italy. PMLR.
Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2018. FRAGE: Frequency-
Agnostic word representation. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems , NIPS’18, page 1341–1352, Red Hook, NY, USA. Curran Associates
Inc.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning . MIT Press.
Andreas Griewank and Andrea Walther. 2008. Evaluating Derivatives: Principles and Techniques
of Algorithmic Differentiation , 2ndedition. SIAM.
Charles M. Grinstead and J. Laurie Snell. 1997. Introduction to Probability , 2ndrevised edition.
American Mathematical Society.
Michael Hahn. 2020. Theoretical limitations of self-attention in neural sequence models. Transactions
of the Association for Computational Linguistics , 8:156–171.
BIBLIOGRAPHY 245
Yiding Hao, William Merrill, Dana Angluin, Robert Frank, Noah Amsel, Andrew Benz, and Simon
Mendelsohn. 2018. Context-free transductions with neural stacks. In Proceedings of the 2018
EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages
306–315, Brussels, Belgium. Association for Computational Linguistics.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2001. The Elements of Statistical Learning .
Springer Series in Statistics. Springer New York Inc., New York, NY, USA.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers:
Surpassing human-level performance on imagenet classification. In 2015 IEEE International
Conference on Computer Vision (ICCV) , pages 1026–1034.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,
pages 770–778.
Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of
the Sixth Workshop on Statistical Machine Translation , pages 187–197, Edinburgh, Scotland.
Association for Computational Linguistics.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable
modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Volume 2: Short Papers) , pages 690–696, Sofia,
Bulgaria. Association for Computational Linguistics.
D.O. Hebb. 1949. The Organization of Behavior: A Neuropsychological Theory . A Wiley book in
clinical psychology. Wiley.
John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, and Christopher D. Manning. 2020. RNNs
can generate bounded hierarchical languages with optimal memory. In Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1978–2010,
Online. Association for Computational Linguistics.
John Hewitt and Christopher D. Manning. 2019. A structural probe for finding syntax in word
representations. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long
and Short Papers) , pages 4129–4138, Minneapolis, Minnesota. Association for Computational
Linguistics.
Sepp Hochreiter and J¨ urgen Schmidhuber. 1997. Long short-term memory. Neural computation ,
9:1735–80.
Elad Hoffer, Itay Hubara, and Daniel Soudry. 2017. Train longer, generalize better: Closing
the generalization gap in large batch training of neural networks. In Proceedings of the 31st
International Conference on Neural Information Processing Systems , pages 1729—-1739, Red
Hook, NY, USA. Curran Associates Inc.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. 2006. Introduction to Automata Theory,
Languages, and Computation (3rd Edition) . Addison-Wesley Longman Publishing Co., Inc., USA.
246 BIBLIOGRAPHY
Roger A. Horn and Charles R. Johnson. 2012. Matrix Analysis , 2ndedition. Cambridge University
Press.
Ferenc Husz´ ar. 2015. How (not) to Train your Generative Model: Scheduled Sampling, Likelihood,
Adversary? CoRR , abs/1511.05101.
Riny Huybregts, Germen de Haan, Mieke Trommelen, and Wim Zonneveld. 1984. Van periferie naar
kern. Computational Linguistics .
Thomas Icard. 2020a. Calibrating generative models: The probabilistic chomsky-sch¨ utzenberger
hierarchy. Journal of Mathematical Psychology , 95.
Thomas F. Icard. 2020b. Calibrating generative models: The probabilistic chomsky–sch¨ utzenberger
hierarchy. Journal of Mathematical Psychology , 95:102308.
P. Indyk. 1995. Optimal simulation of automata by neural nets. In STACS 95 , pages 337–348,
Berlin, Heidelberg. Springer Berlin Heidelberg.
Gerhard J¨ ager and James Rogers. 2012. Formal language theory: Refining the chomsky hierarchy.
Philos Trans R Soc Lond B Biol Sci , 367(1598):1956–1970.
Ganesh Jawahar, Benoˆ ıt Sagot, and Djam´ e Seddah. 2019. What does BERT learn about the structure
of language? In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics , pages 3651–3657, Florence, Italy. Association for Computational Linguistics.
E. T. Jaynes. 1957. Information theory and statistical mechanics. Phys. Rev. , 106:620–630.
F. Jelinek. 1976. Continuous speech recognition by statistical methods. Proceedings of the IEEE ,
64(4):532–556.
F. Jelinek. 1990. Self-Organized Language Modeling for Speech Recognition , page 450–506. Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA.
Lifeng Jin, Finale Doshi-Velez, Timothy Miller, William Schuler, and Lane Schwartz. 2018. Depth-
bounding is effective: Improvements and evaluation of unsupervised PCFG induction. In Pro-
ceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages
2721–2731, Brussels, Belgium. Association for Computational Linguistics.
Michael I. Jordan. 1986. Serial order: A parallel distributed processing approach. Technical report .
Michael I. Jordan. 1997. Chapter 25 - serial order: A parallel distributed processing approach.
In John W. Donahoe and Vivian Packard Dorsel, editors, Neural-Network Models of Cognition ,
volume 121 of Advances in Psychology , pages 471–495. North-Holland.
Daniel Jurafsky and James H. Martin. 2009. Speech and Language Processing (2nd Edition) .
Prentice-Hall, Inc., USA.
Fred Karlsson. 2007. Constraints on multiple center-embedding of clauses. Journal of Linguistics ,
43(2):365–392.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd
International Conference on Learning Representations .
BIBLIOGRAPHY 247
Samuel A. Korsky and Robert C. Berwick. 2019. On the computational power of rnns.
Matthieu Labeau and Shay B. Cohen. 2019. Experimenting with power divergences for language
modeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP) , pages 4104–4114, Hong Kong, China. Association for Computational Linguistics.
Daniel J. Lehmann. 1977. Algebraic structures for transitive closure. Theoretical Computer Science ,
4(1):59–76.
Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. 2021a. Limitations
of autoregressive models and their alternatives. In Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies , pages 5147–5173, Online. Association for Computational Linguistics.
Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. 2021b. Limitations
of autoregressive models and their alternatives. In Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies , pages 5147–5173, Online. Association for Computational Linguistics.
Chu-Cheng Lin and Arya D. McCarthy. 2022. On the uncomputability of partition functions in
energy-based sequence models. In International Conference on Learning Representations .
Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019.
Linguistic knowledge and transferability of contextual representations. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers) , pages 1073–1094, Minneapolis,
Minnesota. Association for Computational Linguistics.
Christopher D. Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. 2020.
Emergent linguistic structure in artificial neural networks trained by self-supervision. Proceedings
of the National Academy of Sciences , 117(48):30046–30054.
Andr´ e F. T. Martins and Ram´ on F. Astudillo. 2016. From softmax to sparsemax: A sparse model
of attention and multi-label classification. In Proceedings of the 33rd International Conference on
International Conference on Machine Learning - Volume 48 , ICML’16, page 1614–1623. JMLR.org.
Clara Meister, Elizabeth Salesky, and Ryan Cotterell. 2020. Generalized entropy regularization or:
There’s nothing special about label smoothing. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , pages 6870–6886, Online. Association for Computational
Linguistics.
William Merrill. 2019. Sequential neural networks as automata. In Proceedings of the Workshop on
Deep Learning and Formal Languages: Building Bridges , pages 1–13, Florence. Association for
Computational Linguistics.
William Merrill and Ashish Sabharwal. 2023. The parallelism tradeoff: Limitations of log-precision
transformers. Transactions of the Association for Computational Linguistics , 11:531–545.
248 BIBLIOGRAPHY
William Merrill, Ashish Sabharwal, and Noah A. Smith. 2022a. Saturated transformers are constant-
depth threshold circuits. Transactions of the Association for Computational Linguistics , 10:843–
856.
William Merrill, Alex Warstadt, and Tal Linzen. 2022b. Entailment semantics can be extracted from
an ideal language model. In Proceedings of the 26th Conference on Computational Natural Language
Learning (CoNLL) , pages 176–193, Abu Dhabi, United Arab Emirates (Hybrid). Association for
Computational Linguistics.
William Merrill, Gail Weiss, Yoav Goldberg, Roy Schwartz, Noah A. Smith, and Eran Yahav. 2020.
A formal hierarchy of RNN architectures. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , pages 443–459, Online. Association for Computational
Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word
representations in vector space.
George A. Miller and Noam Chomsky. 1963. Finitary models of language users. In D. Luce, editor,
Handbook of Mathematical Psychology , pages 2–419. John Wiley & Sons.
Thomas Minka. 2005. Divergence measures and message passing. Technical report, Microsoft.
Marvin Lee Minsky. 1986. Neural Nets and the brain model problem . Ph.D. thesis, Princeton
University.
Mehryar Mohri, Fernando Pereira, and Michael Riley. 2008. Speech Recognition with Weighted
Finite-State Transducers , pages 559–584. Springer Berlin Heidelberg, Berlin, Heidelberg.
James R. Munkres. 2000. Topology , 2ndedition. Prentice Hall, Inc.
Hermann Ney, Ute Essen, and Reinhard Kneser. 1994. On structuring probabilistic dependences in
stochastic language modelling. Computer Speech & Language , 8(1):1–38.
Frank Nielsen. 2018. What is an information projection? Notices of the American Mathematical
Society , 65:1.
Franz Nowak, Anej Svete, Li Du, and Ryan Cotterell. 2023. On the representational capacity of
recurrent neural language models. In Proceedings of the 2023 Conference on Empirical Methods
in Natural Language Processing , pages 7011–7034, Singapore. Association for Computational
Linguistics.
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for
word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 1532–1543, Doha, Qatar. Association for Computational
Linguistics.
Gabriel Pereyra, George Tucker, Jan Chorowski, /suppress Lukasz Kaiser, and Geoffrey E. Hinton. 2017.
Regularizing neural networks by penalizing confident output distributions. In Proceedings of the
International Conference on Learning Representations .
B.T. Polyak. 1964. Some methods of speeding up the convergence of iteration methods. USSR
Computational Mathematics and Mathematical Physics , 4(5):1–17.
BIBLIOGRAPHY 249
Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra. 2022. Grokking:
Generalization beyond overfitting on small algorithmic datasets. CoRR , abs/2201.02177.
Jorge P´ erez, Pablo Barcel´ o, and Javier Marinkovic. 2021. Attention is turing-complete. Journal of
Machine Learning Research , 22(75):1–35.
Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2021. A Primer in BERTology: What We
Know About How BERT Works. Transactions of the Association for Computational Linguistics ,
8:842–866.
Halsey L. Royden. 1988. Real Analysis , 3rdedition. Prentice-Hall.
Holger Schwenk. 2007. Continuous space language models. Computer Speech & Language , 21(3):492–
518.
Thibault Sellam, Steve Yadlowsky, Ian Tenney, Jason Wei, Naomi Saphra, Alexander D’Amour, Tal
Linzen, Jasmijn Bastings, Iulia Raluca Turc, Jacob Eisenstein, Dipanjan Das, and Ellie Pavlick.
2022. The multiBERTs: BERT reproductions for robustness analysis. In International Conference
on Learning Representations .
C. E. Shannon. 1948a. A mathematical theory of communication. The Bell System Technical
Journal , 27(3):379–423.
Claude E. Shannon. 1948b. A mathematical theory of communication. The Bell System Technical
Journal , 27(3):379–423.
Stuart M. Shieber. 1985. Evidence against the context-freeness of natural language. Linguistics and
Philosophy , 8:333–343.
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2023. Distilling reasoning capabilities
into smaller language models. In Findings of the Association for Computational Linguistics: ACL
2023, pages 7059–7073, Toronto, Canada. Association for Computational Linguistics.
Hava T. Siegelmann and Eduardo D. Sontag. 1992. On the computational power of neural nets. In
Proceedings of the Fifth Annual Workshop on Computational Learning Theory , COLT ’92, page
440–449, New York, NY, USA. Association for Computing Machinery.
Michael Sipser. 2013. Introduction to the Theory of Computation , third edition. Course Technology,
Boston, MA.
Noah A. Smith and Mark Johnson. 2007. Weighted and probabilistic context-free grammars are
equally expressive. Computational Linguistics , 33(4):477–491.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine
Learning Research , 15(56):1929–1958.
Anej Svete, Robin Shing Moon Chan, and Ryan Cotterell. 2024. A theoretical result on the inductive
bias of rnn language models. arXiv preprint arXiv:2402.15814 .
Anej Svete and Ryan Cotterell. 2023a. Efficiently representing finite-state automata with recurrent
neural networks.
250 BIBLIOGRAPHY
Anej Svete and Ryan Cotterell. 2023b. Recurrent neural language models as probabilistic finite-state
automata. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing , pages 8069–8086, Singapore. Association for Computational Linguistics.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2015. Re-
thinking the inception architecture for computer vision. 2016 IEEE Conference on Computer
Vision and Pattern Recognition , pages 2818–2826.
G´ abor Sz´ arnyas. 2020. Graphs and matrices: A translation of ”Graphok ´ es matrixok” by D´ enes
K˝ onig (1931).
Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. oLMpics-On What Language
Model Pre-training Captures. Transactions of the Association for Computational Linguistics ,
8:743–758.
Terence Tao. 2011. An Introduction to Measure Theory . American Mathematical Society.
Terence Tao. 2016. Analysis II: Third Edition . Texts and Readings in Mathematics. Springer
Singapore.
Wilson L. Taylor. 1953. “Cloze Procedure”: A new tool for measuring readability. Journalism
Quarterly , 30(4):415–433.
L. Theis, A. van den Oord, and M. Bethge. 2016. A note on the evaluation of generative models. In
4th International Conference on Learning Representations .
A. M. Turing. 1937. On Computable Numbers, with an Application to the Entscheidungsproblem.
Proceedings of the London Mathematical Society , s2-42(1):230–265.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, /suppress Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information
Processing Systems , volume 30.
Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On the practical computational power of
finite precision RNNs for language recognition. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers) , pages 740–745, Melbourne,
Australia. Association for Computational Linguistics.
Sean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, and Kyunghyun Cho. 2020.
Consistency of a recurrent language model with respect to incomplete decoding. In Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages
5553–5568, Online. Association for Computational Linguistics.
George Kingsley Zipf. 1935. The Psycho-Biology of Language . Houghton-Mifflin, New York, NY,
USA.
